<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns:og="http://ogp.me/ns#">
<head>
    <title>Apache Kafka</title>
<link rel='stylesheet' href='http://localhost:8080/35/documentation/css/styles.css?2' type='text/css'>
<link rel="icon" type="image/gif" href="http://localhost:8080/35/documentation/images/apache_feather.gif">
<meta name="robots" content="index,follow"/>
<meta name="language" content="en"/>
<meta name="keywords" content="apache kafka messaging queuing distributed stream processing">
<meta name="description" content="Apache Kafka: A Distributed Streaming Platform.">
<meta http-equiv='Content-Type' content='text/html;charset=utf-8'/>
<meta name="viewport" content="initial-scale = 1.0,maximum-scale = 1.0"/>
<meta property="og:title" content="Apache Kafka"/>
<meta property="og:image" content="http://apache-kafka.org/images/apache-kafka.png"/>
<meta property="og:description" content="Apache Kafka: A Distributed Streaming Platform."/>
<meta property="og:site_name" content="Apache Kafka"/>
<meta property="og:type" content="website"/>
<link href="http://localhost:8080/35/documentation/css/fonts.css" rel="stylesheet">
<script src="http://localhost:8080/35/documentation/js/jquery.min.js"></script>
<script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>

<link rel="stylesheet" href="http://localhost:8080/35/documentation/css/prism.min.css"/>
<script defer src="http://localhost:8080/35/documentation/js/prism.min.js"></script>

<script>
			
			
			
			var legacyDocPaths = [
				'/07/documentation',
				'/07/documentation/',
				'/08/documentation',
				'/08/documentation/',
				'/081/documentation',
				'/081/documentation/',
				'/082/documentation',
				'/082/documentation/',
				'/090/documentation',
				'/090/documentation/',
				'/0100/documentation',
				'/0100/documentation/'
			];

			
			
			var currentPath = window.location.pathname;
			var shouldRedirect = !legacyDocPaths.includes(currentPath);
			var isDocumenationPage = currentPath.includes('/documentation');

			var hasNotSpecifiedFullPath = !currentPath.includes('/documentation/streams') && !currentPath.includes('/documentation/streams/');

			
			
			var specifiedStreamsAnchor = window.location.hash.includes('#streams_');

			if (shouldRedirect && isDocumenationPage && hasNotSpecifiedFullPath) {
				if (specifiedStreamsAnchor) {
					window.location.pathname = currentPath + 'streams';
				}
			}





    </script>

<script>
			var _paq = window._paq = window._paq || [];
			 
			 
			_paq.push(['disableCookies']);
			_paq.push(['trackPageView']);
			_paq.push(['enableLinkTracking']);
			(function() {
				var u="//matomo.privacy.apache.org/";
				_paq.push(['setTrackerUrl', u+'matomo.php']);
				_paq.push(['setSiteId', '26']);
				var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
				g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
			})();





    </script>


</head>
<body>
<div class="main">
    <div class="header" id="header">
    <a href="/" class="logo-link">
        <span class="visually-hidden">Apache Kafka</span>
    </a>

    <button type="button" class="top-nav-toggle burger-toggle" id="top-nav-toggle" aria-haspopup="true" aria-expanded="false">
        <span class="visually-hidden">Toggle navigation</span>
        <span class="burger-line"></span>
        <span class="burger-line"></span>
        <span class="burger-line"></span>
    </button>

    <div class="top-nav-container" id="top-nav-container">
        <nav class="top-nav" role="navigation" aria-label="Main menu">
            <ul class="top-nav-list">
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false">
                        Get Started
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" title="Get Started">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/intro" tabindex="-1" class="top-nav-anchor">
                                Introduction
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/quickstart" tabindex="-1" class="top-nav-anchor">
                                Quickstart
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/uses" tabindex="-1" class="top-nav-anchor">
                                Use Cases
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/books-and-papers" tabindex="-1" class="top-nav-anchor">
                                Books &amp; Papers
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/videos" tabindex="-1" class="top-nav-anchor">
                                Videos
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/podcasts" tabindex="-1" class="top-nav-anchor">
                                Podcasts
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="/documentation" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-documentation-menu">
                        Docs
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-documentation-menu" title="Docs">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#gettingStarted">
                                Key Concepts
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#api">
                                APIs
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#configuration">
                                Configuration
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#design">
                                Design
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#implementation">
                                Implementation
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#operations">
                                Operations
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#security">
                                Security
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank">
                                Clients
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#connect">
                                Kafka Connect
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation/streams">
                                Kafka Streams
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a class="top-nav-item-anchor" href="/powered-by">
                        Powered By
                    </a>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-community-menu">
                        Community
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-community-menu" title="Community">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://kafka-summit.org/" target="_blank">
                                Kafka Summit
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/project">
                                Project Info
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/trademark">
                                Trademark
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" target="_blank">
                                Ecosystem
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/events">
                                Events
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/contact">
                                Contact us
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-community-menu">
                        Apache
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-community-menu" title="Community">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/licenses/" target="_blank">
                                License
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">
                                Donate
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/foundation/thanks.html" target="_blank">
                                Sponsors
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/security/" target="_blank">
                                Security
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank">
                                Privacy
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/" target="_blank">
                                Apache.org
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="/downloads" class="top-nav-download" tabindex="-1" id="top-nav-download">
                        Download Kafka
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>

</div>
<div class="main">
    <div class="content documentation documentation--current">
        <div class="toc-handle-container">
            <div class="toc-handle">&lt;</div>
        </div>
        <div class="docs-nav">
            <ul class="toc">
    <li>1. Getting Started
        <ul>
            <li><a href="http://localhost:8080/35/documentation/introduction">1.1 Introduction</a>
            <li><a href="http://localhost:8080/35/documentation/uses">1.2 Use Cases</a>
            <li><a href="http://localhost:8080/35/documentation/quickstart">1.3 Quick Start</a>
            <li><a href="http://localhost:8080/35/documentation/ecosystem">1.4 Ecosystem</a>
            <li><a href="http://localhost:8080/35/documentation/upgrade">1.5 Upgrading</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/api">2. APIs</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/api#producerapi">2.1 Producer API</a>
            <li><a href="http://localhost:8080/35/documentation/api#consumerapi">2.2 Consumer API</a>
            <li><a href="http://localhost:8080/35/documentation/api#streamsapi">2.3 Streams API</a>
            <li><a href="http://localhost:8080/35/documentation/api#connectapi">2.4 Connect API</a>
            <li><a href="http://localhost:8080/35/documentation/api#adminapi">2.5 Admin API</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/configuration">3. Configuration</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/configuration#brokerconfigs">3.1 Broker Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#topicconfigs">3.2 Topic Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#producerconfigs">3.3 Producer Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#consumerconfigs">3.4 Consumer Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#connectconfigs">3.5 Kafka Connect Configs</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/configuration#sourceconnectconfigs">Source Connector Configs</a>
                    <li><a href="http://localhost:8080/35/documentation/configuration#sinkconnectconfigs">Sink Connector Configs</a>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/configuration#streamsconfigs">3.6 Kafka Streams Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#adminclientconfigs">3.7 AdminClient Configs</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/design">4. Design</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/design#majordesignelements">4.1 Motivation</a>
            <li><a href="http://localhost:8080/35/documentation/design#persistence">4.2 Persistence</a>
            <li><a href="http://localhost:8080/35/documentation/design#maximizingefficiency">4.3 Efficiency</a>
            <li><a href="http://localhost:8080/35/documentation/design#theproducer">4.4 The Producer</a>
            <li><a href="http://localhost:8080/35/documentation/design#theconsumer">4.5 The Consumer</a>
            <li><a href="http://localhost:8080/35/documentation/design#semantics">4.6 Message Delivery Semantics</a>
            <li><a href="http://localhost:8080/35/documentation/design#replication">4.7 Replication</a>
            <li><a href="http://localhost:8080/35/documentation/design#compaction">4.8 Log Compaction</a>
            <li><a href="http://localhost:8080/35/documentation/design#design_quotas">4.9 Quotas</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/implementation">5. Implementation</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/implementation#networklayer">5.1 Network Layer</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#messages">5.2 Messages</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#messageformat">5.3 Message format</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#log">5.4 Log</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#distributionimpl">5.5 Distribution</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/operations">6. Operations</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/operations#basic_ops">6.1 Basic Kafka Operations</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_add_topic">Adding and removing topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_modify_topic">Modifying topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_restarting">Graceful shutdown</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_leader_balancing">Balancing leadership</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_racks">Balancing Replicas Across Racks</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_mirror_maker">Mirroring data between clusters</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_consumer_lag">Checking consumer position</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_consumer_group">Managing Consumer Groups</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_cluster_expansion">Expanding your cluster</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_decommissioning_brokers">Decommissioning brokers</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_increase_replication_factor">Increasing replication factor</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#rep-throttle">Limiting Bandwidth Usage during Data Migration</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#quotas">Setting quotas</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#datacenters">6.2 Datacenters</a></li>
            <li><a href="http://localhost:8080/35/documentation/operations#georeplication">6.3 Geo-Replication (Cross-Cluster Data Mirroring)</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-overview">Geo-Replication Overview</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-flows">What Are Replication Flows</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-mirrormaker">Configuring Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-starting">Starting Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-stopping">Stopping Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-apply-config-changes">Applying Configuration Changes</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-monitoring">Monitoring Geo-Replication</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#multitenancy">6.4 Multi-Tenancy</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-overview">Multi-Tenancy Overview</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-topic-naming">Creating User Spaces (Namespaces)</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-topic-configs">Configuring Topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-security">Securing Clusters and Topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-isolation">Isolating Tenants</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-monitoring">Monitoring and Metering</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-georeplication">Multi-Tenancy and Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-more">Further considerations</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#config">6.5 Important Configs</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#clientconfig">Important Client Configs</a>
                    <li><a href="http://localhost:8080/35/documentation/operations#prodconfig">A Production Server Configs</a>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#java">6.6 Java Version</a>
            <li><a href="http://localhost:8080/35/documentation/operations#hwandos">6.7 Hardware and OS</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#os">OS</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#diskandfs">Disks and Filesystems</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#appvsosflush">Application vs OS Flush Management</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#linuxflush">Linux Flush Behavior</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#filesystems">Filesystem Selection</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#replace_disk">Replace KRaft Controller Disk</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#monitoring">6.8 Monitoring</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#remote_jmx">Security Considerations for Remote Monitoring using JMX</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_monitoring">KRaft Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#selector_monitoring">Selector Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#common_node_monitoring">Common Node Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#producer_monitoring">Producer Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#consumer_monitoring">Consumer Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#connect_monitoring">Connect Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kafka_streams_monitoring">Streams Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#others_monitoring">Others</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#zk">6.9 ZooKeeper</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#zkversion">Stable Version</a>
                    <li><a href="http://localhost:8080/35/documentation/operations#zkops">Operationalization</a>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#kraft">6.10 KRaft</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_config">Configuration</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_storage">Storage Tool</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_debug">Debugging</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_deployment">Deploying Considerations</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_missing">Missing Features</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_zk_migration">ZooKeeper to KRaft Migration</a></li>
                </ul>
            </li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/security">7. Security</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/security#security_overview">7.1 Security Overview</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#listener_configuration">7.2 Listener Configuration</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_ssl">7.3 Encryption and Authentication using SSL</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_sasl">7.4 Authentication using SASL</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_authz">7.5 Authorization and ACLs</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_rolling_upgrade">7.6 Incorporating Security Features in a Running Cluster</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#zk_authz">7.7 ZooKeeper Authentication</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new">New Clusters</a>
                        <ul>
                            <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new_sasl">ZooKeeper SASL Authentication</a></li>
                            <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new_mtls">ZooKeeper Mutual TLS Authentication</a></li>
                        </ul>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_migration">Migrating Clusters</a></li>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_ensemble">Migrating the ZooKeeper Ensemble</a></li>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_quorum">ZooKeeper Quorum Mutual TLS Authentication</a></li>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/security#zk_encryption">7.8 ZooKeeper Encryption</a></li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/connect">8. Kafka Connect</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_overview">8.1 Overview</a></li>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_user">8.2 User Guide</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_running">Running Kafka Connect</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_configuring">Configuring Connectors</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_transforms">Transformations</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_rest">REST API</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_errorreporting">Error Reporting in Connect</a></li>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_development">8.3 Connector Development Guide</a></li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/streams">9. Kafka Streams</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/streams/quickstart">9.1 Play with a Streams Application</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/tutorial">9.2 Write your own Streams Applications</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/developer-guide">9.3 Developer Manual</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/core-concepts">9.4 Core Concepts</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/architecture">9.5 Architecture</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/upgrade-guide">9.6 Upgrade Guide</a></li>
        </ul>
    </li>
</ul>

        </div>
        <div class="right">
            <article class="markdown"><h1 id="security">
  Security
  <a class="anchor" href="#security">#</a>
</h1>
<h2 id="security_overview">
  7.1 Security Overview
  <a class="anchor" href="#security_overview">#</a>
</h2>
<p>In release 0.9.0.0, the Kafka community added a number of features that,
used either separately or together, increases security in a Kafka
cluster. The following security measures are currently supported:</p>
<ol>
<li>Authentication of connections to brokers from clients (producers and
consumers), other brokers and tools, using either SSL or SASL. Kafka
supports the following SASL mechanisms:
<ul>
<li>SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0</li>
<li>SASL/PLAIN - starting at version 0.10.0.0</li>
<li>SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version
0.10.2.0</li>
<li>SASL/OAUTHBEARER - starting at version 2.0</li>
</ul>
</li>
<li>Authentication of connections from brokers to ZooKeeper</li>
<li>Encryption of data transferred between brokers and clients, between
brokers, or between brokers and tools using SSL (Note that there is
a performance degradation when SSL is enabled, the magnitude of
which depends on the CPU type and the JVM implementation.)</li>
<li>Authorization of read / write operations by clients</li>
<li>Authorization is pluggable and integration with external
authorization services is supported</li>
</ol>
<p>It's worth noting that security is optional - non-secured clusters are
supported, as well as a mix of authenticated, unauthenticated, encrypted
and non-encrypted clients. The guides below explain how to configure and
use the security features in both clients and brokers.</p>
<h2 id="listener_configuration">
  7.2 Listener Configuration
  <a class="anchor" href="#listener_configuration">#</a>
</h2>
<p>In order to secure a Kafka cluster, it is necessary to secure the
channels that are used to communicate with the servers. Each server must
define the set of listeners that are used to receive requests from
clients as well as other servers. Each listener may be configured to
authenticate clients using various mechanisms and to ensure traffic
between the server and the client is encrypted. This section provides a
primer for the configuration of listeners.</p>
<p>Kafka servers support listening for connections on multiple ports. This
is configured through the <code>listeners</code> property in the server
configuration, which accepts a comma-separated list of the listeners to
enable. At least one listener must be defined on each server. The format
of each listener defined in <code>listeners</code> is given below:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">{LISTENER_NAME}://{hostname}:{port}
</code></pre><p>The <code>LISTENER_NAME</code> is usually a descriptive name which defines the
purpose of the listener. For example, many configurations use a separate
listener for client traffic, so they might refer to the corresponding
listener as <code>CLIENT</code> in the configuration:</p>
<p><code>listeners=CLIENT://localhost:9092</code>{.language-text}</p>
<p>The security protocol of each listener is defined in a separate
configuration: <code>listener.security.protocol.map</code>. The value is a
comma-separated list of each listener mapped to its security protocol.
For example, the follow value configuration specifies that the <code>CLIENT</code>
listener will use SSL while the <code>BROKER</code> listener will use plaintext.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listener.security.protocol.map=CLIENT:SSL,BROKER:PLAINTEXT
</code></pre><p>Possible options for the security protocol are given below:</p>
<ol>
<li>PLAINTEXT</li>
<li>SSL</li>
<li>SASL_PLAINTEXT</li>
<li>SASL_SSL</li>
</ol>
<p>The plaintext protocol provides no security and does not require any
additional configuration. In the following sections, this document
covers how to configure the remaining protocols.</p>
<p>If each required listener uses a separate security protocol, it is also
possible to use the security protocol name as the listener name in
<code>listeners</code>. Using the example above, we could skip the definition of
the <code>CLIENT</code> and <code>BROKER</code> listeners using the following definition:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SSL://localhost:9092,PLAINTEXT://localhost:9093
</code></pre><p>However, we recommend users to provide explicit names for the listeners
since it makes the intended usage of each listener clearer.</p>
<p>Among the listeners in this list, it is possible to declare the listener
to be used for inter-broker communication by setting the
<code>inter.broker.listener.name</code> configuration to the name of the listener.
The primary purpose of the inter-broker listener is partition
replication. If not defined, then the inter-broker listener is
determined by the security protocol defined by
<code>security.inter.broker.protocol</code>, which defaults to <code>PLAINTEXT</code>.</p>
<p>For legacy clusters which rely on Zookeeper to store cluster metadata,
it is possible to declare a separate listener to be used for metadata
propagation from the active controller to the brokers. This is defined
by <code>control.plane.listener.name</code>. The active controller will use this
listener when it needs to push metadata updates to the brokers in the
cluster. The benefit of using a control plane listener is that it uses a
separate processing thread, which makes it less likely for application
traffic to impede timely propagation of metadata changes (such as
partition leader and ISR updates). Note that the default value is null,
which means that the controller will use the same listener defined by
<code>inter.broker.listener</code></p>
<p>In a KRaft cluster, a broker is any server which has the <code>broker</code> role
enabled in <code>process.roles</code> and a controller is any server which has the
<code>controller</code> role enabled. Listener configuration depends on the role.
The listener defined by <code>inter.broker.listener.name</code> is used exclusively
for requests between brokers. Controllers, on the other hand, must use
separate listener which is defined by the <code>controller.listener.names</code>
configuration. This cannot be set to the same value as the inter-broker
listener.</p>
<p>Controllers receive requests both from other controllers and from
brokers. For this reason, even if a server does not have the
<code>controller</code> role enabled (i.e. it is just a broker), it must still
define the controller listener along with any security properties that
are needed to configure it. For example, we might use the following
configuration on a standalone broker:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">process.roles=broker
listeners=BROKER://localhost:9092
inter.broker.listener.name=BROKER
controller.quorum.voters=0@localhost:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=BROKER:SASL_SSL,CONTROLLER:SASL_SSL
</code></pre><p>The controller listener is still configured in this example to use the
<code>SASL_SSL</code> security protocol, but it is not included in <code>listeners</code>
since the broker does not expose the controller listener itself. The
port that will be used in this case comes from the
<code>controller.quorum.voters</code> configuration, which defines the complete
list of controllers.</p>
<p>For KRaft servers which have both the broker and controller role
enabled, the configuration is similar. The only difference is that the
controller listener must be included in <code>listeners</code>:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">process.roles=broker,controller
listeners=BROKER://localhost:9092,CONTROLLER://localhost:9093
inter.broker.listener.name=BROKER
controller.quorum.voters=0@localhost:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=BROKER:SASL_SSL,CONTROLLER:SASL_SSL
</code></pre><p>It is a requirement for the port defined in <code>controller.quorum.voters</code>
to exactly match one of the exposed controller listeners. For example,
here the <code>CONTROLLER</code> listener is bound to port 9093. The connection
string defined by <code>controller.quorum.voters</code> must then also use port
9093, as it does here.</p>
<p>The controller will accept requests on all listeners defined by
<code>controller.listener.names</code>. Typically there would be just one
controller listener, but it is possible to have more. For example, this
provides a way to change the active listener from one port or security
protocol to another through a roll of the cluster (one roll to expose
the new listener, and one roll to remove the old listener). When
multiple controller listeners are defined, the first one in the list
will be used for outbound requests.</p>
<p>It is conventional in Kafka to use a separate listener for clients. This
allows the inter-cluster listeners to be isolated at the network level.
In the case of the controller listener in KRaft, the listener should be
isolated since clients do not work with it anyway. Clients are expected
to connect to any other listener configured on a broker. Any requests
that are bound for the controller will be forwarded as described
<a href="#kraft_principal_forwarding">below</a></p>
<p>In the following <a href="#security_ssl">section</a>, this document covers how to
enable SSL on a listener for encryption as well as authentication. The
subsequent <a href="#security_sasl">section</a> will then cover additional
authentication mechanisms using SASL.</p>
<h2 id="security_ssl">
  7.3 Encryption and Authentication using SSL
  <a class="anchor" href="#security_ssl">#</a>
</h2>
<p>Apache Kafka allows clients to use SSL for encryption of traffic as well
as authentication. By default, SSL is disabled but can be turned on if
needed. The following paragraphs explain in detail how to set up your
own PKI infrastructure, use it to create certificates and configure
Kafka to use these.</p>
<h3 id="security_ssl_key">
  1. Generate SSL key and certificate for each Kafka broker
  <a class="anchor" href="#security_ssl_key">#</a>
</h3>
<p>The first step of deploying one or more brokers with SSL support is
to generate a public/private keypair for every server. Since Kafka
expects all keys and certificates to be stored in keystores we will
use Java's keytool command for this task. The tool supports two
different keystore formats, the Java specific jks format which has
been deprecated by now, as well as PKCS12. PKCS12 is the default
format as of Java version 9, to ensure this format is being used
regardless of the Java version in use all following commands
explicitly specify the PKCS12 format.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore {keystorefile} -alias localhost -validity {validity} -genkey -keyalg RSA -storetype pkcs12
</code></pre><p>You need to specify two parameters in the above command:</p>
<ol>
<li>keystorefile: the keystore file that stores the keys (and later
the certificate) for this broker. The keystore file contains the
private and public keys of this broker, therefore it needs to be
kept safe. Ideally this step is run on the Kafka broker that the
key will be used on, as this key should never be
transmitted/leave the server that it is intended for.</li>
<li>validity: the valid time of the key in days. Please note that
this differs from the validity period for the certificate, which
will be determined in <a href="#security_ssl_signing">Signing the certificate</a>.
You can use the same key to request multiple certificates: if your key has a validity of 10
years, but your CA will only sign certificates that are valid
for one year, you can use the same key with 10 certificates over
time.</li>
</ol>
<p>To obtain a certificate that can be used with the private key that
was just created a certificate signing request needs to be created.
This signing request, when signed by a trusted CA results in the
actual certificate which can then be installed in the keystore and
used for authentication purposes.<br>
To generate certificate signing requests run the following command
for all server keystores created so far.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore server.keystore.jks -alias localhost -validity {validity} -genkey -keyalg RSA -destkeystoretype pkcs12 -ext SAN=DNS:{FQDN},IP:{IPADDRESS1}
</code></pre><p>This command assumes that you want to add hostname information to
the certificate, if this is not the case, you can omit the extension
parameter <code>-ext SAN=DNS:{FQDN},IP:{IPADDRESS1}</code>. Please see below
for more information on this.</p>
<h4 id="host-name-verification">
  Host Name Verification
  <a class="anchor" href="#host-name-verification">#</a>
</h4>
<p>Host name verification, when enabled, is the process of checking
attributes from the certificate that is presented by the server you
are connecting to against the actual hostname or ip address of that
server to ensure that you are indeed connecting to the correct
server.<br>
The main reason for this check is to prevent man-in-the-middle
attacks. For Kafka, this check has been disabled by default for a
long time, but as of Kafka 2.0.0 host name verification of servers
is enabled by default for client connections as well as inter-broker
connections.<br>
Server host name verification may be disabled by setting
<code>ssl.endpoint.identification.algorithm</code> to an empty string.<br>
For dynamically configured broker listeners, hostname verification
may be disabled using <code>kafka-configs.sh</code>:\</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-configs.sh --bootstrap-server localhost:9093 --entity-type brokers --entity-name 0 --alter --add-config &#34;listener.name.internal.ssl.endpoint.identification.algorithm=&#34;
</code></pre><p><strong>Note:</strong></p>
<p>Normally there is no good reason to disable hostname verification
apart from being the quickest way to &quot;just get it to work&quot;
followed by the promise to &quot;fix it later when there is more
time&quot;!<br>
Getting hostname verification right is not that hard when done at
the right time, but gets much harder once the cluster is up and
running - do yourself a favor and do it now!</p>
<p>If host name verification is enabled, clients will verify the
server's fully qualified domain name (FQDN) or ip address against
one of the following two fields:</p>
<ol>
<li>Common Name (CN)</li>
<li><a href="https://tools.ietf.org/html/rfc5280#section-4.2.1.6">Subject Alternative Name (SAN)</a></li>
</ol>
<p>While Kafka checks both fields, usage of the common name field for
hostname verification has been
<a href="https://tools.ietf.org/html/rfc2818#section-3.1">deprecated</a> since
2000 and should be avoided if possible. In addition the SAN field is
much more flexible, allowing for multiple DNS and IP entries to be
declared in a certificate.<br>
Another advantage is that if the SAN field is used for hostname
verification the common name can be set to a more meaningful value
for authorization purposes. Since we need the SAN field to be
contained in the signed certificate, it will be specified when
generating the signing request. It can also be specified when
generating the keypair, but this will not automatically be copied
into the signing request.<br>
To add a SAN field append the following argument
<code>-ext SAN=DNS:{FQDN},IP:{IPADDRESS}</code> to the keytool command:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore server.keystore.jks -alias localhost -validity {validity} -genkey -keyalg RSA -destkeystoretype pkcs12 -ext SAN=DNS:{FQDN},IP:{IPADDRESS1}
</code></pre><h3 id="security_ssl_ca">
  2. Creating your own CA
  <a class="anchor" href="#security_ssl_ca">#</a>
</h3>
<p>After this step each machine in the cluster has a public/private key
pair which can already be used to encrypt traffic and a certificate
signing request, which is the basis for creating a certificate. To
add authentication capabilities this signing request needs to be
signed by a trusted authority, which will be created in this step.</p>
<p>A certificate authority (CA) is responsible for signing
certificates. CAs works likes a government that issues passports -
the government stamps (signs) each passport so that the passport
becomes difficult to forge. Other governments verify the stamps to
ensure the passport is authentic. Similarly, the CA signs the
certificates, and the cryptography guarantees that a signed
certificate is computationally difficult to forge. Thus, as long as
the CA is a genuine and trusted authority, the clients have a strong
assurance that they are connecting to the authentic machines.</p>
<p>For this guide we will be our own Certificate Authority. When
setting up a production cluster in a corporate environment these
certificates would usually be signed by a corporate CA that is
trusted throughout the company. Please see
<a href="#security_ssl_production">Common Pitfalls in Production</a> for some things to consider
for this case.</p>
<p>Due to a
<a href="https://www.openssl.org/docs/man1.1.1/man1/x509.html#BUGS">bug</a> in
OpenSSL, the x509 module will not copy requested extension fields
from CSRs into the final certificate. Since we want the SAN
extension to be present in our certificate to enable hostname
verification, we'll use the <em>ca</em> module instead. This requires some
additional configuration to be in place before we generate our CA
keypair.<br>
Save the following listing into a file called openssl-ca.cnf and
adjust the values for validity and common attributes as necessary.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">HOME            = .
RANDFILE        = $ENV::HOME/.rnd

###################################################################
[ ca ]
default_ca    = CA_default      # The default ca section

[ CA_default ]

base_dir      = .
certificate   = $base_dir/cacert.pem   # The CA certifcate
private_key   = $base_dir/cakey.pem    # The CA private key
new_certs_dir = $base_dir              # Location for new certs after signing
database      = $base_dir/index.txt    # Database index file
serial        = $base_dir/serial.txt   # The current serial number

default_days     = 1000         # How long to certify for
default_crl_days = 30           # How long before next CRL
default_md       = sha256       # Use public key default MD
preserve         = no           # Keep passed DN ordering

x509_extensions = ca_extensions # The extensions to add to the cert

email_in_dn     = no            # Don&#39;t concat the email in the DN
copy_extensions = copy          # Required to copy SANs from CSR to cert

###################################################################
[ req ]
default_bits       = 4096
default_keyfile    = cakey.pem
distinguished_name = ca_distinguished_name
x509_extensions    = ca_extensions
string_mask        = utf8only

###################################################################
[ ca_distinguished_name ]
countryName         = Country Name (2 letter code)
countryName_default = DE

stateOrProvinceName         = State or Province Name (full name)
stateOrProvinceName_default = Test Province

localityName                = Locality Name (eg, city)
localityName_default        = Test Town

organizationName            = Organization Name (eg, company)
organizationName_default    = Test Company

organizationalUnitName         = Organizational Unit (eg, division)
organizationalUnitName_default = Test Unit

commonName         = Common Name (e.g. server FQDN or YOUR name)
commonName_default = Test Name

emailAddress         = Email Address
emailAddress_default = test@test.com

###################################################################
[ ca_extensions ]

subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid:always, issuer
basicConstraints       = critical, CA:true
keyUsage               = keyCertSign, cRLSign

###################################################################
[ signing_policy ]
countryName            = optional
stateOrProvinceName    = optional
localityName           = optional
organizationName       = optional
organizationalUnitName = optional
commonName             = supplied
emailAddress           = optional

###################################################################
[ signing_req ]
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid,issuer
basicConstraints       = CA:FALSE
keyUsage               = digitalSignature, keyEncipherment
</code></pre><p>Then create a database and serial number file, these will be used to
keep track of which certificates were signed with this CA. Both of
these are simply text files that reside in the same directory as
your CA keys.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; echo 01 &gt; serial.txt
&gt; touch index.txt
</code></pre><p>With these steps done you are now ready to generate your CA that
will be used to sign certificates later.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; openssl req -x509 -config openssl-ca.cnf -newkey rsa:4096 -sha256 -nodes -out cacert.pem -outform PEM
</code></pre><p>The CA is simply a public/private key pair and certificate that is
signed by itself, and is only intended to sign other certificates.<br>
This keypair should be kept very safe, if someone gains access to
it, they can create and sign certificates that will be trusted by
your infrastructure, which means they will be able to impersonate
anybody when connecting to any service that trusts this CA.<br>
The next step is to add the generated CA to the **clients'
truststore** so that the clients can trust this CA:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert
</code></pre><p><strong>Note:</strong> If you configure the Kafka brokers to require client
authentication by setting ssl.client.auth to be &quot;requested&quot; or
&quot;required&quot; in the <a href="#brokerconfigs">Kafka brokers config</a> then you
must provide a truststore for the Kafka brokers as well and it
should have all the CA certificates that clients' keys were signed
by.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert
</code></pre><p>In contrast to the keystore in step 1 that stores each machine's
own identity, the truststore of a client stores all the certificates
that the client should trust. Importing a certificate into one's
truststore also means trusting all certificates that are signed by
that certificate. As the analogy above, trusting the government (CA)
also means trusting all passports (certificates) that it has issued.
This attribute is called the chain of trust, and it is particularly
useful when deploying SSL on a large Kafka cluster. You can sign all
certificates in the cluster with a single CA, and have all machines
share the same truststore that trusts the CA. That way all machines
can authenticate all other machines.</p>
<h3 id="security_ssl_signing">
  3. Signing the certificate
  <a class="anchor" href="#security_ssl_signing">#</a>
</h3>
<p>Then sign it with the CA:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; openssl ca -config openssl-ca.cnf -policy signing_policy -extensions signing_req -out {server certificate} -infiles {certificate signing request}
</code></pre><p>Finally, you need to import both the certificate of the CA and the
signed certificate into the keystore:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; keytool -keystore {keystore} -alias CARoot -import -file {CA certificate}
&gt; keytool -keystore {keystore} -alias localhost -import -file cert-signed
</code></pre><p>The definitions of the parameters are the following:</p>
<ol>
<li>keystore: the location of the keystore</li>
<li>CA certificate: the certificate of the CA</li>
<li>certificate signing request: the csr created with the server key</li>
<li>server certificate: the file to write the signed certificate of
the server to</li>
</ol>
<p>This will leave you with one truststore called <em>truststore.jks</em> -
this can be the same for all clients and brokers and does not
contain any sensitive information, so there is no need to secure
this.<br>
Additionally you will have one <em>server.keystore.jks</em> file per node
which contains that nodes keys, certificate and your CAs
certificate, please refer to <a href="#security_configbroker">Configuring Kafka
Brokers</a> and <a href="#security_configclients">Configuring Kafka
Clients</a> for information on how to use
these files.</p>
<p>For some tooling assistance on this topic, please check out the
<a href="https://github.com/OpenVPN/easy-rsa">easyRSA</a> project which has
extensive scripting in place to help with these steps.</p>
<h4 id="ssl-key-and-certificates-in-pem-format">
  SSL key and certificates in PEM format
  <a class="anchor" href="#ssl-key-and-certificates-in-pem-format">#</a>
</h4>
<p>From 2.7.0 onwards, SSL key and trust stores can be configured for
Kafka brokers and clients directly in the configuration in PEM
format. This avoids the need to store separate files on the file
system and benefits from password protection features of Kafka
configuration. PEM may also be used as the store type for file-based
key and trust stores in addition to JKS and PKCS12. To configure PEM
key store directly in the broker or client configuration, private
key in PEM format should be provided in <code>ssl.keystore.key</code> and the
certificate chain in PEM format should be provided in
<code>ssl.keystore.certificate.chain</code>. To configure trust store, trust
certificates, e.g. public certificate of CA, should be provided in
<code>ssl.truststore.certificates</code>. Since PEM is typically stored as
multi-line base-64 strings, the configuration value can be included
in Kafka configuration as multi-line strings with lines terminating
in backslash ('\') for line continuation.</p>
<p>Store password configs <code>ssl.keystore.password</code> and
<code>ssl.truststore.password</code> are not used for PEM. If private key is
encrypted using a password, the key password must be provided in
<code>ssl.key.password</code>. Private keys may be provided in unencrypted form
without a password. In production deployments, configs should be
encrypted or externalized using password protection feature in Kafka
in this case. Note that the default SSL engine factory has limited
capabilities for decryption of encrypted private keys when external
tools like OpenSSL are used for encryption. Third party libraries
like BouncyCastle may be integrated witn a custom <code>SslEngineFactory</code>
to support a wider range of encrypted private keys.</p>
<h3 id="security_ssl_production">
  4. Common Pitfalls in Production
  <a class="anchor" href="#security_ssl_production">#</a>
</h3>
<p>The above paragraphs show the process to create your own CA and use
it to sign certificates for your cluster. While very useful for
sandbox, dev, test, and similar systems, this is usually not the
correct process to create certificates for a production cluster in a
corporate environment. Enterprises will normally operate their own
CA and users can send in CSRs to be signed with this CA, which has
the benefit of users not being responsible to keep the CA secure as
well as a central authority that everybody can trust. However it
also takes away a lot of control over the process of signing
certificates from the user. Quite often the persons operating
corporate CAs will apply tight restrictions on certificates that can
cause issues when trying to use these certificates with Kafka.</p>
<ol>
<li>
<p><strong><a href="https://tools.ietf.org/html/rfc5280#section-4.2.1.12">Extended Key Usage</a></strong><br>
Certificates may contain an extension field that controls the
purpose for which the certificate can be used. If this field is
empty, there are no restricitions on the usage, but if any usage
is specified in here, valid SSL implementations have to enforce
these usages.<br>
Relevant usages for Kafka are:</p>
<ul>
<li>Client authentication</li>
<li>Server authentication</li>
</ul>
<p>Kafka brokers need both these usages to be allowed, as for
intra-cluster communication every broker will behave as both the
client and the server towards other brokers. It is not uncommon
for corporate CAs to have a signing profile for webservers and
use this for Kafka as well, which will only contain the
<em>serverAuth</em> usage value and cause the SSL handshake to fail.</p>
</li>
<li>
<p><strong>Intermediate Certificates</strong><br>
Corporate Root CAs are often kept offline for security reasons.
To enable day-to-day usage, so called intermediate CAs are
created, which are then used to sign the final certificates.
When importing a certificate into the keystore that was signed
by an intermediate CA it is necessarry to provide the entire
chain of trust up to the root CA. This can be done by simply
<em>cat</em>ing the certificate files into one combined certificate
file and then importing this with keytool.</p>
</li>
<li>
<p><strong>Failure to copy extension fields</strong><br>
CA operators are often hesitant to copy and requested extension
fields from CSRs and prefer to specify these themselves as this
makes it harder for a malicious party to obtain certificates
with potentially misleading or fraudulent values. It is
adviseable to double check signed certificates, whether these
contain all requested SAN fields to enable proper hostname
verification. The following command can be used to print
certificate details to the console, which should be compared
with what was originally requested:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; openssl x509 -in certificate.crt -text -noout
</code></pre></li>
</ol>
<h3 id="security_configbroker">
  5. Configuring Kafka Brokers
  <a class="anchor" href="#security_configbroker">#</a>
</h3>
<p>If SSL is not enabled for inter-broker communication (see below for
how to enable it), both PLAINTEXT and SSL ports will be necessary.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=PLAINTEXT://host.name:port,SSL://host.name:port
</code></pre><p>Following SSL configs are needed on the broker side</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">ssl.keystore.location=/var/private/ssl/server.keystore.jks
ssl.keystore.password=test1234
ssl.key.password=test1234
ssl.truststore.location=/var/private/ssl/server.truststore.jks
ssl.truststore.password=test1234
</code></pre><p>Note: ssl.truststore.password is technically optional but highly
recommended. If a password is not set access to the truststore is
still available, but integrity checking is disabled. Optional
settings that are worth considering:</p>
<ol>
<li>ssl.client.auth=none (&quot;required&quot; =&gt; client authentication is
required, &quot;requested&quot; =&gt; client authentication is requested
and client without certs can still connect. The usage of
&quot;requested&quot; is discouraged as it provides a false sense of
security and misconfigured clients will still connect
successfully.)</li>
<li>ssl.cipher.suites (Optional). A cipher suite is a named
combination of authentication, encryption, MAC and key exchange
algorithm used to negotiate the security settings for a network
connection using TLS or SSL network protocol. (Default is an
empty list)</li>
<li>ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1 (list out the SSL
protocols that you are going to accept from clients. Do note
that SSL is deprecated in favor of TLS and using SSL in
production is not recommended)</li>
<li>ssl.keystore.type=JKS</li>
<li>ssl.truststore.type=JKS</li>
<li>ssl.secure.random.implementation=SHA1PRNG</li>
</ol>
<p>If you want to enable SSL for inter-broker communication, add the
following to the server.properties file (it defaults to PLAINTEXT)</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.inter.broker.protocol=SSL
</code></pre><p>Due to import regulations in some countries, the Oracle
implementation limits the strength of cryptographic algorithms
available by default. If stronger algorithms are needed (for
example, AES with 256-bit keys), the <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">JCE Unlimited Strength
Jurisdiction Policy
Files</a>
must be obtained and installed in the JDK/JRE. See the <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html">JCA
Providers
Documentation</a>
for more information.</p>
<p>The JRE/JDK will have a default pseudo-random number generator
(PRNG) that is used for cryptography operations, so it is not
required to configure the implementation used with the
<code>ssl.secure.random.implementation</code>. However, there are performance
issues with some implementations (notably, the default chosen on
Linux systems, <code>NativePRNG</code>, utilizes a global lock). In cases where
performance of SSL connections becomes an issue, consider explicitly
setting the implementation to be used. The <code>SHA1PRNG</code> implementation
is non-blocking, and has shown very good performance characteristics
under heavy load (50 MB/sec of produced messages, plus replication
traffic, per-broker).</p>
<p>Once you start the broker you should be able to see in the
server.log</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">with addresses: PLAINTEXT -&gt; EndPoint(192.168.64.1,9092,PLAINTEXT),SSL -&gt; EndPoint(192.168.64.1,9093,SSL)
</code></pre><p>To check quickly if the server keystore and truststore are setup
properly you can run the following command</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; openssl s_client -debug -connect localhost:9093 -tls1
</code></pre><p>(Note: TLSv1 should be listed under ssl.enabled.protocols)<br>
In the output of this command you should see server's certificate:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-----BEGIN CERTIFICATE-----
{variable sized random bytes}
-----END CERTIFICATE-----
subject=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=Sriharsha Chintalapani
issuer=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=kafka/emailAddress=test@test.com
</code></pre><p>If the certificate does not show up or if there are any other error
messages then your keystore is not setup properly.</p>
<h3 id="security_configclients">
  6. Configuring Kafka Clients
  <a class="anchor" href="#security_configclients">#</a>
</h3>
<p>SSL is supported only for the new Kafka Producer and Consumer, the
older API is not supported. The configs for SSL will be the same for
both producer and consumer.<br>
If client authentication is not required in the broker, then the
following is a minimal configuration example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.protocol=SSL
ssl.truststore.location=/var/private/ssl/client.truststore.jks
ssl.truststore.password=test1234
</code></pre><p>Note: ssl.truststore.password is technically optional but highly
recommended. If a password is not set access to the truststore is
still available, but integrity checking is disabled. If client
authentication is required, then a keystore must be created like in
step 1 and the following must also be configured:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">ssl.keystore.location=/var/private/ssl/client.keystore.jks
ssl.keystore.password=test1234
ssl.key.password=test1234
</code></pre><p>Other configuration settings that may also be needed depending on
our requirements and the broker configuration:</p>
<ol>
<li>ssl.provider (Optional). The name of the security provider used
for SSL connections. Default value is the default security
provider of the JVM.</li>
<li>ssl.cipher.suites (Optional). A cipher suite is a named
combination of authentication, encryption, MAC and key exchange
algorithm used to negotiate the security settings for a network
connection using TLS or SSL network protocol.</li>
<li>ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1. It should list at
least one of the protocols configured on the broker side</li>
<li>ssl.truststore.type=JKS</li>
<li>ssl.keystore.type=JKS</li>
</ol>
<p>Examples using console-producer and console-consumer:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; kafka-console-producer.sh --bootstrap-server localhost:9093 --topic test --producer.config client-ssl.properties
&gt; kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test --consumer.config client-ssl.properties
</code></pre><h2 id="security_sasl">
  7.4 Authentication using SASL
  <a class="anchor" href="#security_sasl">#</a>
</h2>
<h3 id="security_sasl_jaasconfig">
  1. JAAS configuration
  <a class="anchor" href="#security_sasl_jaasconfig">#</a>
</h3>
<p>Kafka uses the Java Authentication and Authorization Service
(<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jaas/JAASRefGuide.html">JAAS</a>)
for SASL configuration.</p>
<h4 id="security_jaas_broker">
  1. JAAS configuration for Kafka brokers
  <a class="anchor" href="#security_jaas_broker">#</a>
</h4>
<p><code>KafkaServer</code> is the section name in the JAAS file used by each
KafkaServer/Broker. This section provides SASL configuration
options for the broker including any SASL client connections
made by the broker for inter-broker communication. If multiple
listeners are configured to use SASL, the section name may be
prefixed with the listener name in lower-case followed by a
period, e.g. <code>sasl_ssl.KafkaServer</code>.</p>
<p><code>Client</code> section is used to authenticate a SASL connection with
zookeeper. It also allows the brokers to set SASL ACL on
zookeeper nodes which locks these nodes down so that only the
brokers can modify it. It is necessary to have the same
principal name across all brokers. If you want to use a section
name other than Client, set the system property
<code>zookeeper.sasl.clientconfig</code> to the appropriate name (<em>e.g.</em>,
<code>-Dzookeeper.sasl.clientconfig=ZkClient</code>).</p>
<p>ZooKeeper uses &quot;zookeeper&quot; as the service name by default. If
you want to change this, set the system property
<code>zookeeper.sasl.client.username</code> to the appropriate name
(<em>e.g.</em>, <code>-Dzookeeper.sasl.client.username=zk</code>).</p>
<p>Brokers may also configure JAAS using the broker configuration
property <code>sasl.jaas.config</code>. The property name must be prefixed
with the listener prefix including the SASL mechanism, i.e.
<code>listener.name.{listenerName}.{saslMechanism}.sasl.jaas.config</code>.
Only one login module may be specified in the config value. If
multiple mechanisms are configured on a listener, configs must
be provided for each mechanism using the listener and mechanism
prefix. For example,</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username=&#34;admin&#34; \
    password=&#34;admin-secret&#34;;
listener.name.sasl_ssl.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username=&#34;admin&#34; \
    password=&#34;admin-secret&#34; \
    user_admin=&#34;admin-secret&#34; \
    user_alice=&#34;alice-secret&#34;;
</code></pre><p>If JAAS configuration is defined at different levels, the order
of precedence used is:</p>
<ul>
<li>Broker configuration property
<code>listener.name.{listenerName}.{saslMechanism}.sasl.jaas.config</code></li>
<li><code>{listenerName}.KafkaServer</code> section of static JAAS
configuration</li>
<li><code>KafkaServer</code> section of static JAAS configuration</li>
</ul>
<p>Note that ZooKeeper JAAS config may only be configured using
static JAAS configuration.</p>
<p>See <a href="#security_sasl_kerberos_brokerconfig">GSSAPI (Kerberos)</a>,
<a href="#security_sasl_plain_brokerconfig">PLAIN</a>,
<a href="#security_sasl_scram_brokerconfig">SCRAM</a> or
<a href="#security_sasl_oauthbearer_brokerconfig">OAUTHBEARER</a> for
example broker configurations.</p>
<h4 id="security_jaas_client">
  2. JAAS configuration for Kafka clients
  <a class="anchor" href="#security_jaas_client">#</a>
</h4>
<p>Clients may configure JAAS using the client configuration
property <a href="#security_client_dynamicjaas">sasl.jaas.config</a> or
using the <a href="#security_client_staticjaas">static JAAS config file</a>
similar to brokers.</p>
<h5 id="security_client_dynamicjaas">
  1. JAAS configuration using client configuration property
  <a class="anchor" href="#security_client_dynamicjaas">#</a>
</h5>
<p>Clients may specify JAAS configuration as a producer or
consumer property without creating a physical configuration
file. This mode also enables different producers and
consumers within the same JVM to use different credentials
by specifying different properties for each client. If both
static JAAS configuration system property
<code>java.security.auth.login.config</code> and client property
<code>sasl.jaas.config</code> are specified, the client property will
be used.</p>
<p>See <a href="#security_sasl_kerberos_clientconfig">GSSAPI
(Kerberos)</a>,
<a href="#security_sasl_plain_clientconfig">PLAIN</a>,
<a href="#security_sasl_scram_clientconfig">SCRAM</a> or
<a href="#security_sasl_oauthbearer_clientconfig">OAUTHBEARER</a> for
example configurations.</p>
<h5 id="security_client_staticjaas">
  2. JAAS configuration using static config file
  <a class="anchor" href="#security_client_staticjaas">#</a>
</h5>
<p>To configure SASL authentication on the clients using static
JAAS config file:</p>
<ol>
<li>
<p>Add a JAAS config file with a client login section named
<code>KafkaClient</code>. Configure a login module in <code>KafkaClient</code>
for the selected mechanism as described in the examples
for setting up <a href="#security_sasl_kerberos_clientconfig">GSSAPI (Kerberos)</a>,
<a href="#security_sasl_plain_clientconfig">PLAIN</a>,
<a href="#security_sasl_scram_clientconfig">SCRAM</a> or
<a href="#security_sasl_oauthbearer_clientconfig">OAUTHBEARER</a>.
For example,
<a href="#security_sasl_gssapi_clientconfig">GSSAPI</a> credentials
may be configured as:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab=&#34;/etc/security/keytabs/kafka_client.keytab&#34;
    principal=&#34;kafka-client-1@EXAMPLE.COM&#34;;
};
</code></pre></li>
<li>
<p>Pass the JAAS config file location as JVM parameter to
each client JVM. For example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf
</code></pre></li>
</ol>
<h3 id="security_sasl_config">
  2. SASL configuration
  <a class="anchor" href="#security_sasl_config">#</a>
</h3>
<p>SASL may be used with PLAINTEXT or SSL as the transport layer using
the security protocol SASL_PLAINTEXT or SASL_SSL respectively. If
SASL_SSL is used, then <a href="#security_ssl">SSL must also be configured</a>.</p>
<h4 id="security_sasl_mechanism">
  1. SASL mechanisms
  <a class="anchor" href="#security_sasl_mechanism">#</a>
</h4>
<p>Kafka supports the following SASL mechanisms:</p>
<ul>
<li><a href="#security_sasl_kerberos">GSSAPI</a> (Kerberos)</li>
<li><a href="#security_sasl_plain">PLAIN</a></li>
<li><a href="#security_sasl_scram">SCRAM-SHA-256</a></li>
<li><a href="#security_sasl_scram">SCRAM-SHA-512</a></li>
<li><a href="#security_sasl_oauthbearer">OAUTHBEARER</a></li>
</ul>
<h4 id="security_sasl_brokerconfig">
  2. SASL configuration for Kafka brokers
  <a class="anchor" href="#security_sasl_brokerconfig">#</a>
</h4>
<ol>
<li>
<p>Configure a SASL port in server.properties, by adding at
least one of SASL_PLAINTEXT or SASL_SSL to the <em>listeners</em>
parameter, which contains one or more comma-separated
values:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">listeners=SASL_PLAINTEXT://host.name:port
</span></span></code></pre></div><p>If you are only configuring a SASL port (or if you want the
Kafka brokers to authenticate each other using SASL) then
make sure you set the same SASL protocol for inter-broker
communication:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
</span></span></code></pre></div></li>
<li>
<p>Select one or more <a href="#security_sasl_mechanism">supported
mechanisms</a> to enable in the
broker and follow the steps to configure SASL for the
mechanism. To enable multiple mechanisms in the broker,
follow the steps <a href="#security_sasl_multimechanism">here</a>.</p>
</li>
</ol>
<h4 id="security_sasl_clientconfig">
  3. SASL configuration for Kafka clients
  <a class="anchor" href="#security_sasl_clientconfig">#</a>
</h4>
<p>SASL authentication is only supported for the new Java Kafka
producer and consumer, the older API is not supported.</p>
<p>To configure SASL authentication on the clients, select a SASL
<a href="#security_sasl_mechanism">mechanism</a> that is enabled in the
broker for client authentication and follow the steps to
configure SASL for the selected mechanism.</p>
<p>Note: When establishing connections to brokers via SASL, clients
may perform a reverse DNS lookup of the broker address. Due to
how the JRE implements reverse DNS lookups, clients may observe
slow SASL handshakes if fully qualified domain names are not
used, for both the client's <code>bootstrap.servers</code> and a broker's
<a href="#brokerconfigs_advertised.listeners"><code>advertised.listeners</code></a>.</p>
<h3 id="security_sasl_kerberos">
  3. Authentication using SASL/Kerberos
  <a class="anchor" href="#security_sasl_kerberos">#</a>
</h3>
<h4 id="security_sasl_kerberos_prereq">
  1. Prerequisites
  <a class="anchor" href="#security_sasl_kerberos_prereq">#</a>
</h4>
<ol>
<li>
<p><strong>Kerberos</strong><br>
If your organization is already using a Kerberos server (for
example, by using Active Directory), there is no need to
install a new server just for Kafka. Otherwise you will need
to install one, your Linux vendor likely has packages for
Kerberos and a short guide on how to install and configure
it (<a href="https://help.ubuntu.com/community/Kerberos">Ubuntu</a>,
<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/installing-kerberos.html">Redhat</a>).
Note that if you are using Oracle Java, you will need to
download JCE policy files for your Java version and copy
them to $JAVA_HOME/jre/lib/security.</p>
</li>
<li>
<p><strong>Create Kerberos Principals</strong><br>
If you are using the organization's Kerberos or Active
Directory server, ask your Kerberos administrator for a
principal for each Kafka broker in your cluster and for
every operating system user that will access Kafka with
Kerberos authentication (via clients and tools).<br>
If you have installed your own Kerberos, you will need to
create these principals yourself using the following
commands:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; sudo /usr/sbin/kadmin.local -q &#39;addprinc -randkey kafka/{hostname}@{REALM}&#39;
&gt; sudo /usr/sbin/kadmin.local -q &#34;ktadd -k /etc/security/keytabs/{keytabname}.keytab kafka/{hostname}@{REALM}&#34;
</code></pre></li>
<li>
<p><strong>Make sure all hosts can be reachable using hostnames</strong> -
it is a Kerberos requirement that all your hosts can be
resolved with their FQDNs.</p>
</li>
</ol>
<h4 id="security_sasl_kerberos_brokerconfig">
  2. Configuring Kafka Brokers
  <a class="anchor" href="#security_sasl_kerberos_brokerconfig">#</a>
</h4>
<ol>
<li>
<p>Add a suitably modified JAAS file similar to the one below
to each Kafka broker's config directory, let's call it
kafka_server_jaas.conf for this example (note that each
broker should have its own keytab):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaServer {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab=&#34;/etc/security/keytabs/kafka_server.keytab&#34;
    principal=&#34;kafka/kafka1.hostname.com@EXAMPLE.COM&#34;;
};

// Zookeeper client authentication
Client {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab=&#34;/etc/security/keytabs/kafka_server.keytab&#34;
    principal=&#34;kafka/kafka1.hostname.com@EXAMPLE.COM&#34;;
};
</code></pre><p><code>KafkaServer</code> section in the JAAS file tells the broker
which principal to use and the location of the keytab where
this principal is stored. It allows the broker to login
using the keytab specified in this section. See
<a href="#security_jaas_broker">notes</a> for more details on Zookeeper
SASL configuration.</p>
</li>
<li>
<p>Pass the JAAS and optionally the krb5 file locations as JVM
parameters to each Kafka broker (see
<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html">here</a>
for more details):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.krb5.conf=/etc/kafka/krb5.conf
-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
</code></pre></li>
<li>
<p>Make sure the keytabs configured in the JAAS file are
readable by the operating system user who is starting kafka
broker.</p>
</li>
<li>
<p>Configure SASL port and SASL mechanisms in server.properties
as described <a href="#security_sasl_brokerconfig">here</a>. For
example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SASL_PLAINTEXT://host.name:port
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=GSSAPI
sasl.enabled.mechanisms=GSSAPI
</code></pre><p>We must also configure the service name in
server.properties, which should match the principal name of
the kafka brokers. In the above example, principal is
&quot;kafka/kafka1.hostname.com@EXAMPLE.com&quot;, so:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.kerberos.service.name=kafka
</code></pre></li>
</ol>
<h4 id="security_sasl_kerberos_clientconfig">
  3. Configuring Kafka Clients
  <a class="anchor" href="#security_sasl_kerberos_clientconfig">#</a>
</h4>
<p>To configure SASL authentication on the clients:</p>
<ol>
<li>
<p>Clients (producers, consumers, connect workers, etc) will
authenticate to the cluster with their own principal
(usually with the same name as the user running the client),
so obtain or create these principals as needed. Then
configure the JAAS configuration property for each client.
Different clients within a JVM may run as different users by
specifying different principals. The property
<code>sasl.jaas.config</code> in producer.properties or
consumer.properties describes how clients like producer and
consumer can connect to the Kafka Broker. The following is
an example configuration for a client using a keytab
(recommended for long-running processes):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
    useKeyTab=true \
    storeKey=true  \
    keyTab=&#34;/etc/security/keytabs/kafka_client.keytab&#34; \
    principal=&#34;kafka-client-1@EXAMPLE.COM&#34;;
</code></pre><p>For command-line utilities like kafka-console-consumer or
kafka-console-producer, kinit can be used along with
&quot;useTicketCache=true&quot; as in:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
    useTicketCache=true;
</code></pre><p>JAAS configuration for clients may alternatively be
specified as a JVM parameter similar to brokers as described
<a href="#security_client_staticjaas">here</a>. Clients use the login
section named <code>KafkaClient</code>. This option allows only one
user for all client connections from a JVM.</p>
</li>
<li>
<p>Make sure the keytabs configured in the JAAS configuration
are readable by the operating system user who is starting
kafka client.</p>
</li>
<li>
<p>Optionally pass the krb5 file locations as JVM parameters to
each client JVM (see
<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html">here</a>
for more details):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.krb5.conf=/etc/kafka/krb5.conf
</code></pre></li>
<li>
<p>Configure the following properties in producer.properties or
consumer.properties:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.protocol=SASL_PLAINTEXT (or SASL_SSL)
sasl.mechanism=GSSAPI
sasl.kerberos.service.name=kafka
</code></pre></li>
</ol>
<h3 id="security_sasl_plain">
  4. Authentication using SASL/PLAIN
  <a class="anchor" href="#security_sasl_plain">#</a>
</h3>
<p>SASL/PLAIN is a simple username/password authentication mechanism
that is typically used with TLS for encryption to implement secure
authentication. Kafka supports a default implementation for
SASL/PLAIN which can be extended for production use as described
<a href="#security_sasl_plain_production">here</a>.</p>
<p>Under the default implementation of <code>principal.builder.class</code>, the
username is used as the authenticated <code>Principal</code> for configuration
of ACLs etc.</p>
<h4 id="security_sasl_plain_brokerconfig">
  1. Configuring Kafka Brokers
  <a class="anchor" href="#security_sasl_plain_brokerconfig">#</a>
</h4>
<ol>
<li>
<p>Add a suitably modified JAAS file similar to the one below
to each Kafka broker's config directory, let's call it
kafka_server_jaas.conf for this example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaServer {
    org.apache.kafka.common.security.plain.PlainLoginModule required
    username=&#34;admin&#34;
    password=&#34;admin-secret&#34;
    user_admin=&#34;admin-secret&#34;
    user_alice=&#34;alice-secret&#34;;
};
</code></pre><p>This configuration defines two users (<em>admin</em> and <em>alice</em>).
The properties <code>username</code> and <code>password</code> in the
<code>KafkaServer</code> section are used by the broker to initiate
connections to other brokers. In this example, <em>admin</em> is
the user for inter-broker communication. The set of
properties <code>user_</code><em><code>userName</code></em> defines the passwords for all
users that connect to the broker and the broker validates
all client connections including those from other brokers
using these properties.</p>
</li>
<li>
<p>Pass the JAAS config file location as JVM parameter to each
Kafka broker:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
</code></pre></li>
<li>
<p>Configure SASL port and SASL mechanisms in server.properties
as described <a href="#security_sasl_brokerconfig">here</a>. For
example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SASL_SSL://host.name:port
security.inter.broker.protocol=SASL_SSL
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
</code></pre></li>
</ol>
<h4 id="security_sasl_plain_clientconfig">
  2. Configuring Kafka Clients
  <a class="anchor" href="#security_sasl_plain_clientconfig">#</a>
</h4>
<p>To configure SASL authentication on the clients:</p>
<ol>
<li>
<p>Configure the JAAS configuration property for each client in
producer.properties or consumer.properties. The login module
describes how the clients like producer and consumer can
connect to the Kafka Broker. The following is an example
configuration for a client for the PLAIN mechanism:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username=&#34;alice&#34; \
    password=&#34;alice-secret&#34;;
</code></pre><p>The options <code>username</code> and <code>password</code> are used by clients to
configure the user for client connections. In this example,
clients connect to the broker as user <em>alice</em>. Different
clients within a JVM may connect as different users by
specifying different user names and passwords in
<code>sasl.jaas.config</code>.</p>
<p>JAAS configuration for clients may alternatively be
specified as a JVM parameter similar to brokers as described
<a href="#security_client_staticjaas">here</a>. Clients use the login
section named <code>KafkaClient</code>. This option allows only one
user for all client connections from a JVM.</p>
</li>
<li>
<p>Configure the following properties in producer.properties or
consumer.properties:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.protocol=SASL_SSL
sasl.mechanism=PLAIN
</code></pre></li>
</ol>
<h4 id="security_sasl_plain_production">
  3. Use of SASL/PLAIN in production
  <a class="anchor" href="#security_sasl_plain_production">#</a>
</h4>
<ul>
<li>SASL/PLAIN should be used only with SSL as transport layer
to ensure that clear passwords are not transmitted on the
wire without encryption.</li>
<li>The default implementation of SASL/PLAIN in Kafka specifies
usernames and passwords in the JAAS configuration file as
shown <a href="#security_sasl_plain_brokerconfig">here</a>. From Kafka
version 2.0 onwards, you can avoid storing clear passwords
on disk by configuring your own callback handlers that
obtain username and password from an external source using
the configuration options
<code>sasl.server.callback.handler.class</code> and
<code>sasl.client.callback.handler.class</code>.</li>
<li>In production systems, external authentication servers may
implement password authentication. From Kafka version 2.0
onwards, you can plug in your own callback handlers that use
external authentication servers for password verification by
configuring <code>sasl.server.callback.handler.class</code>.</li>
</ul>
<h3 id="security_sasl_scram">
  5. Authentication using SASL/SCRAM
  <a class="anchor" href="#security_sasl_scram">#</a>
</h3>
<p>Salted Challenge Response Authentication Mechanism (SCRAM) is a
family of SASL mechanisms that addresses the security concerns with
traditional mechanisms that perform username/password authentication
like PLAIN and DIGEST-MD5. The mechanism is defined in <a href="https://tools.ietf.org/html/rfc5802">RFC
5802</a>. Kafka supports
<a href="https://tools.ietf.org/html/rfc7677">SCRAM-SHA-256</a> and
SCRAM-SHA-512 which can be used with TLS to perform secure
authentication. Under the default implementation of
<code>principal.builder.class</code>, the username is used as the authenticated
<code>Principal</code> for configuration of ACLs etc. The default SCRAM
implementation in Kafka stores SCRAM credentials in Zookeeper and is
suitable for use in Kafka installations where Zookeeper is on a
private network. Refer to <a href="#security_sasl_scram_security">Security
Considerations</a> for more details.</p>
<h4 id="security_sasl_scram_credentials">
  1. Creating SCRAM Credentials
  <a class="anchor" href="#security_sasl_scram_credentials">#</a>
</h4>
<p>The SCRAM implementation in Kafka uses Zookeeper as credential
store. Credentials can be created in Zookeeper using
<code>kafka-configs.sh</code>. For each SCRAM mechanism enabled,
credentials must be created by adding a config with the
mechanism name. Credentials for inter-broker communication must
be created before Kafka brokers are started. Client credentials
may be created and updated dynamically and updated credentials
will be used to authenticate new connections.</p>
<p>Create SCRAM credentials for user <em>alice</em> with password
<em>alice-secret</em>:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-configs.sh --zookeeper localhost:2182 --zk-tls-config-file zk_tls_config.properties --alter --add-config &#39;SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]&#39; --entity-type users --entity-name alice
</code></pre><p>The default iteration count of 4096 is used if iterations are
not specified. A random salt is created and the SCRAM identity
consisting of salt, iterations, StoredKey and ServerKey are
stored in Zookeeper. See <a href="https://tools.ietf.org/html/rfc5802">RFC
5802</a> for details on SCRAM
identity and the individual fields.</p>
<p>The following examples also require a user <em>admin</em> for
inter-broker communication which can be created using:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-configs.sh --zookeeper localhost:2182 --zk-tls-config-file zk_tls_config.properties --alter --add-config &#39;SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]&#39; --entity-type users --entity-name admin
</code></pre><p>Existing credentials may be listed using the <em>--describe</em>
option:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-configs.sh --zookeeper localhost:2182 --zk-tls-config-file zk_tls_config.properties --describe --entity-type users --entity-name alice
</code></pre><p>Credentials may be deleted for one or more SCRAM mechanisms
using the <em>--alter --delete-config</em> option:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-configs.sh --zookeeper localhost:2182 --zk-tls-config-file zk_tls_config.properties --alter --delete-config &#39;SCRAM-SHA-512&#39; --entity-type users --entity-name alice
</code></pre><h4 id="security_sasl_scram_brokerconfig">
  2. Configuring Kafka Brokers
  <a class="anchor" href="#security_sasl_scram_brokerconfig">#</a>
</h4>
<ol>
<li>
<p>Add a suitably modified JAAS file similar to the one below
to each Kafka broker's config directory, let's call it
kafka_server_jaas.conf for this example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaServer {
    org.apache.kafka.common.security.scram.ScramLoginModule required
    username=&#34;admin&#34;
    password=&#34;admin-secret&#34;;
};
</code></pre><p>The properties <code>username</code> and <code>password</code> in the
<code>KafkaServer</code> section are used by the broker to initiate
connections to other brokers. In this example, <em>admin</em> is
the user for inter-broker communication.</p>
</li>
<li>
<p>Pass the JAAS config file location as JVM parameter to each
Kafka broker:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
</code></pre></li>
<li>
<p>Configure SASL port and SASL mechanisms in server.properties
as described <a href="#security_sasl_brokerconfig">here</a>. For
example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SASL_SSL://host.name:port
security.inter.broker.protocol=SASL_SSL
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 (or SCRAM-SHA-512)
sasl.enabled.mechanisms=SCRAM-SHA-256 (or SCRAM-SHA-512)
</code></pre></li>
</ol>
<h4 id="security_sasl_scram_clientconfig">
  3. Configuring Kafka Clients
  <a class="anchor" href="#security_sasl_scram_clientconfig">#</a>
</h4>
<p>To configure SASL authentication on the clients:</p>
<ol>
<li>
<p>Configure the JAAS configuration property for each client in
producer.properties or consumer.properties. The login module
describes how the clients like producer and consumer can
connect to the Kafka Broker. The following is an example
configuration for a client for the SCRAM mechanisms:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username=&#34;alice&#34; \
    password=&#34;alice-secret&#34;;
</code></pre><p>The options <code>username</code> and <code>password</code> are used by clients to
configure the user for client connections. In this example,
clients connect to the broker as user <em>alice</em>. Different
clients within a JVM may connect as different users by
specifying different user names and passwords in
<code>sasl.jaas.config</code>.</p>
<p>JAAS configuration for clients may alternatively be
specified as a JVM parameter similar to brokers as described
<a href="#security_client_staticjaas">here</a>. Clients use the login
section named <code>KafkaClient</code>. This option allows only one
user for all client connections from a JVM.</p>
</li>
<li>
<p>Configure the following properties in producer.properties or
consumer.properties:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256 (or SCRAM-SHA-512)
</code></pre></li>
</ol>
<h4 id="security_sasl_scram_security">
  4. Security Considerations for SASL/SCRAM
  <a class="anchor" href="#security_sasl_scram_security">#</a>
</h4>
<ul>
<li>The default implementation of SASL/SCRAM in Kafka stores
SCRAM credentials in Zookeeper. This is suitable for
production use in installations where Zookeeper is secure
and on a private network.</li>
<li>Kafka supports only the strong hash functions SHA-256 and
SHA-512 with a minimum iteration count of 4096. Strong hash
functions combined with strong passwords and high iteration
counts protect against brute force attacks if Zookeeper
security is compromised.</li>
<li>SCRAM should be used only with TLS-encryption to prevent
interception of SCRAM exchanges. This protects against
dictionary or brute force attacks and against impersonation
if Zookeeper is compromised.</li>
<li>From Kafka version 2.0 onwards, the default SASL/SCRAM
credential store may be overridden using custom callback
handlers by configuring <code>sasl.server.callback.handler.class</code>
in installations where Zookeeper is not secure.</li>
<li>For more details on security considerations, refer to <a href="https://tools.ietf.org/html/rfc5802#section-9">RFC 5802</a>.</li>
</ul>
<h3 id="security_sasl_oauthbearer">
  6. Authentication using SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer">#</a>
</h3>
<p>The <a href="https://tools.ietf.org/html/rfc6749">OAuth 2 Authorization Framework</a> &quot;enables a
third-party application to obtain limited access to an HTTP service,
either on behalf of a resource owner by orchestrating an approval
interaction between the resource owner and the HTTP service, or by
allowing the third-party application to obtain access on its own
behalf.&quot; The SASL OAUTHBEARER mechanism enables the use of the
framework in a SASL (i.e. a non-HTTP) context; it is defined in <a href="https://tools.ietf.org/html/rfc7628">RFC
7628</a>. The default OAUTHBEARER
implementation in Kafka creates and validates <a href="https://tools.ietf.org/html/rfc7515#appendix-A.5">Unsecured JSON Web
Tokens</a> and is
only suitable for use in non-production Kafka installations. Refer
to <a href="#security_sasl_oauthbearer_security">Security Considerations</a>
for more details.</p>
<p>Under the default implementation of <code>principal.builder.class</code>, the
principalName of OAuthBearerToken is used as the authenticated
<code>Principal</code> for configuration of ACLs etc.</p>
<h4 id="security_sasl_oauthbearer_brokerconfig">
  1. Configuring Kafka Brokers
  <a class="anchor" href="#security_sasl_oauthbearer_brokerconfig">#</a>
</h4>
<ol>
<li>
<p>Add a suitably modified JAAS file similar to the one below
to each Kafka broker's config directory, let's call it
kafka_server_jaas.conf for this example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaServer {
    org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required
    unsecuredLoginStringClaim_sub=&#34;admin&#34;;
};
</code></pre><p>The property <code>unsecuredLoginStringClaim_sub</code> in the
<code>KafkaServer</code> section is used by the broker when it
initiates connections to other brokers. In this example,
<em>admin</em> will appear in the subject (<code>sub</code>) claim and will be
the user for inter-broker communication.</p>
</li>
<li>
<p>Pass the JAAS config file location as JVM parameter to each
Kafka broker:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
</code></pre></li>
<li>
<p>Configure SASL port and SASL mechanisms in server.properties
as described <a href="#security_sasl_brokerconfig">here</a>. For
example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SASL_SSL://host.name:port (or SASL_PLAINTEXT if non-production)
security.inter.broker.protocol=SASL_SSL (or SASL_PLAINTEXT if non-production)
sasl.mechanism.inter.broker.protocol=OAUTHBEARER
sasl.enabled.mechanisms=OAUTHBEARER
</code></pre></li>
</ol>
<h4 id="security_sasl_oauthbearer_clientconfig">
  2. Configuring Kafka Clients
  <a class="anchor" href="#security_sasl_oauthbearer_clientconfig">#</a>
</h4>
<p>To configure SASL authentication on the clients:</p>
<ol>
<li>
<p>Configure the JAAS configuration property for each client in
producer.properties or consumer.properties. The login module
describes how the clients like producer and consumer can
connect to the Kafka Broker. The following is an example
configuration for a client for the OAUTHBEARER mechanisms:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
    unsecuredLoginStringClaim_sub=&#34;alice&#34;;
</code></pre><p>The option <code>unsecuredLoginStringClaim_sub</code> is used by
clients to configure the subject (<code>sub</code>) claim, which
determines the user for client connections. In this example,
clients connect to the broker as user <em>alice</em>. Different
clients within a JVM may connect as different users by
specifying different subject (<code>sub</code>) claims in
<code>sasl.jaas.config</code>.</p>
<p>JAAS configuration for clients may alternatively be
specified as a JVM parameter similar to brokers as described
<a href="#security_client_staticjaas">here</a>. Clients use the login
section named <code>KafkaClient</code>. This option allows only one
user for all client connections from a JVM.</p>
</li>
<li>
<p>Configure the following properties in producer.properties or
consumer.properties:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.protocol=SASL_SSL (or SASL_PLAINTEXT if non-production)
sasl.mechanism=OAUTHBEARER
</code></pre></li>
<li>
<p>The default implementation of SASL/OAUTHBEARER depends on
the jackson-databind library. Since it's an optional
dependency, users have to configure it as a dependency via
their build tool.</p>
</li>
</ol>
<h4 id="security_sasl_oauthbearer_unsecured_retrieval">
  3. Unsecured Token Creation Options for SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer_unsecured_retrieval">#</a>
</h4>
<ul>
<li>The default implementation of SASL/OAUTHBEARER in Kafka
creates and validates <a href="https://tools.ietf.org/html/rfc7515#appendix-A.5">Unsecured JSON Web Tokens</a>.
While suitable only for non-production use, it does provide
the flexibility to create arbitrary tokens in a DEV or TEST
environment.</li>
<li>Here are the various supported JAAS module options on the
client side (and on the broker side if OAUTHBEARER is the
inter-broker protocol):
JAAS Module Option for Unsecured Token Creation</li>
</ul>
<h4 id="security_sasl_oauthbearer_unsecured_validation">
  4. Unsecured Token Validation Options for SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer_unsecured_validation">#</a>
</h4>
<ul>
<li>Here are the various supported JAAS module options on the
broker side for <a href="https://tools.ietf.org/html/rfc7515#appendix-A.5">Unsecured JSON Web
Token</a>
validation:
JAAS Module Option for Unsecured Token Validation   Documentation
<hr>
<code>unsecuredValidatorPrincipalClaimName=&quot;value&quot;</code>      Set to a non-empty value if you wish a particular <code>String</code> claim holding a principal name to be checked for existence; the default is to check for the existence of the '<code>sub</code>' claim.
<code>unsecuredValidatorScopeClaimName=&quot;value&quot;</code>          Set to a custom claim name if you wish the name of the <code>String</code> or <code>String List</code> claim holding any token scope to be something other than '<code>scope</code>'.
<code>unsecuredValidatorRequiredScope=&quot;value&quot;</code>           Set to a space-delimited list of scope values if you wish the <code>String/String List</code> claim holding the token scope to be checked to make sure it contains certain values.
<code>unsecuredValidatorAllowableClockSkewMs=&quot;value&quot;</code>    Set to a positive integer value if you wish to allow up to some number of positive milliseconds of clock skew (the default is 0).</li>
<li>The default unsecured SASL/OAUTHBEARER implementation may be
overridden (and must be overridden in production
environments) using custom login and SASL Server callback
handlers.</li>
<li>For more details on security considerations, refer to <a href="https://tools.ietf.org/html/rfc6749#section-10">RFC 6749, Section 10</a>.</li>
</ul>
<h4 id="security_sasl_oauthbearer_refresh">
  5. Token Refresh for SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer_refresh">#</a>
</h4>
<p>Kafka periodically refreshes any token before it expires so that
the client can continue to make connections to brokers. The
parameters that impact how the refresh algorithm operates are
specified as part of the producer/consumer/broker configuration
and are as follows. See the documentation for these properties
elsewhere for details. The default values are usually
reasonable, in which case these configuration parameters would
not need to be explicitly set.</p>
<h2 id="producerconsumerbroker-configuration-property">
  Producer/Consumer/Broker Configuration Property
  <a class="anchor" href="#producerconsumerbroker-configuration-property">#</a>
</h2>
<p><code>sasl.login.refresh.window.factor</code>
<code>sasl.login.refresh.window.jitter</code>
<code>sasl.login.refresh.min.period.seconds</code>
<code>sasl.login.refresh.min.buffer.seconds</code></p>
<h4 id="security_sasl_oauthbearer_prod">
  6. Secure/Production Use of SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer_prod">#</a>
</h4>
<p>Production use cases will require writing an implementation of
<code>org.apache.kafka.common.security.auth.AuthenticateCallbackHandler</code>
that can handle an instance of
<code>org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback</code>
and declaring it via either the
<code>sasl.login.callback.handler.class</code> configuration option for a
non-broker client or via the
<code>listener.name.sasl_ssl.oauthbearer.sasl.login.callback.handler.class</code>
configuration option for brokers (when SASL/OAUTHBEARER is the
inter-broker protocol).</p>
<p>Production use cases will also require writing an implementation
of
<code>org.apache.kafka.common.security.auth.AuthenticateCallbackHandler</code>
that can handle an instance of
<code>org.apache.kafka.common.security.oauthbearer.OAuthBearerValidatorCallback</code>
and declaring it via the
<code>listener.name.sasl_ssl.oauthbearer.sasl.server.callback.handler.class</code>
broker configuration option.</p>
<h4 id="security_sasl_oauthbearer_security">
  7. Security Considerations for SASL/OAUTHBEARER
  <a class="anchor" href="#security_sasl_oauthbearer_security">#</a>
</h4>
<ul>
<li>The default implementation of SASL/OAUTHBEARER in Kafka
creates and validates <a href="https://tools.ietf.org/html/rfc7515#appendix-A.5">Unsecured JSON Web
Tokens</a>.
This is suitable only for non-production use.</li>
<li>OAUTHBEARER should be used in production enviromnments only
with TLS-encryption to prevent interception of tokens.</li>
<li>The default unsecured SASL/OAUTHBEARER implementation may be
overridden (and must be overridden in production
environments) using custom login and SASL Server callback
handlers as described above.</li>
<li>For more details on OAuth 2 security considerations in
general, refer to <a href="https://tools.ietf.org/html/rfc6749#section-10">RFC 6749, Section
10</a>.</li>
</ul>
<h3 id="security_sasl_multimechanism">
  7. Enabling multiple SASL mechanisms in a broker
  <a class="anchor" href="#security_sasl_multimechanism">#</a>
</h3>
<ol>
<li>
<p>Specify configuration for the login modules of all enabled
mechanisms in the <code>KafkaServer</code> section of the JAAS config file.
For example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">KafkaServer {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab=&#34;/etc/security/keytabs/kafka_server.keytab&#34;
    principal=&#34;kafka/kafka1.hostname.com@EXAMPLE.COM&#34;;

    org.apache.kafka.common.security.plain.PlainLoginModule required
    username=&#34;admin&#34;
    password=&#34;admin-secret&#34;
    user_admin=&#34;admin-secret&#34;
    user_alice=&#34;alice-secret&#34;;
};
</code></pre></li>
<li>
<p>Enable the SASL mechanisms in server.properties:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.enabled.mechanisms=GSSAPI,PLAIN,SCRAM-SHA-256,SCRAM-SHA-512,OAUTHBEARER
</code></pre></li>
<li>
<p>Specify the SASL security protocol and mechanism for
inter-broker communication in server.properties if required:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
sasl.mechanism.inter.broker.protocol=GSSAPI (or one of the other enabled mechanisms)
</code></pre></li>
<li>
<p>Follow the mechanism-specific steps in <a href="#security_sasl_kerberos_brokerconfig">GSSAPI (Kerberos)</a>,
<a href="#security_sasl_plain_brokerconfig">PLAIN</a>,
<a href="#security_sasl_scram_brokerconfig">SCRAM</a> and
<a href="#security_sasl_oauthbearer_brokerconfig">OAUTHBEARER</a> to
configure SASL for the enabled mechanisms.</p>
</li>
</ol>
<h3 id="saslmechanism_rolling_upgrade">
  8. Modifying SASL mechanism in a Running Cluster
  <a class="anchor" href="#saslmechanism_rolling_upgrade">#</a>
</h3>
<p>SASL mechanism can be modified in a running cluster using the
following sequence:</p>
<ol>
<li>Enable new SASL mechanism by adding the mechanism to
<code>sasl.enabled.mechanisms</code> in server.properties for each broker.
Update JAAS config file to include both mechanisms as described
<a href="#security_sasl_multimechanism">here</a>. Incrementally bounce the
cluster nodes.</li>
<li>Restart clients using the new mechanism.</li>
<li>To change the mechanism of inter-broker communication (if this
is required), set <code>sasl.mechanism.inter.broker.protocol</code> in
server.properties to the new mechanism and incrementally bounce
the cluster again.</li>
<li>To remove old mechanism (if this is required), remove the old
mechanism from <code>sasl.enabled.mechanisms</code> in server.properties
and remove the entries for the old mechanism from JAAS config
file. Incrementally bounce the cluster again.</li>
</ol>
<h3 id="security_delegation_token">
  9. Authentication using Delegation Tokens
  <a class="anchor" href="#security_delegation_token">#</a>
</h3>
<p>Delegation token based authentication is a lightweight
authentication mechanism to complement existing SASL/SSL methods.
Delegation tokens are shared secrets between kafka brokers and
clients. Delegation tokens will help processing frameworks to
distribute the workload to available workers in a secure environment
without the added cost of distributing Kerberos TGT/keytabs or
keystores when 2-way SSL is used. See
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-48&#43;Delegation&#43;token&#43;support&#43;for&#43;Kafka">KIP-48</a>
for more details.</p>
<p>Under the default implementation of <code>principal.builder.class</code>, the
owner of delegation token is used as the authenticated <code>Principal</code>
for configuration of ACLs etc.</p>
<p>Typical steps for delegation token usage are:</p>
<ol>
<li>User authenticates with the Kafka cluster via SASL or SSL, and
obtains a delegation token. This can be done using Admin APIs or
using <code>kafka-delegation-tokens.sh</code> script.</li>
<li>User securely passes the delegation token to Kafka clients for
authenticating with the Kafka cluster.</li>
<li>Token owner/renewer can renew/expire the delegation tokens.</li>
</ol>
<pre tabindex="0"><code>&lt;!-- --&gt;
</code></pre><h4 id="security_token_management">
  1. Token Management
  <a class="anchor" href="#security_token_management">#</a>
</h4>
<p>A secret is used to generate and verify delegation tokens. This
is supplied using config option <code>delegation.token.secret.key</code>.
The same secret key must be configured across all the brokers.
If the secret is not set or set to empty string, brokers will
disable the delegation token authentication.</p>
<p>In the current implementation, token details are stored in
Zookeeper and is suitable for use in Kafka installations where
Zookeeper is on a private network. Also currently, this secret
is stored as plain text in the server.properties config file. We
intend to make these configurable in a future Kafka release.</p>
<p>A token has a current life, and a maximum renewable life. By
default, tokens must be renewed once every 24 hours for up to 7
days. These can be configured using
<code>delegation.token.expiry.time.ms</code> and
<code>delegation.token.max.lifetime.ms</code> config options.</p>
<p>Tokens can also be cancelled explicitly. If a token is not
renewed by the token&rsquo;s expiration time or if token is beyond the
max life time, it will be deleted from all broker caches as well
as from zookeeper.</p>
<h4 id="security_sasl_create_tokens">
  2. Creating Delegation Tokens
  <a class="anchor" href="#security_sasl_create_tokens">#</a>
</h4>
<p>Tokens can be created by using Admin APIs or using
<code>kafka-delegation-tokens.sh</code> script. Delegation token requests
(create/renew/expire/describe) should be issued only on SASL or
SSL authenticated channels. Tokens can not be requests if the
initial authentication is done through delegation token. A token
can be created by the user for that user or others as well by
specifying the <code>--owner-principal</code> parameter. Owner/Renewers can
renew or expire tokens. Owner/renewers can always describe their
own tokens. To describe other tokens, a DESCRIBE_TOKEN
permission needs to be added on the User resource representing
the owner of the token. <code>kafka-delegation-tokens.sh</code> script
examples are given below.</p>
<p>Create a delegation token:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-delegation-tokens.sh --bootstrap-server localhost:9092 --create   --max-life-time-period -1 --command-config client.properties --renewer-principal User:user1
</code></pre><p>Create a delegation token for a different owner:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-delegation-tokens.sh --bootstrap-server localhost:9092 --create   --max-life-time-period -1 --command-config client.properties --renewer-principal User:user1 --owner-principal User:owner1
</code></pre><p>Renew a delegation token:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-delegation-tokens.sh --bootstrap-server localhost:9092 --renew    --renew-time-period -1 --command-config client.properties --hmac ABCDEFGHIJK
</code></pre><p>Expire a delegation token:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-delegation-tokens.sh --bootstrap-server localhost:9092 --expire   --expiry-time-period -1   --command-config client.properties  --hmac ABCDEFGHIJK
</code></pre><p>Existing tokens can be described using the --describe option:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-delegation-tokens.sh --bootstrap-server localhost:9092 --describe --command-config client.properties  --owner-principal User:user1
</code></pre><h4 id="security_token_authentication">
  3. Token Authentication
  <a class="anchor" href="#security_token_authentication">#</a>
</h4>
<p>Delegation token authentication piggybacks on the current
SASL/SCRAM authentication mechanism. We must enable SASL/SCRAM
mechanism on Kafka cluster as described in
<a href="#security_sasl_scram">here</a>.</p>
<p>Configuring Kafka Clients:</p>
<ol>
<li>
<p>Configure the JAAS configuration property for each client in
producer.properties or consumer.properties. The login module
describes how the clients like producer and consumer can
connect to the Kafka Broker. The following is an example
configuration for a client for the token authentication:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username=&#34;tokenID123&#34; \
    password=&#34;lAYYSFmLs4bTjf+lTZ1LCHR/ZZFNA==&#34; \
    tokenauth=&#34;true&#34;;
</code></pre><p>The options <code>username</code> and <code>password</code> are used by clients to
configure the token id and token HMAC. And the option
<code>tokenauth</code> is used to indicate the server about token
authentication. In this example, clients connect to the
broker using token id: <em>tokenID123</em>. Different clients
within a JVM may connect using different tokens by
specifying different token details in <code>sasl.jaas.config</code>.</p>
<p>JAAS configuration for clients may alternatively be
specified as a JVM parameter similar to brokers as described
<a href="#security_client_staticjaas">here</a>. Clients use the login
section named <code>KafkaClient</code>. This option allows only one
user for all client connections from a JVM.</p>
</li>
</ol>
<h4 id="security_token_secret_rotation">
  4. Procedure to manually rotate the secret
  <a class="anchor" href="#security_token_secret_rotation">#</a>
</h4>
<p>We require a re-deployment when the secret needs to be rotated.
During this process, already connected clients will continue to
work. But any new connection requests and renew/expire requests
with old tokens can fail. Steps are given below.</p>
<ol>
<li>Expire all existing tokens.</li>
<li>Rotate the secret by rolling upgrade, and</li>
<li>Generate new tokens</li>
</ol>
<p>We intend to automate this in a future Kafka release.</p>
<h2 id="security_authz">
  7.5 Authorization and ACLs
  <a class="anchor" href="#security_authz">#</a>
</h2>
<p>Kafka ships with a pluggable authorization framework, which is
configured with the <code>authorizer.class.name</code> property in the server
confgiuration. Configured implementations must extend
<code>org.apache.kafka.server.authorizer.Authorizer</code>. Kafka provides default
implementations which store ACLs in the cluster metadata (either
Zookeeper or the KRaft metadata log). For Zookeeper-based clusters, the
provided implementation is configured as follows:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">authorizer.class.name=kafka.security.authorizer.AclAuthorizer
</code></pre><p>For KRaft clusters, use the following configuration on all nodes
(brokers, controllers, or combined broker/controller nodes):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">authorizer.class.name=org.apache.kafka.metadata.authorizer.StandardAuthorizer
</code></pre><p>Kafka ACLs are defined in the general format of &quot;Principal {P} is
[Allowed|Denied] Operation {O} From Host {H} on any Resource {R}
matching ResourcePattern {RP}&quot;. You can read more about the ACL
structure in
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-11&#43;-&#43;Authorization&#43;Interface">KIP-11</a>
and resource patterns in
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A&#43;Support&#43;for&#43;Prefixed&#43;ACLs">KIP-290</a>.
In order to add, remove, or list ACLs, you can use the Kafka ACL CLI
<code>kafka-acls.sh</code>. By default, if no ResourcePatterns match a specific
Resource R, then R has no associated ACLs, and therefore no one other
than super users is allowed to access R. If you want to change that
behavior, you can include the following in server.properties.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">allow.everyone.if.no.acl.found=true
</code></pre><p>One can also add super users in server.properties like the following
(note that the delimiter is semicolon since SSL user names may contain
comma). Default PrincipalType string &quot;User&quot; is case sensitive.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">super.users=User:Bob;User:Alice
</code></pre><h4 id="kraft_principal_forwarding">
  KRaft Principal Forwarding
  <a class="anchor" href="#kraft_principal_forwarding">#</a>
</h4>
<p>In KRaft clusters, admin requests such as <code>CreateTopics</code> and
<code>DeleteTopics</code> are sent to the broker listeners by the client. The
broker then forwards the request to the active controller through the
first listener configured in <code>controller.listener.names</code>. Authorization
of these requests is done on the controller node. This is achieved by
way of an <code>Envelope</code> request which packages both the underlying request
from the client as well as the client principal. When the controller
receives the forwarded <code>Envelope</code> request from the broker, it first
authorizes the <code>Envelope</code> request using the authenticated broker
principal. Then it authorizes the underlying request using the forwarded
principal.<br>
All of this implies that Kafka must understand how to serialize and
deserialize the client principal. The authentication framework allows
for customized principals by overriding the <code>principal.builder.class</code>
configuration. In order for customized principals to work with KRaft,
the configured class must implement
<code>org.apache.kafka.common.security.auth.KafkaPrincipalSerde</code> so that
Kafka knows how to serialize and deserialize the principals. The default
implementation
<code>org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder</code>
uses the Kafka RPC format defined in the source code:
<code>clients/src/main/resources/common/message/DefaultPrincipalData.json</code>.
For more detail about request forwarding in KRaft, see
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-590%3A&#43;Redirect&#43;Zookeeper&#43;Mutation&#43;Protocols&#43;to&#43;The&#43;Controller">KIP-590</a></p>
<h4 id="security_authz_ssl">
  Customizing SSL User Name
  <a class="anchor" href="#security_authz_ssl">#</a>
</h4>
<p>By default, the SSL user name will be of the form
&quot;CN=writeuser,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown&quot;.
One can change that by setting <code>ssl.principal.mapping.rules</code> to a
customized rule in server.properties. This config allows a list of rules
for mapping X.500 distinguished name to short name. The rules are
evaluated in order and the first rule that matches a distinguished name
is used to map it to a short name. Any later rules in the list are
ignored.<br>
The format of <code>ssl.principal.mapping.rules</code> is a list where each rule
starts with &quot;RULE:&quot; and contains an expression as the following
formats. Default rule will return string representation of the X.500
certificate distinguished name. If the distinguished name matches the
pattern, then the replacement command will be run over the name. This
also supports lowercase/uppercase options, to force the translated
result to be all lower/uppercase case. This is done by adding a &quot;/L&quot;
or &quot;/U' to the end of the rule.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">RULE:pattern/replacement/
RULE:pattern/replacement/[LU]
</code></pre><p>Example <code>ssl.principal.mapping.rules</code> values are:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">RULE:^CN=(.*?),OU=ServiceUsers.*$/$1/,
RULE:^CN=(.*?),OU=(.*?),O=(.*?),L=(.*?),ST=(.*?),C=(.*?)$/$1@$2/L,
RULE:^.*[Cc][Nn]=([a-zA-Z0-9.]*).*$/$1/L,
DEFAULT
</code></pre><p>Above rules translate distinguished name
&quot;CN=serviceuser,OU=ServiceUsers,O=Unknown,L=Unknown,ST=Unknown,C=Unknown&quot;
to &quot;serviceuser&quot; and
&quot;CN=adminUser,OU=Admin,O=Unknown,L=Unknown,ST=Unknown,C=Unknown&quot; to
&quot;adminuser@admin&quot;.<br>
For advanced use cases, one can customize the name by setting a
customized PrincipalBuilder in server.properties like the following.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">principal.builder.class=CustomizedPrincipalBuilderClass
</code></pre><h4 id="security_authz_sasl">
  Customizing SASL User Name
  <a class="anchor" href="#security_authz_sasl">#</a>
</h4>
<p>By default, the SASL user name will be the primary part of the Kerberos
principal. One can change that by setting
<code>sasl.kerberos.principal.to.local.rules</code> to a customized rule in
server.properties. The format of
<code>sasl.kerberos.principal.to.local.rules</code> is a list where each rule works
in the same way as the auth_to_local in <a href="http://web.mit.edu/Kerberos/krb5-latest/doc/admin/conf_files/krb5_conf.html">Kerberos configuration file
(krb5.conf)</a>.
This also support additional lowercase/uppercase rule, to force the
translated result to be all lowercase/uppercase. This is done by adding
a &quot;/L&quot; or &quot;/U&quot; to the end of the rule. check below formats for
syntax. Each rules starts with RULE: and contains an expression as the
following formats. See the kerberos documentation for more details.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">RULE:[n:string](regexp)s/pattern/replacement/
RULE:[n:string](regexp)s/pattern/replacement/g
RULE:[n:string](regexp)s/pattern/replacement//L
RULE:[n:string](regexp)s/pattern/replacement/g/L
RULE:[n:string](regexp)s/pattern/replacement//U
RULE:[n:string](regexp)s/pattern/replacement/g/U
</code></pre><p>An example of adding a rule to properly translate <a href="mailto:user@MYDOMAIN.COM">user@MYDOMAIN.COM</a> to
user while also keeping the default rule in place is:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">sasl.kerberos.principal.to.local.rules=RULE:[1:$1@$0](.*@MYDOMAIN.COM)s/@.*//,DEFAULT
</code></pre><h3 id="security_authz_cli">
  Command Line Interface
  <a class="anchor" href="#security_authz_cli">#</a>
</h3>
<p>Kafka Authorization management CLI can be found under bin directory with
all the other CLIs. The CLI script is called <strong>kafka-acls.sh</strong>.
Following lists all the options that the script supports:</p>
<p>+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| Option          | Description     | Default         | Option type     |
+=================+=================+=================+=================+
| --add          | Indicates to    |                 | Action          |
|                 | the script that |                 |                 |
|                 | user is trying  |                 |                 |
|                 | to add an acl.  |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --remove       | Indicates to    |                 | Action          |
|                 | the script that |                 |                 |
|                 | user is trying  |                 |                 |
|                 | to remove an    |                 |                 |
|                 | acl.            |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --list         | Indicates to    |                 | Action          |
|                 | the script that |                 |                 |
|                 | user is trying  |                 |                 |
|                 | to list acls.   |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --b            | A list of       |                 | Configuration   |
| ootstrap-server | host/port pairs |                 |                 |
|                 | to use for      |                 |                 |
|                 | establishing    |                 |                 |
|                 | the connection  |                 |                 |
|                 | to the Kafka    |                 |                 |
|                 | cluster. Only   |                 |                 |
|                 | one of          |                 |                 |
|                 | --b            |                 |                 |
|                 | ootstrap-server |                 |                 |
|                 | or              |                 |                 |
|                 | --authorizer   |                 |                 |
|                 | option must be  |                 |                 |
|                 | specified.      |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| -              | A property file |                 | Configuration   |
| -command-config | containing      |                 |                 |
|                 | configs to be   |                 |                 |
|                 | passed to Admin |                 |                 |
|                 | Client. This    |                 |                 |
|                 | option can only |                 |                 |
|                 | be used with    |                 |                 |
|                 | --b            |                 |                 |
|                 | ootstrap-server |                 |                 |
|                 | option.         |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --cluster      | Indicates to    |                 | ResourcePattern |
|                 | the script that |                 |                 |
|                 | the user is     |                 |                 |
|                 | trying to       |                 |                 |
|                 | interact with   |                 |                 |
|                 | acls on the     |                 |                 |
|                 | singular        |                 |                 |
|                 | cluster         |                 |                 |
|                 | resource.       |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --topic        | Indicates to    |                 | ResourcePattern |
| [topic-name]  | the script that |                 |                 |
|                 | the user is     |                 |                 |
|                 | trying to       |                 |                 |
|                 | interact with   |                 |                 |
|                 | acls on topic   |                 |                 |
|                 | resource        |                 |                 |
|                 | pattern(s).     |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --group        | Indicates to    |                 | ResourcePattern |
| [group-name]  | the script that |                 |                 |
|                 | the user is     |                 |                 |
|                 | trying to       |                 |                 |
|                 | interact with   |                 |                 |
|                 | acls on         |                 |                 |
|                 | consumer-group  |                 |                 |
|                 | resource        |                 |                 |
|                 | pattern(s)      |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --t            | The             |                 | ResourcePattern |
| ransactional-id | transactionalId |                 |                 |
| [tra           | to which ACLs   |                 |                 |
| nsactional-id] | should be added |                 |                 |
|                 | or removed. A   |                 |                 |
|                 | value of *     |                 |                 |
|                 | indicates the   |                 |                 |
|                 | ACLs should     |                 |                 |
|                 | apply to all    |                 |                 |
|                 | tr              |                 |                 |
|                 | ansactionalIds. |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --d            | Delegation      |                 | ResourcePattern |
| elegation-token | token to which  |                 |                 |
| [del           | ACLs should be  |                 |                 |
| egation-token] | added or        |                 |                 |
|                 | removed. A      |                 |                 |
|                 | value of *     |                 |                 |
|                 | indicates ACL   |                 |                 |
|                 | should apply to |                 |                 |
|                 | all tokens.     |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| -              | A user resource |                 | ResourcePattern |
| -user-principal | to which ACLs   |                 |                 |
| [u             | should be added |                 |                 |
| ser-principal] | or removed.     |                 |                 |
|                 | This is         |                 |                 |
|                 | currently       |                 |                 |
|                 | supported in    |                 |                 |
|                 | relation with   |                 |                 |
|                 | delegation      |                 |                 |
|                 | tokens. A value |                 |                 |
|                 | of * indicates |                 |                 |
|                 | ACL should      |                 |                 |
|                 | apply to all    |                 |                 |
|                 | users.          |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --resour       | Indicates to    | literal         | Configuration   |
| ce-pattern-type | the script the  |                 |                 |
| \               | type of         |                 |                 |
| [pattern-type] | resource        |                 |                 |
|                 | pattern, (for   |                 |                 |
|                 | --add), or     |                 |                 |
|                 | resource        |                 |                 |
|                 | pattern filter, |                 |                 |
|                 | (for --list    |                 |                 |
|                 | and --remove), |                 |                 |
|                 | the user wishes |                 |                 |
|                 | to use.\        |                 |                 |
|                 | When adding     |                 |                 |
|                 | acls, this      |                 |                 |
|                 | should be a     |                 |                 |
|                 | specific        |                 |                 |
|                 | pattern type,   |                 |                 |
|                 | e.g.            |                 |                 |
|                 | 'literal' or  |                 |                 |
|                 | 'prefixed'.\  |                 |                 |
|                 | When listing or |                 |                 |
|                 | removing acls,  |                 |                 |
|                 | a specific      |                 |                 |
|                 | pattern type    |                 |                 |
|                 | filter can be   |                 |                 |
|                 | used to list or |                 |                 |
|                 | remove acls     |                 |                 |
|                 | from a specific |                 |                 |
|                 | type of         |                 |                 |
|                 | resource        |                 |                 |
|                 | pattern, or the |                 |                 |
|                 | filter values   |                 |                 |
|                 | of 'any' or   |                 |                 |
|                 | 'match' can   |                 |                 |
|                 | be used, where  |                 |                 |
|                 | 'any' will    |                 |                 |
|                 | match any       |                 |                 |
|                 | pattern type,   |                 |                 |
|                 | but will match  |                 |                 |
|                 | the resource    |                 |                 |
|                 | name exactly,   |                 |                 |
|                 | and 'match'   |                 |                 |
|                 | will perform    |                 |                 |
|                 | pattern         |                 |                 |
|                 | matching to     |                 |                 |
|                 | list or remove  |                 |                 |
|                 | all acls that   |                 |                 |
|                 | affect the      |                 |                 |
|                 | supplied        |                 |                 |
|                 | resource(s).\   |                 |                 |
|                 | WARNING:        |                 |                 |
|                 | 'match', when |                 |                 |
|                 | used in         |                 |                 |
|                 | combination     |                 |                 |
|                 | with the        |                 |                 |
|                 | '--remove'   |                 |                 |
|                 | switch, should  |                 |                 |
|                 | be used with    |                 |                 |
|                 | care.           |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --             | Principal is in |                 | Principal       |
| allow-principal | Pri             |                 |                 |
|                 | ncipalType:name |                 |                 |
|                 | format that     |                 |                 |
|                 | will be added   |                 |                 |
|                 | to ACL with     |                 |                 |
|                 | Allow           |                 |                 |
|                 | permission.     |                 |                 |
|                 | Default         |                 |                 |
|                 | PrincipalType   |                 |                 |
|                 | string &quot;User&quot; |                 |                 |
|                 | is case         |                 |                 |
|                 | sensitive.\     |                 |                 |
|                 | You can specify |                 |                 |
|                 | multiple        |                 |                 |
|                 | --             |                 |                 |
|                 | allow-principal |                 |                 |
|                 | in a single     |                 |                 |
|                 | command.        |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| -              | Principal is in |                 | Principal       |
| -deny-principal | Pri             |                 |                 |
|                 | ncipalType:name |                 |                 |
|                 | format that     |                 |                 |
|                 | will be added   |                 |                 |
|                 | to ACL with     |                 |                 |
|                 | Deny            |                 |                 |
|                 | permission.     |                 |                 |
|                 | Default         |                 |                 |
|                 | PrincipalType   |                 |                 |
|                 | string &quot;User&quot; |                 |                 |
|                 | is case         |                 |                 |
|                 | sensitive.\     |                 |                 |
|                 | You can specify |                 |                 |
|                 | multiple        |                 |                 |
|                 | -              |                 |                 |
|                 | -deny-principal |                 |                 |
|                 | in a single     |                 |                 |
|                 | command.        |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --principal    | Principal is in |                 | Principal       |
|                 | Pri             |                 |                 |
|                 | ncipalType:name |                 |                 |
|                 | format that     |                 |                 |
|                 | will be used    |                 |                 |
|                 | along with      |                 |                 |
|                 | --list option. |                 |                 |
|                 | Default         |                 |                 |
|                 | PrincipalType   |                 |                 |
|                 | string &quot;User&quot; |                 |                 |
|                 | is case         |                 |                 |
|                 | sensitive. This |                 |                 |
|                 | will list the   |                 |                 |
|                 | ACLs for the    |                 |                 |
|                 | specified       |                 |                 |
|                 | principal.\     |                 |                 |
|                 | You can specify |                 |                 |
|                 | multiple        |                 |                 |
|                 | --principal in |                 |                 |
|                 | a single        |                 |                 |
|                 | command.        |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --allow-host   | IP address from | if              | Host            |
|                 | which           | --             |                 |
|                 | principals      | allow-principal |                 |
|                 | listed in       | is specified    |                 |
|                 | --             | defaults to *  |                 |
|                 | allow-principal | which           |                 |
|                 | will have       | translates to   |                 |
|                 | access.         | &quot;all hosts&quot;   |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --deny-host    | IP address from | if              | Host            |
|                 | which           | -              |                 |
|                 | principals      | -deny-principal |                 |
|                 | listed in       | is specified    |                 |
|                 | -              | defaults to *  |                 |
|                 | -deny-principal | which           |                 |
|                 | will be denied  | translates to   |                 |
|                 | access.         | &quot;all hosts&quot;   |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --operation    | Operation that  | All             | Operation       |
|                 | will be allowed |                 |                 |
|                 | or denied.\     |                 |                 |
|                 | Valid values    |                 |                 |
|                 | are:            |                 |                 |
|                 |                 |                 |                 |
|                 | -   Read        |                 |                 |
|                 | -   Write       |                 |                 |
|                 | -   Create      |                 |                 |
|                 | -   Delete      |                 |                 |
|                 | -   Alter       |                 |                 |
|                 | -   Describe    |                 |                 |
|                 | -               |                 |                 |
|                 |   ClusterAction |                 |                 |
|                 | -               |                 |                 |
|                 | DescribeConfigs |                 |                 |
|                 | -               |                 |                 |
|                 |    AlterConfigs |                 |                 |
|                 | -               |                 |                 |
|                 | IdempotentWrite |                 |                 |
|                 | -               |                 |                 |
|                 |    CreateTokens |                 |                 |
|                 | -               |                 |                 |
|                 |  DescribeTokens |                 |                 |
|                 | -   All         |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --producer     | Convenience     |                 | Convenience     |
|                 | option to       |                 |                 |
|                 | add/remove acls |                 |                 |
|                 | for producer    |                 |                 |
|                 | role. This will |                 |                 |
|                 | generate acls   |                 |                 |
|                 | that allows     |                 |                 |
|                 | WRITE, DESCRIBE |                 |                 |
|                 | and CREATE on   |                 |                 |
|                 | topic.          |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --consumer     | Convenience     |                 | Convenience     |
|                 | option to       |                 |                 |
|                 | add/remove acls |                 |                 |
|                 | for consumer    |                 |                 |
|                 | role. This will |                 |                 |
|                 | generate acls   |                 |                 |
|                 | that allows     |                 |                 |
|                 | READ, DESCRIBE  |                 |                 |
|                 | on topic and    |                 |                 |
|                 | READ on         |                 |                 |
|                 | consumer-group. |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --idempotent   | Enable          |                 | Convenience     |
|                 | idempotence for |                 |                 |
|                 | the producer.   |                 |                 |
|                 | This should be  |                 |                 |
|                 | used in         |                 |                 |
|                 | combination     |                 |                 |
|                 | with the        |                 |                 |
|                 | --producer     |                 |                 |
|                 | option.\        |                 |                 |
|                 | Note that       |                 |                 |
|                 | idempotence is  |                 |                 |
|                 | enabled         |                 |                 |
|                 | automatically   |                 |                 |
|                 | if the producer |                 |                 |
|                 | is authorized   |                 |                 |
|                 | to a particular |                 |                 |
|                 | tr              |                 |                 |
|                 | ansactional-id. |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --force        | Convenience     |                 | Convenience     |
|                 | option to       |                 |                 |
|                 | assume yes to   |                 |                 |
|                 | all queries and |                 |                 |
|                 | do not prompt.  |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --authorizer   | (DEPRECATED:    | kafka.sec       | Configuration   |
|                 | not supported   | urity.authorize |                 |
|                 | in KRaft) Fully | r.AclAuthorizer |                 |
|                 | qualified class |                 |                 |
|                 | name of the     |                 |                 |
|                 | authorizer.     |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --author       | (DEPRECATED:    |                 | Configuration   |
| izer-properties | not supported   |                 |                 |
|                 | in KRaft)       |                 |                 |
|                 | key=val pairs   |                 |                 |
|                 | that will be    |                 |                 |
|                 | passed to       |                 |                 |
|                 | authorizer for  |                 |                 |
|                 | initialization. |                 |                 |
|                 | For the default |                 |                 |
|                 | authorizer in   |                 |                 |
|                 | ZK clsuters,    |                 |                 |
|                 | the example     |                 |                 |
|                 | values are:     |                 |                 |
|                 | zo              |                 |                 |
|                 | okeeper.connect |                 |                 |
|                 | =localhost:2181 |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+
| --zk-          | (DEPRECATED:    |                 | Configuration   |
| tls-config-file | not supported   |                 |                 |
|                 | in KRaft)       |                 |                 |
|                 | Identifies the  |                 |                 |
|                 | file where      |                 |                 |
|                 | ZooKeeper       |                 |                 |
|                 | client TLS      |                 |                 |
|                 | connectivity    |                 |                 |
|                 | properties for  |                 |                 |
|                 | the authorizer  |                 |                 |
|                 | are defined.    |                 |                 |
|                 | Any properties  |                 |                 |
|                 | other than the  |                 |                 |
|                 | following (with |                 |                 |
|                 | or without an   |                 |                 |
|                 | &quot;authorizer.&quot; |                 |                 |
|                 | prefix) are     |                 |                 |
|                 | ignored:        |                 |                 |
|                 | zookeeper.cl    |                 |                 |
|                 | ientCnxnSocket, |                 |                 |
|                 | zookeeper.ssl   |                 |                 |
|                 | .cipher.suites, |                 |                 |
|                 | zookeeper.ssl   |                 |                 |
|                 | .client.enable, |                 |                 |
|                 | zookeeper.      |                 |                 |
|                 | ssl.crl.enable, |                 |                 |
|                 | zo              |                 |                 |
|                 | okeeper.ssl.ena |                 |                 |
|                 | bled.protocols, |                 |                 |
|                 | zoo             |                 |                 |
|                 | keeper.ssl.endp |                 |                 |
|                 | oint.identifica |                 |                 |
|                 | tion.algorithm, |                 |                 |
|                 | zo              |                 |                 |
|                 | okeeper.ssl.key |                 |                 |
|                 | store.location, |                 |                 |
|                 | zo              |                 |                 |
|                 | okeeper.ssl.key |                 |                 |
|                 | store.password, |                 |                 |
|                 | zookeeper.ssl   |                 |                 |
|                 | .keystore.type, |                 |                 |
|                 | zookeeper.s     |                 |                 |
|                 | sl.ocsp.enable, |                 |                 |
|                 | zookeepe        |                 |                 |
|                 | r.ssl.protocol, |                 |                 |
|                 | zook            |                 |                 |
|                 | eeper.ssl.trust |                 |                 |
|                 | store.location, |                 |                 |
|                 | zook            |                 |                 |
|                 | eeper.ssl.trust |                 |                 |
|                 | store.password, |                 |                 |
|                 | zookeeper.ssl.  |                 |                 |
|                 | truststore.type |                 |                 |
+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;+</p>
<h3 id="security_authz_examples">
  Examples
  <a class="anchor" href="#security_authz_examples">#</a>
</h3>
<ul>
<li>
<p><strong>Adding Acls</strong><br>
Suppose you want to add an acl &quot;Principals User:Bob and User:Alice
are allowed to perform Operation Read and Write on Topic Test-Topic
from IP 198.51.100.0 and IP 198.51.100.1&quot;. You can do that by
executing the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic
</code></pre><p>By default, all principals that don't have an explicit acl that
allows access for an operation to a resource are denied. In rare
cases where an allow acl is defined that allows access to all but
some principal we will have to use the --deny-principal and
--deny-host option. For example, if we want to allow all users to
Read from Test-topic but only deny User:BadBob from IP 198.51.100.3
we can do so using following commands:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:&#39;*&#39; --allow-host &#39;*&#39; --deny-principal User:BadBob --deny-host 198.51.100.3 --operation Read --topic Test-topic
</code></pre><p>Note that <code>--allow-host</code> and <code>--deny-host</code> only support IP addresses
(hostnames are not supported). Above examples add acls to a topic by
specifying --topic [topic-name] as the resource pattern option.
Similarly user can add acls to cluster by specifying --cluster and
to a consumer group by specifying --group [group-name]. You can
add acls on any resource of a certain type, e.g. suppose you wanted
to add an acl &quot;Principal User:Peter is allowed to produce to any
Topic from IP 198.51.200.0&quot; You can do that by using the wildcard
resource '*', e.g. by executing the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Peter --allow-host 198.51.200.1 --producer --topic &#39;*&#39;
</code></pre><p>You can add acls on prefixed resource patterns, e.g. suppose you
want to add an acl &quot;Principal User:Jane is allowed to produce to
any Topic whose name starts with 'Test-' from any host&quot;. You can
do that by executing the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Jane --producer --topic Test- --resource-pattern-type prefixed
</code></pre><p>Note, --resource-pattern-type defaults to 'literal', which only
affects resources with the exact same name or, in the case of the
wildcard resource name '*', a resource with any name.</p>
</li>
<li>
<p><strong>Removing Acls</strong><br>
Removing acls is pretty much the same. The only difference is
instead of --add option users will have to specify --remove
option. To remove the acls added by the first example above we can
execute the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --remove --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic 
</code></pre><p>If you want to remove the acl added to the prefixed resource pattern
above we can execute the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --remove --allow-principal User:Jane --producer --topic Test- --resource-pattern-type Prefixed
</code></pre></li>
<li>
<p><strong>List Acls</strong><br>
We can list acls for any resource by specifying the --list option
with the resource. To list all acls on the literal resource pattern
Test-topic, we can execute the CLI with following options:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --list --topic Test-topic
</code></pre><p>However, this will only return the acls that have been added to this
exact resource pattern. Other acls can exist that affect access to
the topic, e.g. any acls on the topic wildcard '*', or any acls
on prefixed resource patterns. Acls on the wildcard resource pattern
can be queried explicitly:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --list --topic &#39;*&#39;
</code></pre><p>However, it is not necessarily possible to explicitly query for acls
on prefixed resource patterns that match Test-topic as the name of
such patterns may not be known. We can list <em>all</em> acls affecting
Test-topic by using '--resource-pattern-type match', e.g.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --list --topic Test-topic --resource-pattern-type match
</code></pre><p>This will list acls on all matching literal, wildcard and prefixed
resource patterns.</p>
</li>
<li>
<p><strong>Adding or removing a principal as producer or consumer</strong><br>
The most common use case for acl management are adding/removing a
principal as producer or consumer so we added convenience options to
handle these cases. In order to add User:Bob as a producer of
Test-topic we can execute the following command:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Bob --producer --topic Test-topic
</code></pre><p>Similarly to add Alice as a consumer of Test-topic with consumer
group Group-1 we just have to pass --consumer option:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/kafka-acls.sh --bootstrap-server localhost:9092 --add --allow-principal User:Bob --consumer --topic Test-topic --group Group-1 
</code></pre><p>Note that for consumer option we must also specify the consumer
group. In order to remove a principal from producer or consumer role
we just need to pass --remove option.</p>
</li>
<li>
<p><strong>Admin API based acl management</strong><br>
Users having Alter permission on ClusterResource can use Admin API
for ACL management. kafka-acls.sh script supports AdminClient API to
manage ACLs without interacting with zookeeper/authorizer directly.
All the above examples can be executed by using
<strong>--bootstrap-server</strong> option. For example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config /tmp/adminclient-configs.conf --add --allow-principal User:Bob --producer --topic Test-topic
bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config /tmp/adminclient-configs.conf --add --allow-principal User:Bob --consumer --topic Test-topic --group Group-1
bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config /tmp/adminclient-configs.conf --list --topic Test-topic
bin/kafka-acls.sh --bootstrap-server localhost:9092 --command-config /tmp/adminclient-configs.conf --add --allow-principal User:tokenRequester --operation CreateTokens --user-principal &#34;owner1&#34;
</code></pre></li>
</ul>
<h3 id="security_authz_primitives">
  Authorization Primitives
  <a class="anchor" href="#security_authz_primitives">#</a>
</h3>
<p>Protocol calls are usually performing some operations on certain
resources in Kafka. It is required to know the operations and resources
to set up effective protection. In this section we'll list these
operations and resources, then list the combination of these with the
protocols to see the valid scenarios.</p>
<h4 id="operations_in_kafka">
  Operations in Kafka
  <a class="anchor" href="#operations_in_kafka">#</a>
</h4>
<p>There are a few operation primitives that can be used to build up
privileges. These can be matched up with certain resources to allow
specific protocol calls for a given user. These are:</p>
<ul>
<li>Read</li>
<li>Write</li>
<li>Create</li>
<li>Delete</li>
<li>Alter</li>
<li>Describe</li>
<li>ClusterAction</li>
<li>DescribeConfigs</li>
<li>AlterConfigs</li>
<li>IdempotentWrite</li>
<li>CreateTokens</li>
<li>DescribeTokens</li>
<li>All</li>
</ul>
<h4 id="resources_in_kafka">
  Resources in Kafka
  <a class="anchor" href="#resources_in_kafka">#</a>
</h4>
<p>The operations above can be applied on certain resources which are
described below.</p>
<ul>
<li><strong>Topic:</strong> this simply represents a Topic. All protocol calls that
are acting on topics (such as reading, writing them) require the
corresponding privilege to be added. If there is an authorization
error with a topic resource, then a TOPIC_AUTHORIZATION_FAILED
(error code: 29) will be returned.</li>
<li><strong>Group:</strong> this represents the consumer groups in the brokers. All
protocol calls that are working with consumer groups, like joining a
group must have privileges with the group in subject. If the
privilege is not given then a GROUP_AUTHORIZATION_FAILED (error
code: 30) will be returned in the protocol response.</li>
<li><strong>Cluster:</strong> this resource represents the cluster. Operations that
are affecting the whole cluster, like controlled shutdown are
protected by privileges on the Cluster resource. If there is an
authorization problem on a cluster resource, then a
CLUSTER_AUTHORIZATION_FAILED (error code: 31) will be returned.</li>
<li><strong>TransactionalId:</strong> this resource represents actions related to
transactions, such as committing. If any error occurs, then a
TRANSACTIONAL_ID_AUTHORIZATION_FAILED (error code: 53) will be
returned by brokers.</li>
<li><strong>DelegationToken:</strong> this represents the delegation tokens in the
cluster. Actions, such as describing delegation tokens could be
protected by a privilege on the DelegationToken resource. Since
these objects have a little special behavior in Kafka it is
recommended to read
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-48&#43;Delegation&#43;token&#43;support&#43;for&#43;Kafka#KIP-48DelegationtokensupportforKafka-DescribeDelegationTokenRequest">KIP-48</a>
and the related upstream documentation at <a href="#security_delegation_token">Authentication using Delegation Tokens</a>.</li>
<li><strong>User:</strong> CreateToken and DescribeToken operations can be granted to
User resources to allow creating and describing tokens for other
users. More info can be found in
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-373%3A&#43;Allow&#43;users&#43;to&#43;create&#43;delegation&#43;tokens&#43;for&#43;other&#43;users">KIP-373</a>.</li>
</ul>
<h4 id="operations_resources_and_protocols">
  Operations and Resources on Protocols
  <a class="anchor" href="#operations_resources_and_protocols">#</a>
</h4>
<p>In the below table we'll list the valid operations on resources that
are executed by the Kafka API protocols.</p>
<p>Protocol (API key)                   Operation         Resource          Note</p>
<hr>
<p>PRODUCE (0)                          Write             TransactionalId   An transactional producer which has its transactional.id set requires this privilege.
PRODUCE (0)                          IdempotentWrite   Cluster           An idempotent produce action requires this privilege.
PRODUCE (0)                          Write             Topic             This applies to a normal produce action.
FETCH (1)                            ClusterAction     Cluster           A follower must have ClusterAction on the Cluster resource in order to fetch partition data.
FETCH (1)                            Read              Topic             Regular Kafka consumers need READ permission on each partition they are fetching.
LIST_OFFSETS (2)                     Describe          Topic          <br>
METADATA (3)                         Describe          Topic          <br>
METADATA (3)                         Create            Cluster           If topic auto-creation is enabled, then the broker-side API will check for the existence of a Cluster level privilege. If it's found then it'll allow creating the topic, otherwise it'll iterate through the Topic level privileges (see the next one).
METADATA (3)                         Create            Topic             This authorizes auto topic creation if enabled but the given user doesn't have a cluster level permission (above).
LEADER_AND_ISR (4)                   ClusterAction     Cluster        <br>
STOP_REPLICA (5)                     ClusterAction     Cluster        <br>
UPDATE_METADATA (6)                  ClusterAction     Cluster        <br>
CONTROLLED_SHUTDOWN (7)              ClusterAction     Cluster        <br>
OFFSET_COMMIT (8)                    Read              Group             An offset can only be committed if it's authorized to the given group and the topic too (see below). Group access is checked first, then Topic access.
OFFSET_COMMIT (8)                    Read              Topic             Since offset commit is part of the consuming process, it needs privileges for the read action.
OFFSET_FETCH (9)                     Describe          Group             Similarly to OFFSET_COMMIT, the application must have privileges on group and topic level too to be able to fetch. However in this case it requires describe access instead of read. Group access is checked first, then Topic access.
OFFSET_FETCH (9)                     Describe          Topic          <br>
FIND_COORDINATOR (10)                Describe          Group             The FIND_COORDINATOR request can be of &quot;Group&quot; type in which case it is looking for consumergroup coordinators. This privilege would represent the Group mode.
FIND_COORDINATOR (10)                Describe          TransactionalId   This applies only on transactional producers and checked when a producer tries to find the transaction coordinator.
JOIN_GROUP (11)                      Read              Group          <br>
HEARTBEAT (12)                       Read              Group          <br>
LEAVE_GROUP (13)                     Read              Group          <br>
SYNC_GROUP (14)                      Read              Group          <br>
DESCRIBE_GROUPS (15)                 Describe          Group          <br>
LIST_GROUPS (16)                     Describe          Cluster           When the broker checks to authorize a list_groups request it first checks for this cluster level authorization. If none found then it proceeds to check the groups individually. This operation doesn't return CLUSTER_AUTHORIZATION_FAILED.
LIST_GROUPS (16)                     Describe          Group             If none of the groups are authorized, then just an empty response will be sent back instead of an error. This operation doesn't return CLUSTER_AUTHORIZATION_FAILED. This is applicable from the 2.1 release.
SASL_HANDSHAKE (17)                                                      The SASL handshake is part of the authentication process and therefore it's not possible to apply any kind of authorization here.
API_VERSIONS (18)                                                        The API_VERSIONS request is part of the Kafka protocol handshake and happens on connection and before any authentication. Therefore it's not possible to control this with authorization.
CREATE_TOPICS (19)                   Create            Cluster           If there is no cluster level authorization then it won't return CLUSTER_AUTHORIZATION_FAILED but fall back to use topic level, which is just below. That'll throw error if there is a problem.
CREATE_TOPICS (19)                   Create            Topic             This is applicable from the 2.0 release.
DELETE_TOPICS (20)                   Delete            Topic          <br>
DELETE_RECORDS (21)                  Delete            Topic          <br>
INIT_PRODUCER_ID (22)                Write             TransactionalId<br>
INIT_PRODUCER_ID (22)                IdempotentWrite   Cluster        <br>
OFFSET_FOR_LEADER_EPOCH (23)         ClusterAction     Cluster           If there is no cluster level privilege for this operation, then it'll check for topic level one.
OFFSET_FOR_LEADER_EPOCH (23)         Describe          Topic             This is applicable from the 2.1 release.
ADD_PARTITIONS_TO_TXN (24)           Write             TransactionalId   This API is only applicable to transactional requests. It first checks for the Write action on the TransactionalId resource, then it checks the Topic in subject (below).
ADD_PARTITIONS_TO_TXN (24)           Write             Topic          <br>
ADD_OFFSETS_TO_TXN (25)              Write             TransactionalId   Similarly to ADD_PARTITIONS_TO_TXN this is only applicable to transactional request. It first checks for Write action on the TransactionalId resource, then it checks whether it can Read on the given group (below).
ADD_OFFSETS_TO_TXN (25)              Read              Group          <br>
END_TXN (26)                         Write             TransactionalId<br>
WRITE_TXN_MARKERS (27)               ClusterAction     Cluster        <br>
TXN_OFFSET_COMMIT (28)               Write             TransactionalId<br>
TXN_OFFSET_COMMIT (28)               Read              Group          <br>
TXN_OFFSET_COMMIT (28)               Read              Topic          <br>
DESCRIBE_ACLS (29)                   Describe          Cluster        <br>
CREATE_ACLS (30)                     Alter             Cluster        <br>
DELETE_ACLS (31)                     Alter             Cluster        <br>
DESCRIBE_CONFIGS (32)                DescribeConfigs   Cluster           If broker configs are requested, then the broker will check cluster level privileges.
DESCRIBE_CONFIGS (32)                DescribeConfigs   Topic             If topic configs are requested, then the broker will check topic level privileges.
ALTER_CONFIGS (33)                   AlterConfigs      Cluster           If broker configs are altered, then the broker will check cluster level privileges.
ALTER_CONFIGS (33)                   AlterConfigs      Topic             If topic configs are altered, then the broker will check topic level privileges.
ALTER_REPLICA_LOG_DIRS (34)          Alter             Cluster        <br>
DESCRIBE_LOG_DIRS (35)               Describe          Cluster           An empty response will be returned on authorization failure.
SASL_AUTHENTICATE (36)                                                   SASL_AUTHENTICATE is part of the authentication process and therefore it's not possible to apply any kind of authorization here.
CREATE_PARTITIONS (37)               Alter             Topic          <br>
CREATE_DELEGATION_TOKEN (38)                                             Creating delegation tokens has special rules, for this please see the <a href="#security_delegation_token">Authentication using Delegation Tokens</a>{#security_delegation_token_1} section.
CREATE_DELEGATION_TOKEN (38)         CreateTokens      User              Allows creating delegation tokens for the User resource.
RENEW_DELEGATION_TOKEN (39)                                              Renewing delegation tokens has special rules, for this please see the <a href="#security_delegation_token">Authentication using Delegation Tokens</a>{#security_delegation_token_2} section.
EXPIRE_DELEGATION_TOKEN (40)                                             Expiring delegation tokens has special rules, for this please see the <a href="#security_delegation_token">Authentication using Delegation Tokens</a>{#security_delegation_token_3} section.
DESCRIBE_DELEGATION_TOKEN (41)       Describe          DelegationToken   Describing delegation tokens has special rules, for this please see the <a href="#security_delegation_token">Authentication using Delegation Tokens</a>{#security_delegation_token_4} section.
DESCRIBE_DELEGATION_TOKEN (41)       DescribeTokens    User              Allows describing delegation tokens of the User resource.
DELETE_GROUPS (42)                   Delete            Group          <br>
ELECT_PREFERRED_LEADERS (43)         ClusterAction     Cluster        <br>
INCREMENTAL_ALTER_CONFIGS (44)       AlterConfigs      Cluster           If broker configs are altered, then the broker will check cluster level privileges.
INCREMENTAL_ALTER_CONFIGS (44)       AlterConfigs      Topic             If topic configs are altered, then the broker will check topic level privileges.
ALTER_PARTITION_REASSIGNMENTS (45)   Alter             Cluster        <br>
LIST_PARTITION_REASSIGNMENTS (46)    Describe          Cluster        <br>
OFFSET_DELETE (47)                   Delete            Group          <br>
OFFSET_DELETE (47)                   Read              Topic</p>
<h2 id="security_rolling_upgrade">
  7.6 Incorporating Security Features in a Running Cluster
  <a class="anchor" href="#security_rolling_upgrade">#</a>
</h2>
<p>You can secure a running cluster via one or more of the supported
protocols discussed previously. This is done in phases:</p>
<ul>
<li>Incrementally bounce the cluster nodes to open additional secured
port(s).</li>
<li>Restart clients using the secured rather than PLAINTEXT port
(assuming you are securing the client-broker connection).</li>
<li>Incrementally bounce the cluster again to enable broker-to-broker
security (if this is required)</li>
<li>A final incremental bounce to close the PLAINTEXT port.</li>
</ul>
<p>The specific steps for configuring SSL and SASL are described in
sections <a href="#security_ssl">7.3</a> and <a href="#security_sasl">7.4</a>. Follow these
steps to enable security for your desired protocol(s).</p>
<p>The security implementation lets you configure different protocols for
both broker-client and broker-broker communication. These must be
enabled in separate bounces. A PLAINTEXT port must be left open
throughout so brokers and/or clients can continue to communicate.</p>
<p>When performing an incremental bounce stop the brokers cleanly via a
SIGTERM. It's also good practice to wait for restarted replicas to
return to the ISR list before moving onto the next node.</p>
<p>As an example, say we wish to encrypt both broker-client and
broker-broker communication with SSL. In the first incremental bounce,
an SSL port is opened on each node:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092
</code></pre><p>We then restart the clients, changing their config to point at the newly
opened, secured port:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">bootstrap.servers = [broker1:9092,...]
security.protocol = SSL
...etc
</code></pre><p>In the second incremental server bounce we instruct Kafka to use SSL as
the broker-broker protocol (which will use the same SSL port):</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092
security.inter.broker.protocol=SSL
</code></pre><p>In the final bounce we secure the cluster by closing the PLAINTEXT port:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SSL://broker1:9092
security.inter.broker.protocol=SSL
</code></pre><p>Alternatively we might choose to open multiple ports so that different
protocols can be used for broker-broker and broker-client communication.
Say we wished to use SSL encryption throughout (i.e. for broker-broker
and broker-client communication) but we'd like to add SASL
authentication to the broker-client connection also. We would achieve
this by opening two additional ports during the first bounce:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092,SASL_SSL://broker1:9093
</code></pre><p>We would then restart the clients, changing their config to point at the
newly opened, SASL &amp; SSL secured port:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">bootstrap.servers = [broker1:9093,...]
security.protocol = SASL_SSL
...etc
</code></pre><p>The second server bounce would switch the cluster to use encrypted
broker-broker communication via the SSL port we previously opened on
port 9092:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=PLAINTEXT://broker1:9091,SSL://broker1:9092,SASL_SSL://broker1:9093
security.inter.broker.protocol=SSL
</code></pre><p>The final bounce secures the cluster by closing the PLAINTEXT port.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">listeners=SSL://broker1:9092,SASL_SSL://broker1:9093
security.inter.broker.protocol=SSL
</code></pre><p>ZooKeeper can be secured independently of the Kafka cluster. The steps
for doing this are covered in section <a href="#zk_authz_migration">7.7.2</a>.</p>
<h2 id="zk_authz">
  7.7 ZooKeeper Authentication
  <a class="anchor" href="#zk_authz">#</a>
</h2>
<p>ZooKeeper supports mutual TLS (mTLS) authentication beginning with the
3.5.x versions. Kafka supports authenticating to ZooKeeper with SASL and
mTLS -- either individually or both together -- beginning with version
2.5. See
<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-515%3A&#43;Enable&#43;ZK&#43;client&#43;to&#43;use&#43;the&#43;new&#43;TLS&#43;supported&#43;authentication">KIP-515: Enable ZK client to use the new TLS supported authentication</a>
for more details.</p>
<p>When using mTLS alone, every broker and any CLI tools (such as the
<a href="#zk_authz_migration">ZooKeeper Security Migration Tool</a>) should
identify itself with the same Distinguished Name (DN) because it is the
DN that is ACL'ed. This can be changed as described below, but it
involves writing and deploying a custom ZooKeeper authentication
provider. Generally each certificate should have the same DN but a
different Subject Alternative Name (SAN) so that hostname verification
of the brokers and any CLI tools by ZooKeeper will succeed.</p>
<p>When using SASL authentication to ZooKeeper together with mTLS, both the
SASL identity and either the DN that created the znode (i.e. the
creating broker's certificate) or the DN of the Security Migration Tool
(if migration was performed after the znode was created) will be
ACL'ed, and all brokers and CLI tools will be authorized even if they
all use different DNs because they will all use the same ACL'ed SASL
identity. It is only when using mTLS authentication alone that all the
DNs must match (and SANs become critical -- again, in the absence of
writing and deploying a custom ZooKeeper authentication provider as
described below).</p>
<p>Use the broker properties file to set TLS configs for brokers as
described below.</p>
<p>Use the <code>--zk-tls-config-file &lt;file&gt;</code> option to set TLS configs in the
Zookeeper Security Migration Tool. The <code>kafka-acls.sh</code> and
<code>kafka-configs.sh</code> CLI tools also support the
<code>--zk-tls-config-file &lt;file&gt;</code> option.</p>
<p>Use the <code>-zk-tls-config-file &lt;file&gt;</code> option (note the single-dash rather
than double-dash) to set TLS configs for the <code>zookeeper-shell.sh</code> CLI
tool.</p>
<h3 id="zk_authz_new">
  7.7.1 New clusters
  <a class="anchor" href="#zk_authz_new">#</a>
</h3>
<h4 id="zk_authz_new_sasl">
  7.7.1.1 ZooKeeper SASL Authentication
  <a class="anchor" href="#zk_authz_new_sasl">#</a>
</h4>
<p>To enable ZooKeeper SASL authentication on brokers, there are two
necessary steps:</p>
<ol>
<li>Create a JAAS login file and set the appropriate system property to
point to it as described above</li>
<li>Set the configuration property <code>zookeeper.set.acl</code> in each broker to
true</li>
</ol>
<p>The metadata stored in ZooKeeper for the Kafka cluster is
world-readable, but can only be modified by the brokers. The rationale
behind this decision is that the data stored in ZooKeeper is not
sensitive, but inappropriate manipulation of that data can cause cluster
disruption. We also recommend limiting the access to ZooKeeper via
network segmentation (only brokers and some admin tools need access to
ZooKeeper).</p>
<h4 id="zk_authz_new_mtls">
  7.7.1.2 ZooKeeper Mutual TLS Authentication
  <a class="anchor" href="#zk_authz_new_mtls">#</a>
</h4>
<p>ZooKeeper mTLS authentication can be enabled with or without SASL
authentication. As mentioned above, when using mTLS alone, every broker
and any CLI tools (such as the <a href="#zk_authz_migration">ZooKeeper Security Migration
Tool</a>) must generally identify itself with the same
Distinguished Name (DN) because it is the DN that is ACL'ed, which
means each certificate should have an appropriate Subject Alternative
Name (SAN) so that hostname verification of the brokers and any CLI tool
by ZooKeeper will succeed.</p>
<p>It is possible to use something other than the DN for the identity of
mTLS clients by writing a class that extends
<code>org.apache.zookeeper.server.auth.X509AuthenticationProvider</code> and
overrides the method
<code>protected String getClientId(X509Certificate clientCert)</code>. Choose a
scheme name and set <code>authProvider.[scheme]</code> in ZooKeeper to be the
fully-qualified class name of the custom implementation; then set
<code>ssl.authProvider=[scheme]</code> to use it.</p>
<p>Here is a sample (partial) ZooKeeper configuration for enabling TLS
authentication. These configurations are described in the <a href="https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_authOptions">ZooKeeper
Admin
Guide</a>.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">secureClientPort=2182
serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory
authProvider.x509=org.apache.zookeeper.server.auth.X509AuthenticationProvider
ssl.keyStore.location=/path/to/zk/keystore.jks
ssl.keyStore.password=zk-ks-passwd
ssl.trustStore.location=/path/to/zk/truststore.jks
ssl.trustStore.password=zk-ts-passwd
</code></pre><p><strong>IMPORTANT</strong>: ZooKeeper does not support setting the key password in
the ZooKeeper server keystore to a value different from the keystore
password itself. Be sure to set the key password to be the same as the
keystore password.</p>
<p>Here is a sample (partial) Kafka Broker configuration for connecting to
ZooKeeper with mTLS authentication. These configurations are described
above in <a href="#brokerconfigs">Broker Configs</a>.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers"># connect to the ZooKeeper port configured for TLS
zookeeper.connect=zk1:2182,zk2:2182,zk3:2182
# required to use TLS to ZooKeeper (default is false)
zookeeper.ssl.client.enable=true
# required to use TLS to ZooKeeper
zookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty
# define key/trust stores to use TLS to ZooKeeper; ignored unless zookeeper.ssl.client.enable=true
zookeeper.ssl.keystore.location=/path/to/kafka/keystore.jks
zookeeper.ssl.keystore.password=kafka-ks-passwd
zookeeper.ssl.truststore.location=/path/to/kafka/truststore.jks
zookeeper.ssl.truststore.password=kafka-ts-passwd
# tell broker to create ACLs on znodes
zookeeper.set.acl=true
</code></pre><p><strong>IMPORTANT</strong>: ZooKeeper does not support setting the key password in
the ZooKeeper client (i.e. broker) keystore to a value different from
the keystore password itself. Be sure to set the key password to be the
same as the keystore password.</p>
<h3 id="zk_authz_migration">
  7.7.2 Migrating clusters
  <a class="anchor" href="#zk_authz_migration">#</a>
</h3>
<p>If you are running a version of Kafka that does not support security or
simply with security disabled, and you want to make the cluster secure,
then you need to execute the following steps to enable ZooKeeper
authentication with minimal disruption to your operations:</p>
<ol>
<li>
<p>Enable SASL and/or mTLS authentication on ZooKeeper. If enabling
mTLS, you would now have both a non-TLS port and a TLS port, like
this:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">clientPort=2181
secureClientPort=2182
serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory
authProvider.x509=org.apache.zookeeper.server.auth.X509AuthenticationProvider
ssl.keyStore.location=/path/to/zk/keystore.jks
ssl.keyStore.password=zk-ks-passwd
ssl.trustStore.location=/path/to/zk/truststore.jks
ssl.trustStore.password=zk-ts-passwd
</code></pre></li>
<li>
<p>Perform a rolling restart of brokers setting the JAAS login file
and/or defining ZooKeeper mutual TLS configurations (including
connecting to the TLS-enabled ZooKeeper port) as required, which
enables brokers to authenticate to ZooKeeper. At the end of the
rolling restart, brokers are able to manipulate znodes with strict
ACLs, but they will not create znodes with those ACLs</p>
</li>
<li>
<p>If you enabled mTLS, disable the non-TLS port in ZooKeeper</p>
</li>
<li>
<p>Perform a second rolling restart of brokers, this time setting the
configuration parameter <code>zookeeper.set.acl</code> to true, which enables
the use of secure ACLs when creating znodes</p>
</li>
<li>
<p>Execute the ZkSecurityMigrator tool. To execute the tool, there is
this script: <code>bin/zookeeper-security-migration.sh</code> with
<code>zookeeper.acl</code> set to secure. This tool traverses the corresponding
sub-trees changing the ACLs of the znodes. Use the
<code>--zk-tls-config-file &lt;file&gt;</code> option if you enable mTLS.</p>
</li>
</ol>
<p>It is also possible to turn off authentication in a secure cluster. To
do it, follow these steps:</p>
<ol>
<li>Perform a rolling restart of brokers setting the JAAS login file
and/or defining ZooKeeper mutual TLS configurations, which enables
brokers to authenticate, but setting <code>zookeeper.set.acl</code> to false.
At the end of the rolling restart, brokers stop creating znodes with
secure ACLs, but are still able to authenticate and manipulate all
znodes</li>
<li>Execute the ZkSecurityMigrator tool. To execute the tool, run this
script <code>bin/zookeeper-security-migration.sh</code> with <code>zookeeper.acl</code>
set to unsecure. This tool traverses the corresponding sub-trees
changing the ACLs of the znodes. Use the
<code>--zk-tls-config-file &lt;file&gt;</code> option if you need to set TLS
configuration.</li>
<li>If you are disabling mTLS, enable the non-TLS port in ZooKeeper</li>
<li>Perform a second rolling restart of brokers, this time omitting the
system property that sets the JAAS login file and/or removing
ZooKeeper mutual TLS configuration (including connecting to the
non-TLS-enabled ZooKeeper port) as required</li>
<li>If you are disabling mTLS, disable the TLS port in ZooKeeper</li>
</ol>
<p>Here is an example of how to run the migration tool:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/zookeeper-security-migration.sh --zookeeper.acl=secure --zookeeper.connect=localhost:2181
</code></pre><p>Run this to see the full list of parameters:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">&gt; bin/zookeeper-security-migration.sh --help
</code></pre><h3 id="zk_authz_ensemble">
  7.7.3 Migrating the ZooKeeper ensemble
  <a class="anchor" href="#zk_authz_ensemble">#</a>
</h3>
<p>It is also necessary to enable SASL and/or mTLS authentication on the
ZooKeeper ensemble. To do it, we need to perform a rolling restart of
the server and set a few properties. See above for mTLS information.
Please refer to the ZooKeeper documentation for more detail:</p>
<ol>
<li><a href="https://zookeeper.apache.org/doc/r3.5.7/zookeeperProgrammers.html#sc_ZooKeeperAccessControl">Apache ZooKeeper documentation</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zookeeper&#43;and&#43;SASL">Apache ZooKeeper wiki</a></li>
</ol>
<h3 id="zk_authz_quorum">
  7.7.4 ZooKeeper Quorum Mutual TLS Authentication
  <a class="anchor" href="#zk_authz_quorum">#</a>
</h3>
<p>It is possible to enable mTLS authentication between the ZooKeeper
servers themselves. Please refer to the <a href="https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#Quorum&#43;TLS">ZooKeeper
documentation</a>
for more detail.</p>
<h2 id="zk_encryption">
  7.8 ZooKeeper Encryption
  <a class="anchor" href="#zk_encryption">#</a>
</h2>
<p>ZooKeeper connections that use mutual TLS are encrypted. Beginning with
ZooKeeper version 3.5.7 (the version shipped with Kafka version 2.5)
ZooKeeper supports a sever-side config <code>ssl.clientAuth</code>
(case-insensitively: <code>want</code>/<code>need</code>/<code>none</code> are the valid options, the
default is <code>need</code>), and setting this value to <code>none</code> in ZooKeeper allows
clients to connect via a TLS-encrypted connection without presenting
their own certificate. Here is a sample (partial) Kafka Broker
configuration for connecting to ZooKeeper with just TLS encryption.
These configurations are described above in <a href="#brokerconfigs">Broker
Configs</a>.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers"># connect to the ZooKeeper port configured for TLS
zookeeper.connect=zk1:2182,zk2:2182,zk3:2182
# required to use TLS to ZooKeeper (default is false)
zookeeper.ssl.client.enable=true
# required to use TLS to ZooKeeper
zookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty
# define trust stores to use TLS to ZooKeeper; ignored unless zookeeper.ssl.client.enable=true
# no need to set keystore information assuming ssl.clientAuth=none on ZooKeeper
zookeeper.ssl.truststore.location=/path/to/kafka/truststore.jks
zookeeper.ssl.truststore.password=kafka-ts-passwd
# tell broker to create ACLs on znodes (if using SASL authentication, otherwise do not set this)
zookeeper.set.acl=true
</code></pre></article>
        </div>
    </div>
</div>
<div class="footer">
    <div class="footer__inner">
        <div class="footer__legal">
            <span class="footer__legal__one">The contents of this website are &copy; 2023 <a
                    href="https://www.apache.org/" target="_blank">Apache Software Foundation</a> under the terms of the <a
                    href="https://www.apache.org/licenses/LICENSE-2.0.html"
                    target="_blank">Apache License v2</a>.</span>
            <span class="footer__legal__two">Apache Kafka, Kafka, and the Kafka logo are either registered trademarks or trademarks of The Apache Software Foundation</span>
            <span class="footer__legal__three">in the United States and other countries.</span>
            <div>
                <a href="https://kafka.apache.org/project-security" target="_blank" rel="noreferrer">Security</a>&nbsp;|&nbsp;
                <a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noreferrer">Donate</a>&nbsp;|&nbsp;
                <a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noreferrer">Thanks</a>&nbsp;|&nbsp;
                <a href="https://apache.org/events/current-event" target="_blank" rel="noreferrer">Events</a>&nbsp;|&nbsp;
                <a href="https://apache.org/licenses/" target="_blank" rel="noreferrer">License</a>&nbsp;|&nbsp;
                <a href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank"
                   rel="noreferrer">Privacy</a>
            </div>
        </div>
        <a class="apache-feather" target="_blank" href="http://www.apache.org">
            <img width="40" src="http://localhost:8080/35/documentation/images/feather-small.png" alt="Apache Feather">
        </a>
    </div>
</div>


<script src="http://localhost:8080/35/documentation/js/handlebars.js"></script>
<script>
			$(function () {
				
				var templates = [
					'introduction',
					'implementation',
					'design',
					'api',
					'configuration',
					'ops',
					'security',
					'connect',
					'streams',
					'quickstart',
					'quickstart-docker',
					'toc',
					'upgrade',
					'content'
				];

				
				for(var i = 0; i < templates.length; i++) {
					var templateScript = $("#" + templates[i] + "-template").html();
					if(templateScript) {
						var template = Handlebars.compile(templateScript);
						var html = template(context);
						$(".p-" + templates[i]).html(html);
					}
				}
			});






</script>

<script src="http://localhost:8080/35/documentation/js/jquery.sticky-kit.min.js"></script>
<script>

			let mobileScrolling = false;
			let mobileTimeout;

			
			function checkPageScroll() {
				if (window.scrollY >= 80 || document.body.scrollTop >= 100) {
					document.getElementById('header').classList.add('scrolled');
				} else {
					document.getElementById('header').classList.remove('scrolled');
				}
			}

			function mobilePageScroll() {
				mobileScrolling = true;
				mobileTimeout = null;
				mobileTimeout = setTimeout(checkPageScroll, 200);
			}

			function pageScroll(e) {
				requestAnimationFrame(checkPageScroll);
			};

			function setUpA11yMenus(nav) {
				const itemsWithMenu = nav.querySelectorAll('[aria-haspopup="true"]');
				const topLevelItems = nav.querySelectorAll('.top-nav-item-anchor');
				const navAnchors = nav.querySelectorAll('a');
				const menus = nav.querySelectorAll('[role="menu"]');
				const keyMap = {
					
					'left': 37, 'up': 38, 'right': 39, 'down': 40,
					
					'tab': 9, 'enter': 13, 'esc': 27, 'space': 32,
				}
				
				const keyCodes = [37, 38, 39, 40, 9, 13, 27, 32];

				
				nav.addEventListener('click', (e) => {
					if (e.target.hasAttribute('href') && e.target.getAttribute('href') === '#') {
						e.preventDefault();
						e.stopPropagation();
					}
				});

				for (let i = 0; i < topLevelItems.length; i++) {
					const item = topLevelItems[i];
					const attrNames = {
						currentFocus: 'data-current-focus',
						hidden: 'aria-hidden',
						expanded: 'aria-expanded',
					};
					
					
					const itemMenu = item.parentNode.querySelector('[role="menu"]');

					function selectPreviousMenuItem(e, keyPressed) {
						let selectedIndex = -1;

						if (!itemMenu) {
							selectPreviousTopLevelItem(e);
						}

						if (itemMenu && itemMenu.hasAttribute(attrNames.currentFocus)) {
							selectedIndex = parseInt(itemMenu.getAttribute(attrNames.currentFocus), 10);
						}
						let nextIndex = selectedIndex - 1;
						const menuItems = itemMenu.getElementsByTagName('a');

						if (nextIndex >= 0) {
						
							menuItems[nextIndex].focus();
							itemMenu.setAttribute(attrNames.currentFocus, nextIndex);
						} else if (nextIndex === -1) {
						
							item.focus();
							itemMenu.setAttribute(attrNames.currentFocus, -1);
						} else if (nextIndex === -2) {
							if (keyPressed === 'tab') {
								
								selectPreviousTopLevelItem(e);
							} else {
								
								menuItems[menuItems.length - 1].focus()
								itemMenu.setAttribute(attrNames.currentFocus, menuItems.length - 1);
							}
						}
					}

					function selectNextMenuItem(keyPressed) {
						let selectedIndex = -1;
						let nextIndex;
						let menuItems;

						if (!itemMenu) {
							selectNextTopLevelItem();
						}

						menuItems = itemMenu.getElementsByTagName('a');

						for (let j = 0; j < menuItems.length; j++) {
							if (menuItems[j] === document.activeElement) {
								selectedIndex = j;
								break;
							}
						}

						nextIndex = selectedIndex + 1;

						

						if (keyPressed === 'tab') {
							if (nextIndex >= menuItems.length) {
								selectNextTopLevelItem()
							} else {
								menuItems[nextIndex].focus();
							}
						} else {
							if (nextIndex < menuItems.length) {
								menuItems[nextIndex].focus();
							} else {
								item.focus();
							}
						}
					}

					function selectPreviousTopLevelItem() {
						let newIndex;

						for (let j = 0; j < topLevelItems.length; j++) {
							if (topLevelItems[j] === item) {
								newIndex = j - 1;
								break;
							}
						}

						if (itemMenu) {
							hideMenu();
						}

						if (newIndex < 0) {
							document.querySelector('.logo-link').focus();
						} else {
							topLevelItems[newIndex].focus();
						}
					}

					function selectNextTopLevelItem() {
						let newIndex;

						for (let j = 0; j < topLevelItems.length; j++) {
							if (topLevelItems[j] === item) {
								newIndex = j + 1;
								break;
							}
						}

						if (itemMenu) {
							hideMenu();
						}

						if (newIndex > topLevelItems.length - 1) {
							document.getElementById('top-nav-download').focus();
						} else {
							topLevelItems[newIndex].focus();
						}
					}

					function isMenuOpen() {
						return itemMenu && itemMenu.getAttribute(attrNames.hidden) === 'false';
					}

					function showMenu() {
						item.setAttribute(attrNames.expanded, 'true');
						if (itemMenu) {
							itemMenu.setAttribute(attrNames.hidden, 'false');
						}
					}

					function hideMenu() {
						item.setAttribute(attrNames.expanded, 'false');
						if (itemMenu) {
							itemMenu.setAttribute(attrNames.hidden, 'true');
							delete itemMenu.dataset.currentFocus;
						}
					}

					item.parentNode.addEventListener('mouseover', showMenu);
					item.parentNode.addEventListener('mouseout', hideMenu);
					item.parentNode.addEventListener('blur', hideMenu);
					item.addEventListener('focus', showMenu);

					
					item.parentNode.addEventListener('keydown', (e) => {
						
						if (keyCodes.indexOf(e.keyCode) > -1) {
							e.preventDefault();
							e.stopPropagation();

							switch(e.keyCode) {
								case keyMap.left:
									if (itemMenu) {
										if (!isMenuOpen()) {
											selectPreviousTopLevelItem();
										} else {
											hideMenu();
											item.focus();
										}
									} else {
										selectPreviousTopLevelItem();
									}
								break;
								case keyMap.up:
									if (itemMenu) {
										if (isMenuOpen()) {
											selectPreviousMenuItem(e, 'up');
										}
									}
								break;
								case keyMap.right:
									if (!isMenuOpen() || document.activeElement === item) {
										selectNextTopLevelItem('right');
									}
								break;
								case keyMap.down:
									if (itemMenu) {
										if (isMenuOpen()) {
											selectNextMenuItem('down');
										} else {
											showMenu();
										}
									}
								break;
								case keyMap.tab:
									const isShiftActive = e.getModifierState('Shift');
									if (itemMenu) {
										if (isShiftActive) {
											selectPreviousTopLevelItem();
										} else {
											if (isMenuOpen()) {
												selectNextMenuItem('tab');
											} else {
												showMenu();
											}
										}
									} else {
										if (isShiftActive) {
											selectPreviousTopLevelItem();
										} else {
											selectNextTopLevelItem('tab');
										}
									}
								break;
								case keyMap.esc:
									if (itemMenu && isMenuOpen()) {
										hideMenu();
									}
								break;
								default:
									
								break;
							}
						}
					});
				};
			}

			function is_touch_enabled() {
				return ( 'ontouchstart' in window ) ||
							( navigator.maxTouchPoints > 0 ) ||
							( navigator.msMaxTouchPoints > 0 );
			}

			function setupDocsNav() {
				var docsContainer = document.querySelector('.documentation--current');
				var docsHandle = document.querySelector('.toc-handle');

				function toggleDocsWidth() {
					let isExpanded = docsContainer.classList.toggle('expanded');
					
					if (isExpanded) {
						docsHandle.textContent = ">";
					} else {
						docsHandle.textContent = "<";
					}
				}

				docsHandle.addEventListener('click', toggleDocsWidth);
			}

			
			(function() {
				var navToggle = document.getElementById('top-nav-toggle');
				var nav = document.getElementById('top-nav-container');
				var docsNav = document.querySelector('.docs-nav');

				navToggle.addEventListener('click', () => {
					navToggle.classList.toggle('active');
				});

				
				if (is_touch_enabled()) {
					nav.querySelectorAll('a').forEach(anchor => anchor.removeAttribute('tabindex'));
					window.addEventListener('touchmove', mobilePageScroll, false);
				} else {
					setUpA11yMenus(nav);
					window.addEventListener('scroll', pageScroll, false);
				}

				
				document.addEventListener("DOMContentLoaded", function() {
					
					pageScroll();

					if (docsNav) {
						setupDocsNav();
					}

					
					
					
				});
			}());






</script>
</body>
</html>
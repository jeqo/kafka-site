<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Kafka documentation</title>
    <link>http://localhost:8080/35/documentation/streams/developer-guide/</link>
    <description>Recent content on Apache Kafka documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://localhost:8080/35/documentation/streams/developer-guide/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/app-reset-tool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/app-reset-tool/</guid>
      <description>Application Reset Tool # You can reset an application and force it to reprocess its data from scratch by using the application reset tool. This can be useful for development and testing, or when fixing bugs.
The application reset tool handles the Kafka Streams user topics (input, output, and intermediate topics) and internal topics differently when resetting the application.
Here&amp;rsquo;s what the application reset tool does for each topic type:</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/config-streams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/config-streams/</guid>
      <description>Configuring a Streams Application # Kafka and Kafka Streams configuration options must be configured before using Streams. You can configure Kafka Streams by specifying parameters in a java.util.Properties instance.
Create a java.util.Properties instance.
Set the parameters. For example:
import java.util.Properties; import org.apache.kafka.streams.StreamsConfig; Properties settings = new Properties(); // Set a few key parameters settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &amp;#34;my-first-streams-application&amp;#34;); settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;#34;kafka-broker1:9092&amp;#34;); // Any further settings settings.put(... , ...); Configuration parameter reference # This section contains the most common Streams configuration parameters.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/datatypes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/datatypes/</guid>
      <description>Data Types and Serialization # Every Kafka Streams application must provide Serdes (Serializer/Deserializer) for the data types of record keys and record values (e.g. java.lang.String) to materialize the data when necessary. Operations that require such Serdes information include: stream(), table(), to(), repartition(), groupByKey(), groupBy().
You can provide Serdes by using either of these methods, but you must use at least one:
By setting default Serdes in the java.util.Properties config instance.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/dsl-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/dsl-api/</guid>
      <description>Streams DSL # The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.
Table of Contents
Overview Creating source streams from Kafka Transform a stream Stateless transformations Stateful transformations Aggregating Joining Join co-partitioning requirements KStream-KStream Join KTable-KTable Equi-Join KTable-KTable Foreign-Key Join KStream-KTable Join KStream-GlobalKTable Join Windowing Hopping time windows Tumbling time windows Sliding time windows Session Windows Window Final Results Applying processors and transformers (Processor API integration) Naming Operators in a Streams DSL application Controlling KTable update rate Writing streams back to Kafka Testing a Streams application Kafka Streams DSL for Scala Sample Usage Implicit Serdes User-Defined Serdes Overview # In comparison to the Processor API, only the DSL supports:</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/dsl-topology-naming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/dsl-topology-naming/</guid>
      <description>Naming Operators in a Kafka Streams DSL Application # You now can give names to processors when using the Kafka Streams DSL. In the PAPI there are Processors and State Stores and you are required to explicitly name each one.
At the DSL layer, there are operators. A single DSL operator may compile down to multiple Processors and State Stores, and if required repartition topics. But with the Kafka Streams DSL, all these names are generated for you.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/interactive-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/interactive-queries/</guid>
      <description>Interactive Queries # Interactive queries allow you to leverage the state of your application from outside your application. The Kafka Streams enables your applications to be queryable.
Table of Contents
Querying local state stores for an app instance Querying local key-value stores Querying local window stores Querying local custom state stores Querying remote state stores for the entire app Adding an RPC layer to your application Exposing the RPC endpoints of your application Discovering and accessing application instances and their local state stores Demo applications The full state of your application is typically split across many distributed instances of your application, and across many state stores that are managed locally by these application instances.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/manage-topics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/manage-topics/</guid>
      <description>Managing Streams Application Topics # A Kafka Streams application continuously reads from Kafka topics, processes the read data, and then writes the processing results back into Kafka topics. The application may also auto-create other Kafka topics in the Kafka brokers, for example state store changelogs topics. This section describes the differences these topic types and how to manage the topics and your applications.
Kafka Streams distinguishes between user topics and internal topics</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/memory-mgmt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/memory-mgmt/</guid>
      <description>Memory Management # You can specify the total memory (RAM) size used for internal caching and compacting of records. This caching happens before the records are written to state stores or forwarded downstream to other nodes.
The record caches are implemented slightly different in the DSL and Processor API.
Table of Contents
Record caches in the DSL Record caches in the Processor API RocksDB Other memory usage Record caches in the DSL # You can specify the total memory (RAM) size of the record cache for an instance of the processing topology.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/processor-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/processor-api/</guid>
      <description>Processor API # The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.
Table of Contents
Overview Defining a Stream Processor Unit Testing Processors State Stores Defining and creating a State Store Fault-tolerant State Stores Enable or Disable Fault Tolerance of State Stores (Store Changelogs) Timestamped State Stores Implementing Custom State Stores Connecting Processors and State Stores Accessing Processor Context Overview # The Processor API can be used to implement both stateless as well as stateful operations, where the latter is achieved through the use of state stores.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/running-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/running-app/</guid>
      <description>Running Streams Applications # You can run Java applications that use the Kafka Streams library without any additional configuration or requirements. Kafka Streams also provides the ability to receive notification of the various states of the application. The ability to monitor the runtime status is discussed in the monitoring guide.
Table of Contents
Starting a Kafka Streams application Elastic scaling of your application Adding capacity to your application Removing capacity from your application State restoration during workload rebalance Determining how many application instances to run Starting a Kafka Streams application # You can package your Java application as a fat JAR file and then start the application like this:</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/security/</guid>
      <description>Streams Security # Table of Contents
Required ACL setting for secure Kafka clusters Security example Kafka Streams natively integrates with the Kafka&amp;rsquo;s security features and supports all of the client-side security features in Kafka. Streams leverages the Java Producer and Consumer API.
To secure your Stream processing applications, configure the security settings in the corresponding Kafka producer and consumer clients, and then specify the corresponding configuration settings in your Kafka Streams application.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/testing/</guid>
      <description>Testing Kafka Streams # Table of Contents
Importing the test utilities Testing Streams applications Unit testing Processors Importing the test utilities # To test a Kafka Streams application, Kafka provides a test-utils artifact that can be added as regular dependency to your test code base. Example pom.xml snippet when using Maven:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;kafka-streams-test-utils&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;{{fullDotVersion}}&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; Testing a Streams application # The test-utils package provides a TopologyTestDriver that can be used pipe data through a Topology that is either assembled manually using Processor API or via the DSL using StreamsBuilder.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/to-be-removed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/to-be-removed/</guid>
      <description> Developer Guide for Kafka Streams Introduction Run Demo App Tutorial: Write App Concepts Architecture Developer Guide Upgrade This developer guide describes how to write, configure, and execute a Kafka Streams application.
Writing a Streams Application Configuring a Streams Application Streams DSL Processor API Naming Operators in a Streams DSL application Data Types and Serialization Testing a Streams Application Interactive Queries Memory Management Running Streams Applications Managing Streams Application Topics Streams Security Application Reset Tool Previous Next Documentation Kafka Streams </description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:8080/35/documentation/streams/developer-guide/write-streams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:8080/35/documentation/streams/developer-guide/write-streams/</guid>
      <description>Writing a Streams Application # Table of Contents
Libraries and Maven artifacts Using Kafka Streams within your application code Testing a Streams application Any Java or Scala application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).
You can define the processor topology with the Kafka Streams APIs:</description>
    </item>
    
  </channel>
</rss>

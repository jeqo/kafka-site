<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html xmlns:og="http://ogp.me/ns#">
<head>
    <title>Apache Kafka</title>
<link rel='stylesheet' href='http://localhost:8080/35/documentation/css/styles.css?2' type='text/css'>
<link rel="icon" type="image/gif" href="http://localhost:8080/35/documentation/images/apache_feather.gif">
<meta name="robots" content="index,follow"/>
<meta name="language" content="en"/>
<meta name="keywords" content="apache kafka messaging queuing distributed stream processing">
<meta name="description" content="Apache Kafka: A Distributed Streaming Platform.">
<meta http-equiv='Content-Type' content='text/html;charset=utf-8'/>
<meta name="viewport" content="initial-scale = 1.0,maximum-scale = 1.0"/>
<meta property="og:title" content="Apache Kafka"/>
<meta property="og:image" content="http://apache-kafka.org/images/apache-kafka.png"/>
<meta property="og:description" content="Apache Kafka: A Distributed Streaming Platform."/>
<meta property="og:site_name" content="Apache Kafka"/>
<meta property="og:type" content="website"/>
<link href="http://localhost:8080/35/documentation/css/fonts.css" rel="stylesheet">
<script src="http://localhost:8080/35/documentation/js/jquery.min.js"></script>
<script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>

<link rel="stylesheet" href="http://localhost:8080/35/documentation/css/prism.min.css"/>
<script defer src="http://localhost:8080/35/documentation/js/prism.min.js"></script>

<script>
			
			
			
			var legacyDocPaths = [
				'/07/documentation',
				'/07/documentation/',
				'/08/documentation',
				'/08/documentation/',
				'/081/documentation',
				'/081/documentation/',
				'/082/documentation',
				'/082/documentation/',
				'/090/documentation',
				'/090/documentation/',
				'/0100/documentation',
				'/0100/documentation/'
			];

			
			
			var currentPath = window.location.pathname;
			var shouldRedirect = !legacyDocPaths.includes(currentPath);
			var isDocumenationPage = currentPath.includes('/documentation');

			var hasNotSpecifiedFullPath = !currentPath.includes('/documentation/streams') && !currentPath.includes('/documentation/streams/');

			
			
			var specifiedStreamsAnchor = window.location.hash.includes('#streams_');

			if (shouldRedirect && isDocumenationPage && hasNotSpecifiedFullPath) {
				if (specifiedStreamsAnchor) {
					window.location.pathname = currentPath + 'streams';
				}
			}





    </script>

<script>
			var _paq = window._paq = window._paq || [];
			 
			 
			_paq.push(['disableCookies']);
			_paq.push(['trackPageView']);
			_paq.push(['enableLinkTracking']);
			(function() {
				var u="//matomo.privacy.apache.org/";
				_paq.push(['setTrackerUrl', u+'matomo.php']);
				_paq.push(['setSiteId', '26']);
				var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
				g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
			})();





    </script>


</head>
<body>
<div class="main">
    <div class="header" id="header">
    <a href="/" class="logo-link">
        <span class="visually-hidden">Apache Kafka</span>
    </a>

    <button type="button" class="top-nav-toggle burger-toggle" id="top-nav-toggle" aria-haspopup="true" aria-expanded="false">
        <span class="visually-hidden">Toggle navigation</span>
        <span class="burger-line"></span>
        <span class="burger-line"></span>
        <span class="burger-line"></span>
    </button>

    <div class="top-nav-container" id="top-nav-container">
        <nav class="top-nav" role="navigation" aria-label="Main menu">
            <ul class="top-nav-list">
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false">
                        Get Started
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" title="Get Started">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/intro" tabindex="-1" class="top-nav-anchor">
                                Introduction
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/quickstart" tabindex="-1" class="top-nav-anchor">
                                Quickstart
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/uses" tabindex="-1" class="top-nav-anchor">
                                Use Cases
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/books-and-papers" tabindex="-1" class="top-nav-anchor">
                                Books &amp; Papers
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/videos" tabindex="-1" class="top-nav-anchor">
                                Videos
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a href="/podcasts" tabindex="-1" class="top-nav-anchor">
                                Podcasts
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="/documentation" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-documentation-menu">
                        Docs
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-documentation-menu" title="Docs">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#gettingStarted">
                                Key Concepts
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#api">
                                APIs
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#configuration">
                                Configuration
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#design">
                                Design
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#implementation">
                                Implementation
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#operations">
                                Operations
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#security">
                                Security
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank">
                                Clients
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation#connect">
                                Kafka Connect
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/documentation/streams">
                                Kafka Streams
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a class="top-nav-item-anchor" href="/powered-by">
                        Powered By
                    </a>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-community-menu">
                        Community
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-community-menu" title="Community">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://kafka-summit.org/" target="_blank">
                                Kafka Summit
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/project">
                                Project Info
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/trademark">
                                Trademark
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" target="_blank">
                                Ecosystem
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/events">
                                Events
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="/contact">
                                Contact us
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="#" class="top-nav-item-anchor" aria-haspopup="true" aria-expanded="false" aria-controls="nav-community-menu">
                        Apache
                    </a>
                    <ul class="top-nav-menu" aria-hidden="true" role="menu" id="nav-community-menu" title="Community">
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/licenses/" target="_blank">
                                License
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/foundation/sponsorship.html" target="_blank">
                                Donate
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/foundation/thanks.html" target="_blank">
                                Sponsors
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/security/" target="_blank">
                                Security
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank">
                                Privacy
                            </a>
                        </li>
                        <li class="top-nav-menu-item" role="menuitem">
                            <a class="top-nav-anchor" tabindex="-1" href="https://www.apache.org/" target="_blank">
                                Apache.org
                            </a>
                        </li>
                    </ul>
                </li>
                <li class="top-nav-item" role="menuitem">
                    <a href="/downloads" class="top-nav-download" tabindex="-1" id="top-nav-download">
                        Download Kafka
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</div>

</div>
<div class="main">
    <div class="content documentation documentation--current">
        <div class="toc-handle-container">
            <div class="toc-handle">&lt;</div>
        </div>
        <div class="docs-nav">
            <ul class="toc">
    <li>1. Getting Started
        <ul>
            <li><a href="http://localhost:8080/35/documentation/introduction">1.1 Introduction</a>
            <li><a href="http://localhost:8080/35/documentation/uses">1.2 Use Cases</a>
            <li><a href="http://localhost:8080/35/documentation/quickstart">1.3 Quick Start</a>
            <li><a href="http://localhost:8080/35/documentation/ecosystem">1.4 Ecosystem</a>
            <li><a href="http://localhost:8080/35/documentation/upgrade">1.5 Upgrading</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/api">2. APIs</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/api#producerapi">2.1 Producer API</a>
            <li><a href="http://localhost:8080/35/documentation/api#consumerapi">2.2 Consumer API</a>
            <li><a href="http://localhost:8080/35/documentation/api#streamsapi">2.3 Streams API</a>
            <li><a href="http://localhost:8080/35/documentation/api#connectapi">2.4 Connect API</a>
            <li><a href="http://localhost:8080/35/documentation/api#adminapi">2.5 Admin API</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/configuration">3. Configuration</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/configuration#brokerconfigs">3.1 Broker Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#topicconfigs">3.2 Topic Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#producerconfigs">3.3 Producer Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#consumerconfigs">3.4 Consumer Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#connectconfigs">3.5 Kafka Connect Configs</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/configuration#sourceconnectconfigs">Source Connector Configs</a>
                    <li><a href="http://localhost:8080/35/documentation/configuration#sinkconnectconfigs">Sink Connector Configs</a>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/configuration#streamsconfigs">3.6 Kafka Streams Configs</a>
            <li><a href="http://localhost:8080/35/documentation/configuration#adminclientconfigs">3.7 AdminClient Configs</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/design">4. Design</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/design#majordesignelements">4.1 Motivation</a>
            <li><a href="http://localhost:8080/35/documentation/design#persistence">4.2 Persistence</a>
            <li><a href="http://localhost:8080/35/documentation/design#maximizingefficiency">4.3 Efficiency</a>
            <li><a href="http://localhost:8080/35/documentation/design#theproducer">4.4 The Producer</a>
            <li><a href="http://localhost:8080/35/documentation/design#theconsumer">4.5 The Consumer</a>
            <li><a href="http://localhost:8080/35/documentation/design#semantics">4.6 Message Delivery Semantics</a>
            <li><a href="http://localhost:8080/35/documentation/design#replication">4.7 Replication</a>
            <li><a href="http://localhost:8080/35/documentation/design#compaction">4.8 Log Compaction</a>
            <li><a href="http://localhost:8080/35/documentation/design#design_quotas">4.9 Quotas</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/implementation">5. Implementation</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/implementation#networklayer">5.1 Network Layer</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#messages">5.2 Messages</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#messageformat">5.3 Message format</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#log">5.4 Log</a>
            <li><a href="http://localhost:8080/35/documentation/implementation#distributionimpl">5.5 Distribution</a>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/operations">6. Operations</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/operations#basic_ops">6.1 Basic Kafka Operations</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_add_topic">Adding and removing topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_modify_topic">Modifying topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_restarting">Graceful shutdown</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_leader_balancing">Balancing leadership</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_racks">Balancing Replicas Across Racks</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_mirror_maker">Mirroring data between clusters</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_consumer_lag">Checking consumer position</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_consumer_group">Managing Consumer Groups</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_cluster_expansion">Expanding your cluster</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_decommissioning_brokers">Decommissioning brokers</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#basic_ops_increase_replication_factor">Increasing replication factor</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#rep-throttle">Limiting Bandwidth Usage during Data Migration</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#quotas">Setting quotas</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#datacenters">6.2 Datacenters</a></li>
            <li><a href="http://localhost:8080/35/documentation/operations#georeplication">6.3 Geo-Replication (Cross-Cluster Data Mirroring)</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-overview">Geo-Replication Overview</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-flows">What Are Replication Flows</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-mirrormaker">Configuring Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-starting">Starting Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-stopping">Stopping Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-apply-config-changes">Applying Configuration Changes</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#georeplication-monitoring">Monitoring Geo-Replication</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#multitenancy">6.4 Multi-Tenancy</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-overview">Multi-Tenancy Overview</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-topic-naming">Creating User Spaces (Namespaces)</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-topic-configs">Configuring Topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-security">Securing Clusters and Topics</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-isolation">Isolating Tenants</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-monitoring">Monitoring and Metering</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-georeplication">Multi-Tenancy and Geo-Replication</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#multitenancy-more">Further considerations</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#config">6.5 Important Configs</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#clientconfig">Important Client Configs</a>
                    <li><a href="http://localhost:8080/35/documentation/operations#prodconfig">A Production Server Configs</a>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#java">6.6 Java Version</a>
            <li><a href="http://localhost:8080/35/documentation/operations#hwandos">6.7 Hardware and OS</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#os">OS</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#diskandfs">Disks and Filesystems</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#appvsosflush">Application vs OS Flush Management</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#linuxflush">Linux Flush Behavior</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#filesystems">Filesystem Selection</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#replace_disk">Replace KRaft Controller Disk</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#monitoring">6.8 Monitoring</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#remote_jmx">Security Considerations for Remote Monitoring using JMX</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_monitoring">KRaft Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#selector_monitoring">Selector Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#common_node_monitoring">Common Node Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#producer_monitoring">Producer Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#consumer_monitoring">Consumer Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#connect_monitoring">Connect Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kafka_streams_monitoring">Streams Monitoring</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#others_monitoring">Others</a></li>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#zk">6.9 ZooKeeper</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#zkversion">Stable Version</a>
                    <li><a href="http://localhost:8080/35/documentation/operations#zkops">Operationalization</a>
                </ul>
            </li>
            <li><a href="http://localhost:8080/35/documentation/operations#kraft">6.10 KRaft</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_config">Configuration</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_storage">Storage Tool</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_debug">Debugging</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_deployment">Deploying Considerations</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_missing">Missing Features</a></li>
                    <li><a href="http://localhost:8080/35/documentation/operations#kraft_zk_migration">ZooKeeper to KRaft Migration</a></li>
                </ul>
            </li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/security">7. Security</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/security#security_overview">7.1 Security Overview</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#listener_configuration">7.2 Listener Configuration</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_ssl">7.3 Encryption and Authentication using SSL</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_sasl">7.4 Authentication using SASL</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_authz">7.5 Authorization and ACLs</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#security_rolling_upgrade">7.6 Incorporating Security Features in a Running Cluster</a></li>
            <li><a href="http://localhost:8080/35/documentation/security#zk_authz">7.7 ZooKeeper Authentication</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new">New Clusters</a>
                        <ul>
                            <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new_sasl">ZooKeeper SASL Authentication</a></li>
                            <li><a href="http://localhost:8080/35/documentation/security#zk_authz_new_mtls">ZooKeeper Mutual TLS Authentication</a></li>
                        </ul>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_migration">Migrating Clusters</a></li>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_ensemble">Migrating the ZooKeeper Ensemble</a></li>
                    <li><a href="http://localhost:8080/35/documentation/security#zk_authz_quorum">ZooKeeper Quorum Mutual TLS Authentication</a></li>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/security#zk_encryption">7.8 ZooKeeper Encryption</a></li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/connect">8. Kafka Connect</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_overview">8.1 Overview</a></li>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_user">8.2 User Guide</a>
                <ul>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_running">Running Kafka Connect</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_configuring">Configuring Connectors</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_transforms">Transformations</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_rest">REST API</a></li>
                    <li><a href="http://localhost:8080/35/documentation/connect#connect_errorreporting">Error Reporting in Connect</a></li>
                </ul>
            <li><a href="http://localhost:8080/35/documentation/connect#connect_development">8.3 Connector Development Guide</a></li>
        </ul>
    </li>
    <li><a href="http://localhost:8080/35/documentation/streams">9. Kafka Streams</a>
        <ul>
            <li><a href="http://localhost:8080/35/documentation/streams/quickstart">9.1 Play with a Streams Application</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/tutorial">9.2 Write your own Streams Applications</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/developer-guide">9.3 Developer Manual</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/core-concepts">9.4 Core Concepts</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/architecture">9.5 Architecture</a></li>
            <li><a href="http://localhost:8080/35/documentation/streams/upgrade-guide">9.6 Upgrade Guide</a></li>
        </ul>
    </li>
</ul>

        </div>
        <div class="right">
            <article class="markdown"><h1 id="configuring-a-streams-application">
  Configuring a Streams Application
  <a class="anchor" href="#configuring-a-streams-application">#</a>
</h1>
<p>Kafka and Kafka Streams configuration options must be configured before
using Streams. You can configure Kafka Streams by specifying parameters
in a <code>java.util.Properties</code> instance.</p>
<ol>
<li>
<p>Create a <code>java.util.Properties</code> instance.</p>
</li>
<li>
<p>Set the <a href="#streams-developer-guide-required-configs">parameters</a>. For example:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;

Properties settings = new Properties();
// Set a few key parameters
settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &#34;my-first-streams-application&#34;);
settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &#34;kafka-broker1:9092&#34;);
// Any further settings
settings.put(... , ...);
</code></pre></li>
</ol>
<h2 id="configuration-parameter-reference">
  Configuration parameter reference
  <a class="anchor" href="#configuration-parameter-reference">#</a>
</h2>
<p>This section contains the most common Streams configuration parameters.
For a full reference, see the
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/StreamsConfig.html">Streams</a> Javadocs.</p>
<ul>
<li><a href="#required-configuration-parameters">Required configuration parameters</a>
<ul>
<li><a href="#application-id">application.id</a></li>
<li><a href="#bootstrap-servers">bootstrap.servers</a></li>
</ul>
</li>
<li><a href="#recommended-configuration-parameters-for-resiliency">Recommended configuration parameters for resiliency</a>
<ul>
<li><a href="#acks">acks</a></li>
<li><a href="#id2">replication.factor</a></li>
</ul>
</li>
<li><a href="#optional-configuration-parameters">Optional configuration parameters</a>
<ul>
<li><a href="#acceptable-recovery-lag">acceptable.recovery.lag</a></li>
<li><a href="#default-deserialization-exception-handler">default.deserialization.exception.handler</a></li>
<li><a href="#default-key-serde">default.key.serde</a></li>
<li><a href="#default-production-exception-handler">default.production.exception.handler</a></li>
<li><a href="#timestamp-extractor">default.timestamp.extractor</a></li>
<li><a href="#default-value-serde">default.value.serde</a></li>
<li><a href="#default-windowed-key-serde-inner">default.windowed.key.serde.inner</a></li>
<li><a href="#default-windowed-value-serde-inner">default.windowed.value.serde.inner</a></li>
<li><a href="#max-task-idle-ms">max.task.idle.ms</a></li>
<li><a href="#max-warmup-replicas">max.warmup.replicas</a></li>
<li><a href="#num-standby-replicas">num.standby.replicas</a></li>
<li><a href="#num-stream-threads">num.stream.threads</a></li>
<li><a href="#partition-grouper">partition.grouper</a></li>
<li><a href="#probing-rebalance-interval-ms">probing.rebalance.interval.ms</a></li>
<li><a href="#processing-guarantee">processing.guarantee</a></li>
<li><a href="#rack-aware-assignment-tags">rack.aware.assignment.tags</a></li>
<li><a href="#replication-factor">replication.factor</a></li>
<li><a href="#rocksdb-config-setter">rocksdb.config.setter</a></li>
<li><a href="#state-dir">state.dir</a></li>
<li><a href="#topology-optimization">topology.optimization</a></li>
</ul>
</li>
<li><a href="#kafka-consumers-and-producer-configuration-parameters">Kafka consumers and producer configuration parameters</a>
<ul>
<li><a href="#naming">Naming</a></li>
<li><a href="#default-values">Default Values</a></li>
<li><a href="#enable-auto-commit">enable.auto.commit</a></li>
</ul>
</li>
</ul>
<h3 id="required-configuration-parameters">
  Required configuration parameters
  <a class="anchor" href="#required-configuration-parameters">#</a>
</h3>
<p>Here are the required Streams configuration parameters.</p>
<p>Parameter Name      Importance   Description                                                                                         Default Value</p>
<hr>
<p>application.id      Required     An identifier for the stream processing application. Must be unique within the Kafka cluster.       None
bootstrap.servers   Required     A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.      None</p>
<h4 id="application-id">
  application.id
  <a class="anchor" href="#application-id">#</a>
</h4>
<p>(Required) The application ID. Each stream processing application must
have a unique ID. The same ID must be given to all instances of the
application. It is recommended to use only alphanumeric characters,
<code>.</code> (dot), <code>-</code> (hyphen), and <code>_</code> (underscore). Examples:
<code>&quot;hello_world&quot;</code>,
<code>&quot;hello_world-v1.0.0&quot;</code></p>
<p>This ID is used in the following places to isolate resources used by
the application from others:</p>
<ul>
<li>As the default Kafka consumer and producer <code>client.id</code> prefix</li>
<li>As the Kafka consumer <code>group.id</code> for coordination</li>
<li>As the name of the subdirectory in the state directory (cf.
<code>state.dir</code>)</li>
<li>As the prefix of internal Kafka topic names</li>
</ul>
<dl>
<dt>Tip:</dt>
<dd>When an application is updated, the <code>application.id</code> should be changed unless you want to reuse the
existing data in internal topics and state stores. For example,
you could embed the version information within
<code>application.id</code>, as <code>my-app-v1.0.0</code> and <code>my-app-v1.0.2</code>.</dd>
</dl>
<h4 id="bootstrap-servers">
  bootstrap.servers
  <a class="anchor" href="#bootstrap-servers">#</a>
</h4>
<p>(Required) The Kafka bootstrap servers. This is the same
<a href="http://kafka.apache.org/documentation.html#producerconfigs">setting</a>
that is used by the underlying producer and consumer
clients to connect to the Kafka cluster. Example:
<code>&quot;kafka-broker1:9092,kafka-broker2:9092&quot;</code>.</p>
<h3 id="recommended-configuration-parameters-for-resiliency">
  Recommended configuration parameters for resiliency
  <a class="anchor" href="#recommended-configuration-parameters-for-resiliency">#</a>
</h3>
<p>There are several Kafka and Kafka Streams configuration options that
need to be configured explicitly for resiliency in face of broker
failures:</p>
<pre tabindex="0"><code>  Parameter Name                                              Corresponding Client   Default value                          Consider setting to
  ----------------------------------------------------------- ---------------------- -------------------------------------- ----------------------------------------
  acks                                                        Producer               `acks=1`   `acks=all`
  replication.factor (for broker version 2.3 or older)/td\&gt;   Streams                `-1`       `3`
  min.insync.replicas                                         Broker                 `1`        `2`
  num.standby.replicas                                        Streams                `0`        `1`
</code></pre><p>Increasing the replication factor to 3 ensures that the internal Kafka
Streams topic can tolerate up to 2 broker failures. Changing the acks
setting to &ldquo;all&rdquo; guarantees that a record will not be lost as long as
one replica is alive. The tradeoff from moving to the default values to
the recommended ones is that some performance and more storage space (3x
with the replication factor of 3) are sacrificed for more resiliency.</p>
<h4 id="acks">
  acks
  <a class="anchor" href="#acks">#</a>
</h4>
<p>The number of acknowledgments that the leader must have received
before considering a request complete. This controls the durability of
records that are sent. The possible values are:</p>
<ul>
<li><code>acks=0</code> The producer does not wait
for acknowledgment from the server and the record is immediately
added to the socket buffer and considered sent. No guarantee can
be made that the server has received the record in this case, and
the <code>retries</code> configuration will not
take effect (as the client won&rsquo;t generally know of any failures).
The offset returned for each record will always be set to
<code>-1</code>.</li>
<li><code>acks=1</code> The leader writes the record
to its local log and responds without waiting for full
acknowledgement from all followers. If the leader immediately
fails after acknowledging the record, but before the followers
have replicated it, then the record will be lost.</li>
<li><code>acks=all</code> The leader waits for the
full set of in-sync replicas to acknowledge the record. This
guarantees that the record will not be lost if there is at least
one in-sync replica alive. This is the strongest available
guarantee.</li>
</ul>
<p>For more information, see the
<a href="https://kafka.apache.org/documentation/#producerconfigs">Kafka Producer documentation</a>.</p>
<h4 id="id2">
  replication.factor
  <a class="anchor" href="#id2">#</a>
</h4>
<p>See the <a href="#replication-factor-parm">description here</a>.</p>
<h4 id="id23">
  num.standby.replicas
  <a class="anchor" href="#id23">#</a>
</h4>
<p>See the <a href="#streams-developer-guide-standby-replicas">description here</a>.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">Properties streamsSettings = new Properties();
// for broker version 2.3 or older
//streamsSettings.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
streamsSettings.put(StreamsConfig.topicPrefix(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG), 2);
streamsSettings.put(StreamsConfig.producerPrefix(ProducerConfig.ACKS_CONFIG), &#34;all&#34;);
streamsSettings.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG), 1);
</code></pre><h3 id="optional-configuration-parameters">
  Optional configuration parameters
  <a class="anchor" href="#optional-configuration-parameters">#</a>
</h3>
<p>Here are the optional
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/StreamsConfig.html">Streams</a>
javadocs, sorted by level of importance:</p>
<ul>
<li>High: These parameters can have a significant impact on
performance. Take care when deciding the values of these
parameters.</li>
<li>Medium: These parameters can have some impact on performance. Your
specific environment will determine how much tuning effort should
be focused on these parameters.</li>
<li>Low: These parameters have a less general or less significant
impact on performance.</li>
</ul>
<p>Parameter Name
:::
:::
:::</p>
<p>Importance</p>
<p>Description</p>
<p>Default Value</p>
<p>acceptable.recovery.lag</p>
<p>Medium</p>
<p>The maximum acceptable lag (number of offsets to catch up) for an
instance to be considered caught-up and ready for the active task.</p>
<p>10000</p>
<p>application.server</p>
<p>Low</p>
<p>A host:port pair pointing to an embedded user defined endpoint that can
be used for discovering the locations of state stores within a single
Kafka Streams application. The value of this must be different for each
instance of the application.</p>
<p>the empty string</p>
<p>buffered.records.per.partition</p>
<p>Low</p>
<p>The maximum number of records to buffer per partition.</p>
<p>1000</p>
<p>cache.max.bytes.buffering</p>
<p>Medium</p>
<p>Maximum number of memory bytes to be used for record caches across all
threads.</p>
<p>10485760 bytes</p>
<p>client.id</p>
<p>Medium</p>
<p>An ID string to pass to the server when making requests. (This setting
is passed to the consumer/producer clients used internally by Kafka
Streams.)</p>
<p>the empty string</p>
<p>commit.interval.ms</p>
<p>Low</p>
<p>The frequency in milliseconds with which to save the position (offsets
in source topics) of tasks.</p>
<p>30000 milliseconds</p>
<p>default.deserialization.exception.handler</p>
<p>Medium</p>
<p>Exception handling class that implements the
<code>DeserializationExceptionHandler</code> interface.</p>
<p><code>LogAndContinueExceptionHandler</code></p>
<p>default.key.serde</p>
<p>Medium</p>
<p>Default serializer/deserializer class for record keys, implements the
<code>Serde</code> interface. Must be set by the user
or all serdes must be passed in explicitly (see also
default.value.serde).</p>
<p><code>null</code></p>
<p>default.production.exception.handler</p>
<p>Medium</p>
<p>Exception handling class that implements the
<code>ProductionExceptionHandler</code> interface.</p>
<p><code>DefaultProductionExceptionHandler</code></p>
<p>default.timestamp.extractor</p>
<p>Medium</p>
<p>Timestamp extractor class that implements the
<code>TimestampExtractor</code> interface.</p>
<p>See <a href="#streams-developer-guide-timestamp-extractor">Timestamp Extractor</a></p>
<p>default.value.serde</p>
<p>Medium</p>
<p>Default serializer/deserializer class for record values, implements the
<code>Serde</code> interface. Must be set by the user
or all serdes must be passed in explicitly (see also default.key.serde).</p>
<p><code>null</code></p>
<p>default.windowed.key.serde.inner</p>
<p>Medium</p>
<p>Default serializer/deserializer for the inner class of windowed keys,
implementing the <code>Serde</code> interface.</p>
<p>null</p>
<p>default.windowed.value.serde.inner</p>
<p>Medium</p>
<p>Default serializer/deserializer for the inner class of windowed values,
implementing the <code>Serde</code> interface.</p>
<p>null</p>
<p>max.task.idle.ms</p>
<p>Medium</p>
<p>This config controls whether joins and merges may produce out-of-order
results. The config value is the maximum amount of time in milliseconds
a stream task will stay idle when it is fully caught up on some (but not
all) input partitions to wait for producers to send additional records
and avoid potential out-of-order record processing across multiple input
streams. The default (zero) does not wait for producers to send more
records, but it does wait to fetch data that is already present on the
brokers. This default means that for records that are already present on
the brokers, Streams will process them in timestamp order. Set to -1 to
disable idling entirely and process any locally available data, even
though doing so may produce out-of-order processing.</p>
<p>0 milliseconds</p>
<p>max.warmup.replicas</p>
<p>Medium</p>
<p>The maximum number of warmup replicas (extra standbys beyond the
configured num.standbys) that can be assigned at once.</p>
<p>2</p>
<p>metric.reporters</p>
<p>Low</p>
<p>A list of classes to use as metrics reporters.</p>
<p>the empty list</p>
<p>metrics.num.samples</p>
<p>Low</p>
<p>The number of samples maintained to compute metrics.</p>
<p>2</p>
<p>metrics.recording.level</p>
<p>Low</p>
<p>The highest recording level for metrics.</p>
<p><code>INFO</code></p>
<p>metrics.sample.window.ms</p>
<p>Low</p>
<p>The window of time in milliseconds a metrics sample is computed over.</p>
<p>30000 milliseconds (30 seconds)</p>
<p>num.standby.replicas</p>
<p>High</p>
<p>The number of standby replicas for each task.</p>
<p>0</p>
<p>num.stream.threads</p>
<p>Medium</p>
<p>The number of threads to execute stream processing.</p>
<p>1</p>
<p>partition.grouper</p>
<p>Low</p>
<p>Partition grouper class that implements the
<code>PartitionGrouper</code> interface.</p>
<p>See <a href="#streams-developer-guide-partition-grouper">Partition Grouper</a></p>
<p>probing.rebalance.interval.ms</p>
<p>Low</p>
<p>The maximum time in milliseconds to wait before triggering a rebalance
to probe for warmup replicas that have sufficiently caught up.</p>
<p>600000 milliseconds (10 minutes)</p>
<p>processing.guarantee</p>
<p>Medium</p>
<p>The processing mode. Can be either <code>&quot;at_least_once&quot;</code> (default) or <code>&quot;exactly_once_v2&quot;</code> (for EOS version 2, requires broker version 2.5+).
Deprecated config options are <code>&quot;exactly_once&quot;</code> (for EOS version 1) and
<code>&quot;exactly_once_beta&quot;</code> (for EOS version 2,
requires broker version 2.5+)</p>
<p>.</p>
<p>See <a href="#streams-developer-guide-processing-guarantee">Processing Guarantee</a></p>
<p>poll.ms</p>
<p>Low</p>
<p>The amount of time in milliseconds to block waiting for input.</p>
<p>100 milliseconds</p>
<p>rack.aware.assignment.tags</p>
<p>Medium</p>
<p>List of tag keys used to distribute standby replicas across Kafka
Streams clients. When configured, Kafka Streams will make a best-effort
to distribute the standby tasks over clients with different tag values.</p>
<p>the empty list</p>
<p>replication.factor</p>
<p>Medium</p>
<p>The replication factor for changelog topics and repartition topics
created by the application. The default of <code>-1</code> (meaning: use broker
default replication factor) requires broker version 2.4 or newer.</p>
<p>-1</p>
<p>retry.backoff.ms</p>
<p>Medium</p>
<p>The amount of time in milliseconds, before a request is retried. This
applies if the <code>retries</code> parameter is
configured to be greater than 0.</p>
<p>100 milliseconds</p>
<p>rocksdb.config.setter</p>
<p>Medium</p>
<p>The RocksDB configuration.</p>
<p>state.cleanup.delay.ms</p>
<p>Low</p>
<p>The amount of time in milliseconds to wait before deleting state when a
partition has migrated.</p>
<p>600000 milliseconds (10 minutes)</p>
<p>state.dir</p>
<p>High</p>
<p>Directory location for state stores.</p>
<p><code>/tmp/kafka-streams</code></p>
<p>task.timeout.ms</p>
<p>Medium</p>
<p>The maximum amount of time in milliseconds a task might stall due to
internal errors and retries until an error is raised. For a timeout of
<code>0 ms</code>, a task would raise an error for the first internal error. For
any timeout larger than <code>0 ms</code>, a task will retry at least once before
an error is raised.</p>
<p>300000 milliseconds (5 minutes)</p>
<p>topology.optimization</p>
<p>Medium</p>
<p>A configuration telling Kafka Streams if it should optimize the topology
and what optimizations to apply. Acceptable values are:
<code>StreamsConfig.NO_OPTIMIZATION</code> (<code>none</code>), <code>StreamsConfig.OPTIMIZE</code>
(<code>all</code>) or a comma separated list of specific optimizations:
(<code>StreamsConfig.REUSE_KTABLE_SOURCE_TOPICS</code>
(<code>reuse.ktable.source.topics</code>), <code>StreamsConfig.MERGE_REPARTITION_TOPICS</code>
(<code>merge.repartition.topics</code>)).</p>
<p><code> NO_OPTIMIZATION</code></p>
<p>upgrade.from</p>
<p>Medium</p>
<p>The version you are upgrading from during a rolling upgrade.</p>
<p>See <a href="#streams-developer-guide-upgrade-from">Upgrade From</a></p>
<p>windowstore.changelog.additional.retention.ms</p>
<p>Low</p>
<p>Added to a windows maintainMs to ensure data is not deleted from the log
prematurely. Allows for clock drift.</p>
<p>86400000 milliseconds (1 day)</p>
<p>window.size.ms</p>
<p>Low</p>
<p>Sets window size for the deserializer in order to calculate window end
times.</p>
<p>null</p>
<h4 id="acceptable-recovery-lag">
  acceptable.recovery.lag
  <a class="anchor" href="#acceptable-recovery-lag">#</a>
</h4>
<p>The maximum acceptable lag (total number of offsets to catch up from
the changelog) for an instance to be considered caught-up and able to
receive an active task. Streams will only assign stateful active tasks
to instances whose state stores are within the acceptable recovery
lag, if any exist, and assign warmup replicas to restore state in the
background for instances that are not yet caught up. Should correspond
to a recovery time of well under a minute for a given workload. Must
be at least 0.</p>
<p>Note: if you set this to <code>Long.MAX_VALUE</code> it effectively disables the
warmup replicas and task high availability, allowing Streams to
immediately produce a balanced assignment and migrate tasks to a new
instance without first warming them up.</p>
<h4 id="default-deserialization-exception-handler">
  default.deserialization.exception.handler
  <a class="anchor" href="#default-deserialization-exception-handler">#</a>
</h4>
<p>The default deserialization exception handler allows you to manage
record exceptions that fail to deserialize. This can be caused by
corrupt data, incorrect serialization logic, or unhandled record
types. The implemented exception handler needs to return a <code>FAIL</code> or
<code>CONTINUE</code> depending on the record and the exception thrown. Returning
<code>FAIL</code> will signal that Streams should shut down and <code>CONTINUE</code> will
signal that Streams should ignore the issue and continue processing.
The following library built-in exception handlers are available:</p>
<ul>
<li><a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/errors/LogAndContinueExceptionHandler.html">LogAndContinueExceptionHandler</a>:
This handler logs the deserialization exception and
then signals the processing pipeline to continue processing more
records. This log-and-skip strategy allows Kafka Streams to make
progress instead of failing if there are records that fail to
deserialize.</li>
<li><a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/errors/LogAndFailExceptionHandler.html">LogAndFailExceptionHandler</a>.
This handler logs the deserialization exception and
then signals the processing pipeline to stop processing more
records.</li>
</ul>
<p>You can also provide your own customized exception handler besides the
library provided ones to meet your needs. For example, you can choose
to forward corrupt records into a quarantine topic (think: a &quot;dead
letter queue&quot;) for further processing. To do this, use the Producer
API to write a corrupted record directly to the quarantine topic. To
be more concrete, you can create a separate <code>KafkaProducer</code> object
outside the Streams client, and pass in this object as well as the
dead letter queue topic name into the <code>Properties</code> map, which then can
be retrieved from the <code>configure</code> function call. The drawback of this
approach is that &quot;manual&quot; writes are side effects that are invisible
to the Kafka Streams runtime library, so they do not benefit from the
end-to-end processing guarantees of the Streams API:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">public class SendToDeadLetterQueueExceptionHandler implements DeserializationExceptionHandler {
    KafkaProducer&lt;byte[], byte[]&gt; dlqProducer;
    String dlqTopic;

    @Override
    public DeserializationHandlerResponse handle(final ProcessorContext context,
                                                 final ConsumerRecord&lt;byte[], byte[]&gt; record,
                                                 final Exception exception) {

        log.warn(&#34;Exception caught during Deserialization, sending to the dead queue topic; &#34; +
            &#34;taskId: {}, topic: {}, partition: {}, offset: {}&#34;,
            context.taskId(), record.topic(), record.partition(), record.offset(),
            exception);

        dlqProducer.send(new ProducerRecord&lt;&gt;(dlqTopic, record.timestamp(), record.key(), record.value(), record.headers())).get();

        return DeserializationHandlerResponse.CONTINUE;
    }

    @Override
    public void configure(final Map&lt;String, ?&gt; configs) {
        dlqProducer = .. // get a producer from the configs map
        dlqTopic = .. // get the topic name from the configs map
    }
}
</code></pre><h4 id="default-production-exception-handler">
  default.production.exception.handler
  <a class="anchor" href="#default-production-exception-handler">#</a>
</h4>
<p>The default production exception handler allows you to manage
exceptions triggered when trying to interact with a broker such as
attempting to produce a record that is too large. By default, Kafka
provides and uses the
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/errors/DefaultProductionExceptionHandler.html">DefaultProductionExceptionHandler</a> that always fails when these exceptions occur.</p>
<p>Each exception handler can return a <code>FAIL</code> or <code>CONTINUE</code> depending on
the record and the exception thrown. Returning <code>FAIL</code> will signal that
Streams should shut down and <code>CONTINUE</code> will signal that Streams
should ignore the issue and continue processing. If you want to
provide an exception handler that always ignores records that are too
large, you could implement something like the following:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.common.errors.RecordTooLargeException;
import org.apache.kafka.streams.errors.ProductionExceptionHandler;
import org.apache.kafka.streams.errors.ProductionExceptionHandler.ProductionExceptionHandlerResponse;

public class IgnoreRecordTooLargeHandler implements ProductionExceptionHandler {
    public void configure(Map&lt;String, Object&gt; config) {}

    public ProductionExceptionHandlerResponse handle(final ProducerRecord&lt;byte[], byte[]&gt; record,
                                                     final Exception exception) {
        if (exception instanceof RecordTooLargeException) {
            return ProductionExceptionHandlerResponse.CONTINUE;
        } else {
            return ProductionExceptionHandlerResponse.FAIL;
        }
    }
}

Properties settings = new Properties();

// other various kafka streams settings, e.g. bootstrap servers, application id, etc

settings.put(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG,
             IgnoreRecordTooLargeHandler.class);
</code></pre><h4 id="timestamp-extractor">
  default.timestamp.extractor
  <a class="anchor" href="#timestamp-extractor">#</a>
</h4>
<p>A timestamp extractor pulls a timestamp from an instance of
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html">ConsumerRecord</a>.
Timestamps are used to control the progress of streams.</p>
<p>The default extractor is
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.html">FailOnInvalidTimestamp</a>.
This extractor retrieves built-in timestamps that are
automatically embedded into Kafka messages by the Kafka producer
client since <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32&#43;-&#43;Add&#43;timestamps&#43;to&#43;Kafka&#43;message">Kafka version
0.10</a>.
Depending on the setting of Kafka&rsquo;s server-side
<code>log.message.timestamp.type</code> broker and
<code>message.timestamp.type</code> topic parameters,
this extractor provides you with:</p>
<ul>
<li><strong>event-time</strong> processing semantics if
<code>log.message.timestamp.type</code> is set to
<code>CreateTime</code> aka &ldquo;producer time&rdquo;
(which is the default). This represents the time when a Kafka
producer sent the original message. If you use Kafka&rsquo;s official
producer client, the timestamp represents milliseconds since the
epoch.</li>
<li><strong>ingestion-time</strong> processing semantics if
<code>log.message.timestamp.type</code> is set to
<code>LogAppendTime</code> aka &ldquo;broker time&rdquo;.
This represents the time when the Kafka broker received the
original message, in milliseconds since the epoch.</li>
</ul>
<p>The <code>FailOnInvalidTimestamp</code> extractor
throws an exception if a record contains an invalid (i.e. negative)
built-in timestamp, because Kafka Streams would not process this
record but silently drop it. Invalid built-in timestamps can occur for
various reasons: if for example, you consume a topic that is written
to by pre-0.10 Kafka producer clients or by third-party producer
clients that don&rsquo;t support the new Kafka 0.10 message format yet;
another situation where this may happen is after upgrading your Kafka
cluster from <code>0.9</code> to <code>0.10</code>, where all the data that was generated with
<code>0.9</code> does not include the
<code>0.10</code> message timestamps.</p>
<p>If you have data with invalid timestamps and want to process it, then
there are two alternative extractors available. Both work on built-in
timestamps, but handle invalid timestamps differently.</p>
<ul>
<li><a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestamp.html">LogAndSkipOnInvalidTimestamp</a>:
This extractor logs a warn message and returns the
invalid timestamp to Kafka Streams, which will not process but
silently drop the record. This log-and-skip strategy allows Kafka
Streams to make progress instead of failing if there are records
with an invalid built-in timestamp in your input data.</li>
<li><a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestamp.html">UsePartitionTimeOnInvalidTimestamp</a>.
This extractor returns the record&rsquo;s built-in timestamp
if it is valid (i.e. not negative). If the record does not have a
valid built-in timestamps, the extractor returns the previously
extracted valid timestamp from a record of the same topic
partition as the current record as a timestamp estimation. In case
that no timestamp can be estimated, it throws an exception.</li>
</ul>
<p>Another built-in extractor is
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/processor/WallclockTimestampExtractor.html">WallclockTimestampExtractor</a>.
This extractor does not actually &ldquo;extract&rdquo; a timestamp
from the consumed record but rather returns the current time in
milliseconds from the system clock (think:
<code>System.currentTimeMillis()</code>), which
effectively means Streams will operate on the basis of the so-called
<strong>processing-time</strong> of events.</p>
<p>You can also provide your own timestamp extractors, for instance to
retrieve timestamps embedded in the payload of messages. If you cannot
extract a valid timestamp, you can either throw an exception, return a
negative timestamp, or estimate a timestamp. Returning a negative
timestamp will result in data loss &ndash; the corresponding record will
not be processed but silently dropped. If you want to estimate a new
timestamp, you can use the value provided via
<code>previousTimestamp</code> (i.e., a Kafka Streams
timestamp estimation). Here is an example of a custom
<code>TimestampExtractor</code> implementation:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.processor.TimestampExtractor;

// Extracts the embedded timestamp of a record (giving you &#34;event-time&#34; semantics).
public class MyEventTimeExtractor implements TimestampExtractor {

  @Override
  public long extract(final ConsumerRecord&lt;Object, Object&gt; record, final long previousTimestamp) {
    // `Foo` is your own custom class, which we assume has a method that returns
    // the embedded timestamp (milliseconds since midnight, January 1, 1970 UTC).
    long timestamp = -1;
    final Foo myPojo = (Foo) record.value();
    if (myPojo != null) {
      timestamp = myPojo.getTimestampInMillis();
    }
    if (timestamp &lt; 0) {
      // Invalid timestamp!  Attempt to estimate a new timestamp,
      // otherwise fall back to wall-clock time (processing-time).
      if (previousTimestamp &gt;= 0) {
        return previousTimestamp;
      } else {
        return System.currentTimeMillis();
      }
    }
  }

}
</code></pre><p>You would then define the custom timestamp extractor in your Streams
configuration as follows:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">import java.util.Properties;
import org.apache.kafka.streams.StreamsConfig;

Properties streamsConfiguration = new Properties();
streamsConfiguration.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class);
</code></pre><h4 id="default-key-serde">
  default.key.serde
  <a class="anchor" href="#default-key-serde">#</a>
</h4>
<p>The default Serializer/Deserializer class for record keys, null unless
set by user. Serialization and deserialization in Kafka Streams
happens whenever data needs to be materialized, for example:</p>
<ul>
<li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g.,
via the <code>StreamsBuilder#stream()</code> and
<code>KStream#to()</code> methods).</li>
<li>Whenever data is read from or written to a <em>state store</em>.</li>
</ul>
<p>This is discussed in more detail in <a href="datatypes.html#streams-developer-guide-serdes">Data types and
serialization</a>.</p>
<h4 id="default-value-serde">
  default.value.serde](#id9)
  <a class="anchor" href="#default-value-serde">#</a>
</h4>
<p>The default Serializer/Deserializer class for record values, null
unless set by user. Serialization and deserialization in Kafka Streams
happens whenever data needs to be materialized, for example:</p>
<ul>
<li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g.,
via the <code>StreamsBuilder#stream()</code> and
<code>KStream#to()</code> methods).</li>
<li>Whenever data is read from or written to a <em>state store</em>.</li>
</ul>
<p>This is discussed in more detail in <a href="datatypes.html#streams-developer-guide-serdes">Data types and
serialization</a>.</p>
<h4 id="default-windowed-key-serde-inner">
  default.windowed.key.serde.inner
  <a class="anchor" href="#default-windowed-key-serde-inner">#</a>
</h4>
<p>The default Serializer/Deserializer class for the inner class of
windowed keys. Serialization and deserialization in Kafka Streams
happens whenever data needs to be materialized, for example:</p>
<ul>
<li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g.,
via the <code>StreamsBuilder#stream()</code> and
<code>KStream#to()</code> methods).</li>
<li>Whenever data is read from or written to a <em>state store</em>.</li>
</ul>
<p>This is discussed in more detail in <a href="datatypes.html#streams-developer-guide-serdes">Data types and
serialization</a>.</p>
<h4 id="default-windowed-value-serde-inner">
  default.windowed.value.serde.inner
  <a class="anchor" href="#default-windowed-value-serde-inner">#</a>
</h4>
<p>The default Serializer/Deserializer class for the inner class of
windowed values. Serialization and deserialization in Kafka Streams
happens whenever data needs to be materialized, for example:</p>
<ul>
<li>Whenever data is read from or written to a <em>Kafka topic</em> (e.g.,
via the <code>StreamsBuilder#stream()</code> and
<code>KStream#to()</code> methods).</li>
<li>Whenever data is read from or written to a <em>state store</em>.</li>
</ul>
<p>This is discussed in more detail in <a href="datatypes.html#streams-developer-guide-serdes">Data types and
serialization</a>.</p>
<h4 id="rack-aware-assignment-tags">
  rack.aware.assignment.tags
  <a class="anchor" href="#rack-aware-assignment-tags">#</a>
</h4>
<p>This configuration sets a list of tag keys used to distribute standby
replicas across Kafka Streams clients. When configured, Kafka Streams
will make a best-effort to distribute the standby tasks over clients
with different tag values.</p>
<p>Tags for the Kafka Streams clients can be set via
<code>client.tag.</code> prefix. Example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Client-1                                   | Client-2
</span></span><span class="line"><span class="cl">_______________________________________________________________________
</span></span><span class="line"><span class="cl">client.tag.zone: eu-central-1a             | client.tag.zone: eu-central-1b
</span></span><span class="line"><span class="cl">client.tag.cluster: k8s-cluster1           | client.tag.cluster: k8s-cluster1
</span></span><span class="line"><span class="cl">rack.aware.assignment.tags: zone,cluster   | rack.aware.assignment.tags: zone,cluster
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Client-3                                   | Client-4
</span></span><span class="line"><span class="cl">_______________________________________________________________________
</span></span><span class="line"><span class="cl">client.tag.zone: eu-central-1a             | client.tag.zone: eu-central-1b
</span></span><span class="line"><span class="cl">client.tag.cluster: k8s-cluster2           | client.tag.cluster: k8s-cluster2
</span></span><span class="line"><span class="cl">rack.aware.assignment.tags: zone,cluster   | rack.aware.assignment.tags: zone,cluster
</span></span></code></pre></div><p>In the above example, we have four Kafka Streams clients across two
zones (<code>eu-central-1a</code>,
<code>eu-central-1b</code>) and across two clusters
(<code>k8s-cluster1</code>, <code>k8s-cluster2</code>).
For an active task located on <code>Client-1</code>, Kafka Streams will allocate a standby task on
<code>Client-4</code>, since <code>Client-4</code> has a different <code>zone</code>
and a different <code>cluster</code> than
<code>Client-1</code>.</p>
<h4 id="max-task-idle-ms">
  max.task.idle.ms
  <a class="anchor" href="#max-task-idle-ms">#</a>
</h4>
<p>This configuration controls how long Streams will wait to fetch data
in order to provide in-order processing semantics.</p>
<p>When processing a task that has multiple input partitions (as in a
join or merge), Streams needs to choose which partition to process the
next record from. When all input partitions have locally buffered
data, Streams picks the partition whose next record has the lowest
timestamp. This has the desirable effect of collating the input
partitions in timestamp order, which is generally what you want in a
streaming join or merge. However, when Streams does not have any data
buffered locally for one of the partitions, it does not know whether
the next record for that partition will have a lower or higher
timestamp than the remaining partitions' records.</p>
<p>There are two cases to consider: either there is data in that
partition on the broker that Streams has not fetched yet, or Streams
is fully caught up with that partition on the broker, and the
producers simply haven't produced any new records since Streams
polled the last batch.</p>
<p>The default value of <code>0</code> causes Streams to
delay processing a task when it detects that it has no locally
buffered data for a partition, but there is data available on the
brokers. Specifically, when there is an empty partition in the local
buffer, but Streams has a non-zero lag for that partition. However, as
soon as Streams catches up to the broker, it will continue processing,
even if there is no data in one of the partitions. That is, it will
not wait for new data to be <em>produced</em>. This default is designed to
sacrifice some throughput in exchange for intuitively correct join
semantics.</p>
<p>Any config value greater than zero indicates the number of <em>extra</em>
milliseconds that Streams will wait if it has a caught-up but empty
partition. In other words, this is the amount of time to wait for new
data to be produced to the input partitions to ensure in-order
processing of data in the event of a slow producer.</p>
<p>The config value of <code>-1</code> indicates that
Streams will never wait to buffer empty partitions before choosing the
next record by timestamp, which achieves maximum throughput at the
expense of introducing out-of-order processing.</p>
<h4 id="max-warmup-replicas">
  max.warmup.replicas
  <a class="anchor" href="#max-warmup-replicas">#</a>
</h4>
<p>The maximum number of warmup replicas (extra standbys beyond the
configured num.standbys) that can be assigned at once for the purpose
of keeping the task available on one instance while it is warming up
on another instance it has been reassigned to. Used to throttle how
much extra broker traffic and cluster state can be used for high
availability. Increasing this will allow Streams to warm up more tasks
at once, speeding up the time for the reassigned warmups to restore
sufficient state for them to be transitioned to active tasks. Must be
at least 1.</p>
<h4 id="num-standby-replicas">
  num.standby.replicas
  <a class="anchor" href="#num-standby-replicas">#</a>
</h4>
<p>The number of standby replicas. Standby replicas are shadow copies of
local state stores. Kafka Streams attempts to create the specified
number of replicas per store and keep them up to date as long as there
are enough instances running. Standby replicas are used to minimize
the latency of task failover. A task that was previously running on a
failed instance is preferred to restart on an instance that has
standby replicas so that the local state store restoration process
from its changelog can be minimized. Details about how Kafka Streams
makes use of the standby replicas to minimize the cost of resuming
tasks on failover can be found in the <a href="../architecture.html#streams_architecture_state">State</a> section.</p>
<dl>
<dt>Recommendation:</dt>
<dd>Increase the number of standbys to 1 to get instant fail-over,
i.e., high-availability. Increasing the number of standbys
requires more client-side storage space. For example, with 1
standby, 2x space is required.</dd>
</dl>
<pre tabindex="0"><code>&lt;!-- --&gt;
</code></pre><dl>
<dt>Note:</dt>
<dd>If you enable n standby tasks, you need to provision n+1
<code>KafkaStreams</code> instances.</dd>
</dl>
<h4 id="num-stream-threads">
  num.stream.threads
  <a class="anchor" href="#num-stream-threads">#</a>
</h4>
<p>This specifies the number of stream threads in an instance of the
Kafka Streams application. The stream processing code runs in these
thread. For more information about Kafka Streams threading model, see
<a href="../architecture.html#streams_architecture_threads">Threading Model</a>.</p>
<h4 id="partition-grouper">
  partition.grouper
  <a class="anchor" href="#partition-grouper">#</a>
</h4>
<p><strong>[DEPRECATED]</strong> A partition grouper creates a list of stream tasks
from the partitions of source topics, where each created task is
assigned with a group of source topic partitions. The default
implementation provided by Kafka Streams is
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/processor/DefaultPartitionGrouper.html">DefaultPartitionGrouper</a>.
It assigns each task with one partition for each of the
source topic partitions. The generated number of tasks equals the
largest number of partitions among the input topics. Usually an
application does not need to customize the partition grouper.</p>
<h4 id="probing-rebalance-interval-ms">
  probing.rebalance.interval.ms
  <a class="anchor" href="#probing-rebalance-interval-ms">#</a>
</h4>
<p>The maximum time to wait before triggering a rebalance to probe for
warmup replicas that have restored enough to be considered caught up.
Streams will only assign stateful active tasks to instances that are
caught up and within the <a href="#acceptable-recovery-lag">acceptable.recovery.lag</a>, if any
exist. Probing rebalances are used to query the latest total lag of
warmup replicas and transition them to active tasks if ready. They
will continue to be triggered as long as there are warmup tasks, and
until the assignment is balanced. Must be at least 1 minute.</p>
<h4 id="processing-guarantee">
  processing.guarantee
  <a class="anchor" href="#processing-guarantee">#</a>
</h4>
<p>The processing guarantee that should be used. Possible values are
<code>&quot;at_least_once&quot;</code> (default) and
<code>&quot;exactly_once_v2&quot;</code> (for EOS version 2).
Deprecated config options are <code>&quot;exactly_once&quot;</code> (for EOS alpha), and <code>&quot;exactly_once_beta&quot;</code> (for EOS version 2). Using
<code>&quot;exactly_once_v2&quot;</code> (or the deprecated
<code>&quot;exactly_once_beta&quot;</code>) requires broker
version 2.5 or newer, while using the deprecated
<code>&quot;exactly_once&quot;</code> requires broker version
0.11.0 or newer. Note that if exactly-once processing is enabled, the
default for parameter <code>commit.interval.ms</code>
changes to 100ms. Additionally, consumers are configured with
<code>isolation.level=&quot;read_committed&quot;</code> and
producers are configured with <code>enable.idempotence=true</code> per default. Note that by default exactly-once
processing requires a cluster of at least three brokers what is the
recommended setting for production. For development, you can change
this configuration by adjusting broker setting
<code>transaction.state.log.replication.factor</code>
and <code>transaction.state.log.min.isr</code> to the
number of brokers you want to use. For more details see
<a href="../core-concepts#streams_processing_guarantee">Processing Guarantees</a>.</p>
<dl>
<dt>Recommendation:</dt>
<dd>While it is technically possible to use EOS with any replication
factor, using a replication factor lower than 3 effectively voids
EOS. Thus it is strongly recommended to use a replication factor
of 3 (together with <code>min.in.sync.replicas=2</code>). This recommendation
applies to all topics (i.e. <code>__transaction_state</code>,
<code>__consumer_offsets</code>, Kafka Streams internal topics, and user
topics).</dd>
</dl>
<h4 id="replication-factor">
  replication.factor](#id13)
  <a class="anchor" href="#replication-factor">#</a>
</h4>
<p>This specifies the replication factor of internal topics that Kafka
Streams creates when local states are used or a stream is
repartitioned for aggregation. Replication is important for fault
tolerance. Without replication even a single broker failure may
prevent progress of the stream processing application. It is
recommended to use a similar replication factor as source topics.</p>
<dl>
<dt>Recommendation:</dt>
<dd>Increase the replication factor to 3 to ensure that the internal
Kafka Streams topic can tolerate up to 2 broker failures. Note
that you will require more storage space as well (3x with the
replication factor of 3).</dd>
</dl>
<h4 id="rocksdb-config-setter">
  rocksdb.config.setter](#id20)
  <a class="anchor" href="#rocksdb-config-setter">#</a>
</h4>
<p>The RocksDB configuration. Kafka Streams uses RocksDB as the default
storage engine for persistent stores. To change the default
configuration for RocksDB, you can implement
<code>RocksDBConfigSetter</code> and provide your custom class via
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/streams/state/RocksDBConfigSetter.html">rocksdb.config.setter</a>.</p>
<p>Here is an example that adjusts the memory size consumed by RocksDB.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">public static class CustomRocksDBConfig implements RocksDBConfigSetter {
    // This object should be a member variable so it can be closed in RocksDBConfigSetter#close.
    private org.rocksdb.Cache cache = new org.rocksdb.LRUCache(16 * 1024L * 1024L);

    @Override
    public void setConfig(final String storeName, final Options options, final Map&lt;String, Object&gt; configs) {
        // See #1 below.
        BlockBasedTableConfig tableConfig = (BlockBasedTableConfig) options.tableFormatConfig();
        tableConfig.setBlockCache(cache);
        // See #2 below.
        tableConfig.setBlockSize(16 * 1024L);
        // See #3 below.
        tableConfig.setCacheIndexAndFilterBlocks(true);
        options.setTableFormatConfig(tableConfig);
        // See #4 below.
        options.setMaxWriteBufferNumber(2);
    }

    @Override
    public void close(final String storeName, final Options options) {
        // See #5 below.
        cache.close();
    }
}

Properties streamsSettings = new Properties();
streamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class);
</code></pre><dl>
<dt>Notes for example:</dt>
<dd>
<ol>
<li><code>BlockBasedTableConfig tableConfig = (BlockBasedTableConfig) options.tableFormatConfig();</code> Get a reference to the existing table config
rather than create a new one, so you don't accidentally
overwrite defaults such as the <code>BloomFilter</code>, which is an important optimization.</li>
<li><code>tableConfig.setBlockSize(16 * 1024L);</code> Modify the default <a href="https://github.com/apache/kafka/blob/2.3/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java#L79">block
size</a> per these instructions from the <a href="https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB#indexes-and-filter-blocks">RocksDB
GitHub</a>.</li>
<li><code>tableConfig.setCacheIndexAndFilterBlocks(true);</code> Do not let the index and filter blocks grow
unbounded. For more information, see the <a href="https://github.com/facebook/rocksdb/wiki/Block-Cache#caching-index-and-filter-blocks">RocksDB
GitHub</a>.</li>
<li><code>options.setMaxWriteBufferNumber(2);</code> See the advanced options in the <a href="https://github.com/facebook/rocksdb/blob/8dee8cad9ee6b70fd6e1a5989a8156650a70c04f/include/rocksdb/advanced_options.h#L103">RocksDB
GitHub</a>.</li>
<li><code>cache.close();</code> To avoid memory
leaks, you must close any objects you constructed that extend
org.rocksdb.RocksObject. See <a href="https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#memory-management">RocksJava
docs</a> for more details.</li>
</ol>
</dd>
</dl>
<h4 id="state-dir">
  state.dir
  <a class="anchor" href="#state-dir">#</a>
</h4>
<p>The state directory. Kafka Streams persists local states under the
state directory. Each application has a subdirectory on its hosting
machine that is located under the state directory. The name of the
subdirectory is the application ID. The state stores associated with
the application are created under this subdirectory. When running
multiple instances of the same application on a single machine, this
path must be unique for each such instance.</p>
<h4 id="topology-optimization">
  topology.optimization
  <a class="anchor" href="#topology-optimization">#</a>
</h4>
<p>A configuration telling Kafka Streams if it should optimize the
topology and what optimizations to apply. Acceptable values are:
<code>StreamsConfig.NO_OPTIMIZATION</code> (<code>none</code>), <code>StreamsConfig.OPTIMIZE</code>
(<code>all</code>) or a comma separated list of specific optimizations:
(<code>StreamsConfig.REUSE_KTABLE_SOURCE_TOPICS</code>
(<code>reuse.ktable.source.topics</code>),
<code>StreamsConfig.MERGE_REPARTITION_TOPICS</code>
(<code>merge.repartition.topics</code>)).</p>
<p>We recommend listing specific optimizations in the config for
production code so that the structure of your topology will not change
unexpectedly during upgrades of the Streams library.</p>
<p>These optimizations include moving/reducing repartition topics and
reusing the source topic as the changelog for source KTables. These
optimizations will save on network traffic and storage in Kafka
without changing the semantics of your applications. Enabling them is
recommended.</p>
<p>Note that as of 2.3, you need to do two things to enable
optimizations. In addition to setting this config to
<code>StreamsConfig.OPTIMIZE</code>, you'll need to pass in your configuration
properties when building your topology by using the overloaded
<code>StreamsBuilder.build(Properties)</code> method. For example
<code>KafkaStreams myStream = new KafkaStreams(streamsBuilder.build(properties), properties)</code>.</p>
<h4 id="upgrade-from">
  upgrade.from
  <a class="anchor" href="#upgrade-from">#</a>
</h4>
<p>The version you are upgrading from. It is important to set this config
when performing a rolling upgrade to certain versions, as described in
the upgrade guide. You should set this config to the appropriate
version before bouncing your instances and upgrading them to the newer
version. Once everyone is on the newer version, you should remove this
config and do a second rolling bounce. It is only necessary to set
this config and follow the two-bounce upgrade path when upgrading from
below version 2.0, or when upgrading to 2.4+ from any version lower
than 2.4.</p>
<h3 id="kafka-consumers-and-producer-configuration-parameters">
  Kafka consumers, producer and admin client configuration parameters](#id16)
  <a class="anchor" href="#kafka-consumers-and-producer-configuration-parameters">#</a>
</h3>
<p>You can specify parameters for the Kafka
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/clients/consumer/package-summary.html">consumers</a>,
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/clients/producer/package-summary.html">producers</a>, and <a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/kafka/clients/admin/package-summary.html">admin
client</a> that are used internally. The consumer, producer and admin
client settings are defined by specifying parameters in a
<code>StreamsConfig</code> instance.</p>
<p>In this example, the Kafka
<a href="/%7B%7Bversion%7D%7D/javadoc/org/apache/kafka/clients/consumer/ConsumerConfig.html#SESSION_TIMEOUT_MS_CONFIG">consumer session timeout</a> is configured to be 60000 milliseconds in the Streams
settings:</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">Properties streamsSettings = new Properties();
// Example of a &#34;normal&#34; setting for Kafka Streams
streamsSettings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &#34;kafka-broker-01:9092&#34;);
// Customize the Kafka consumer settings of your Streams application
streamsSettings.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 60000);
</code></pre><h4 id="naming">
  Naming
  <a class="anchor" href="#naming">#</a>
</h4>
<p>Some consumer, producer and admin client configuration parameters use
the same parameter name, and Kafka Streams library itself also uses some
parameters that share the same name with its embedded client. For
example, <code>send.buffer.bytes</code> and
<code>receive.buffer.bytes</code> are used to configure
TCP buffers; <code>request.timeout.ms</code> and
<code>retry.backoff.ms</code> control retries for
client request; <code>retries</code> are used to
configure how many retries are allowed when handling retriable errors
from broker request responses. You can avoid duplicate names by prefix
parameter names with <code>consumer.</code>,
<code>producer.</code>, or <code>admin.</code> (e.g., <code>consumer.send.buffer.bytes</code> and <code>producer.send.buffer.bytes</code>).</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">Properties streamsSettings = new Properties();
// same value for consumer, producer, and admin client
streamsSettings.put(&#34;PARAMETER_NAME&#34;, &#34;value&#34;);
// different values for consumer and producer
streamsSettings.put(&#34;consumer.PARAMETER_NAME&#34;, &#34;consumer-value&#34;);
streamsSettings.put(&#34;producer.PARAMETER_NAME&#34;, &#34;producer-value&#34;);
streamsSettings.put(&#34;admin.PARAMETER_NAME&#34;, &#34;admin-value&#34;);
// alternatively, you can use
streamsSettings.put(StreamsConfig.consumerPrefix(&#34;PARAMETER_NAME&#34;), &#34;consumer-value&#34;);
streamsSettings.put(StreamsConfig.producerPrefix(&#34;PARAMETER_NAME&#34;), &#34;producer-value&#34;);
streamsSettings.put(StreamsConfig.adminClientPrefix(&#34;PARAMETER_NAME&#34;), &#34;admin-value&#34;);
</code></pre><p>You could further separate consumer configuration by adding different
prefixes:</p>
<ul>
<li><code>main.consumer.</code> for main consumer which
is the default consumer of stream source.</li>
<li><code>restore.consumer.</code> for restore consumer
which is in charge of state store recovery.</li>
<li><code>global.consumer.</code> for global consumer
which is used in global KTable construction.</li>
</ul>
<p>For example, if you only want to set restore consumer config without
touching other consumers' settings, you could simply use
<code>restore.consumer.</code> to set the config.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">Properties streamsSettings = new Properties();
// same config value for all consumer types
streamsSettings.put(&#34;consumer.PARAMETER_NAME&#34;, &#34;general-consumer-value&#34;);
// set a different restore consumer config. This would make restore consumer take restore-consumer-value,
// while main consumer and global consumer stay with general-consumer-value
streamsSettings.put(&#34;restore.consumer.PARAMETER_NAME&#34;, &#34;restore-consumer-value&#34;);
// alternatively, you can use
streamsSettings.put(StreamsConfig.restoreConsumerPrefix(&#34;PARAMETER_NAME&#34;), &#34;restore-consumer-value&#34;);
</code></pre><p>Same applied to <code>main.consumer.</code> and
<code>main.consumer.</code>, if you only want to
specify one consumer type config.</p>
<p>Additionally, to configure the internal repartition/changelog topics,
you could use the <code>topic.</code> prefix, followed
by any of the standard topic configs.</p>
<pre tabindex="0"><code class="language-line-numbers" data-lang="line-numbers">Properties streamsSettings = new Properties();
// Override default for both changelog and repartition topics
streamsSettings.put(&#34;topic.PARAMETER_NAME&#34;, &#34;topic-value&#34;);
// alternatively, you can use
streamsSettings.put(StreamsConfig.topicPrefix(&#34;PARAMETER_NAME&#34;), &#34;topic-value&#34;);
</code></pre><h4 id="default-values">
  Default Values
  <a class="anchor" href="#default-values">#</a>
</h4>
<p>Kafka Streams uses different default values for some of the underlying
client configs, which are summarized below. For detailed descriptions of
these configs, see <a href="http://kafka.apache.org/0100/documentation.html#producerconfigs">Producer
Configs</a> and <a href="http://kafka.apache.org/0100/documentation.html#newconsumerconfigs">Consumer
Configs</a>.</p>
<p>Parameter Name      Corresponding Client   Streams Default</p>
<hr>
<p>auto.offset.reset   Consumer               earliest
linger.ms           Producer               100
max.poll.records    Consumer               1000</p>
<h3 id="parameters-controlled-by-kafka-streams">
  Parameters controlled by Kafka Streams
  <a class="anchor" href="#parameters-controlled-by-kafka-streams">#</a>
</h3>
<p>Kafka Streams assigns the following configuration parameters. If you try
to change <code>allow.auto.create.topics</code>, your
value is ignored and setting it has no effect in a Kafka Streams
application. You can set the other parameters. Kafka Streams sets them
to different default values than a plain <code>KafkaConsumer</code>.</p>
<p>Kafka Streams uses the <code>client.id</code> parameter
to compute derived client IDs for internal clients. If you don't set
<code>client.id</code>, Kafka Streams sets it to
<code>&lt;application.id&gt;-&lt;random-UUID&gt;</code>.</p>
<hr>
<p>Parameter Name                      Corresponding   Streams Default
Client</p>
<hr>
<p>allow.auto.create.topics            Consumer        <code>false</code></p>
<p>auto.offset.reset                   Consumer        <code>earliest</code></p>
<p>linger.ms                           Producer        <code>100</code></p>
<h2 id="maxpollrecords--------------------consumer--------1000">
  max.poll.records                    Consumer        <code>1000</code>
  <a class="anchor" href="#maxpollrecords--------------------consumer--------1000">#</a>
</h2>
<h4 id="enable-auto-commit">
  enable.auto.commit
  <a class="anchor" href="#enable-auto-commit">#</a>
</h4>
<p>The consumer auto commit. To guarantee at-least-once processing
semantics and turn off auto commits, Kafka Streams overrides this
consumer config value to <code>false</code>.
Consumers will only commit explicitly via <em>commitSync</em> calls when the
Kafka Streams library or a user decides to commit the current
processing state.</p>
</article>
        </div>
    </div>
</div>
<div class="footer">
    <div class="footer__inner">
        <div class="footer__legal">
            <span class="footer__legal__one">The contents of this website are &copy; 2023 <a
                    href="https://www.apache.org/" target="_blank">Apache Software Foundation</a> under the terms of the <a
                    href="https://www.apache.org/licenses/LICENSE-2.0.html"
                    target="_blank">Apache License v2</a>.</span>
            <span class="footer__legal__two">Apache Kafka, Kafka, and the Kafka logo are either registered trademarks or trademarks of The Apache Software Foundation</span>
            <span class="footer__legal__three">in the United States and other countries.</span>
            <div>
                <a href="https://kafka.apache.org/project-security" target="_blank" rel="noreferrer">Security</a>&nbsp;|&nbsp;
                <a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noreferrer">Donate</a>&nbsp;|&nbsp;
                <a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noreferrer">Thanks</a>&nbsp;|&nbsp;
                <a href="https://apache.org/events/current-event" target="_blank" rel="noreferrer">Events</a>&nbsp;|&nbsp;
                <a href="https://apache.org/licenses/" target="_blank" rel="noreferrer">License</a>&nbsp;|&nbsp;
                <a href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank"
                   rel="noreferrer">Privacy</a>
            </div>
        </div>
        <a class="apache-feather" target="_blank" href="http://www.apache.org">
            <img width="40" src="http://localhost:8080/35/documentation/images/feather-small.png" alt="Apache Feather">
        </a>
    </div>
</div>


<script src="http://localhost:8080/35/documentation/js/handlebars.js"></script>
<script>
			$(function () {
				
				var templates = [
					'introduction',
					'implementation',
					'design',
					'api',
					'configuration',
					'ops',
					'security',
					'connect',
					'streams',
					'quickstart',
					'quickstart-docker',
					'toc',
					'upgrade',
					'content'
				];

				
				for(var i = 0; i < templates.length; i++) {
					var templateScript = $("#" + templates[i] + "-template").html();
					if(templateScript) {
						var template = Handlebars.compile(templateScript);
						var html = template(context);
						$(".p-" + templates[i]).html(html);
					}
				}
			});






</script>

<script src="http://localhost:8080/35/documentation/js/jquery.sticky-kit.min.js"></script>
<script>

			let mobileScrolling = false;
			let mobileTimeout;

			
			function checkPageScroll() {
				if (window.scrollY >= 80 || document.body.scrollTop >= 100) {
					document.getElementById('header').classList.add('scrolled');
				} else {
					document.getElementById('header').classList.remove('scrolled');
				}
			}

			function mobilePageScroll() {
				mobileScrolling = true;
				mobileTimeout = null;
				mobileTimeout = setTimeout(checkPageScroll, 200);
			}

			function pageScroll(e) {
				requestAnimationFrame(checkPageScroll);
			};

			function setUpA11yMenus(nav) {
				const itemsWithMenu = nav.querySelectorAll('[aria-haspopup="true"]');
				const topLevelItems = nav.querySelectorAll('.top-nav-item-anchor');
				const navAnchors = nav.querySelectorAll('a');
				const menus = nav.querySelectorAll('[role="menu"]');
				const keyMap = {
					
					'left': 37, 'up': 38, 'right': 39, 'down': 40,
					
					'tab': 9, 'enter': 13, 'esc': 27, 'space': 32,
				}
				
				const keyCodes = [37, 38, 39, 40, 9, 13, 27, 32];

				
				nav.addEventListener('click', (e) => {
					if (e.target.hasAttribute('href') && e.target.getAttribute('href') === '#') {
						e.preventDefault();
						e.stopPropagation();
					}
				});

				for (let i = 0; i < topLevelItems.length; i++) {
					const item = topLevelItems[i];
					const attrNames = {
						currentFocus: 'data-current-focus',
						hidden: 'aria-hidden',
						expanded: 'aria-expanded',
					};
					
					
					const itemMenu = item.parentNode.querySelector('[role="menu"]');

					function selectPreviousMenuItem(e, keyPressed) {
						let selectedIndex = -1;

						if (!itemMenu) {
							selectPreviousTopLevelItem(e);
						}

						if (itemMenu && itemMenu.hasAttribute(attrNames.currentFocus)) {
							selectedIndex = parseInt(itemMenu.getAttribute(attrNames.currentFocus), 10);
						}
						let nextIndex = selectedIndex - 1;
						const menuItems = itemMenu.getElementsByTagName('a');

						if (nextIndex >= 0) {
						
							menuItems[nextIndex].focus();
							itemMenu.setAttribute(attrNames.currentFocus, nextIndex);
						} else if (nextIndex === -1) {
						
							item.focus();
							itemMenu.setAttribute(attrNames.currentFocus, -1);
						} else if (nextIndex === -2) {
							if (keyPressed === 'tab') {
								
								selectPreviousTopLevelItem(e);
							} else {
								
								menuItems[menuItems.length - 1].focus()
								itemMenu.setAttribute(attrNames.currentFocus, menuItems.length - 1);
							}
						}
					}

					function selectNextMenuItem(keyPressed) {
						let selectedIndex = -1;
						let nextIndex;
						let menuItems;

						if (!itemMenu) {
							selectNextTopLevelItem();
						}

						menuItems = itemMenu.getElementsByTagName('a');

						for (let j = 0; j < menuItems.length; j++) {
							if (menuItems[j] === document.activeElement) {
								selectedIndex = j;
								break;
							}
						}

						nextIndex = selectedIndex + 1;

						

						if (keyPressed === 'tab') {
							if (nextIndex >= menuItems.length) {
								selectNextTopLevelItem()
							} else {
								menuItems[nextIndex].focus();
							}
						} else {
							if (nextIndex < menuItems.length) {
								menuItems[nextIndex].focus();
							} else {
								item.focus();
							}
						}
					}

					function selectPreviousTopLevelItem() {
						let newIndex;

						for (let j = 0; j < topLevelItems.length; j++) {
							if (topLevelItems[j] === item) {
								newIndex = j - 1;
								break;
							}
						}

						if (itemMenu) {
							hideMenu();
						}

						if (newIndex < 0) {
							document.querySelector('.logo-link').focus();
						} else {
							topLevelItems[newIndex].focus();
						}
					}

					function selectNextTopLevelItem() {
						let newIndex;

						for (let j = 0; j < topLevelItems.length; j++) {
							if (topLevelItems[j] === item) {
								newIndex = j + 1;
								break;
							}
						}

						if (itemMenu) {
							hideMenu();
						}

						if (newIndex > topLevelItems.length - 1) {
							document.getElementById('top-nav-download').focus();
						} else {
							topLevelItems[newIndex].focus();
						}
					}

					function isMenuOpen() {
						return itemMenu && itemMenu.getAttribute(attrNames.hidden) === 'false';
					}

					function showMenu() {
						item.setAttribute(attrNames.expanded, 'true');
						if (itemMenu) {
							itemMenu.setAttribute(attrNames.hidden, 'false');
						}
					}

					function hideMenu() {
						item.setAttribute(attrNames.expanded, 'false');
						if (itemMenu) {
							itemMenu.setAttribute(attrNames.hidden, 'true');
							delete itemMenu.dataset.currentFocus;
						}
					}

					item.parentNode.addEventListener('mouseover', showMenu);
					item.parentNode.addEventListener('mouseout', hideMenu);
					item.parentNode.addEventListener('blur', hideMenu);
					item.addEventListener('focus', showMenu);

					
					item.parentNode.addEventListener('keydown', (e) => {
						
						if (keyCodes.indexOf(e.keyCode) > -1) {
							e.preventDefault();
							e.stopPropagation();

							switch(e.keyCode) {
								case keyMap.left:
									if (itemMenu) {
										if (!isMenuOpen()) {
											selectPreviousTopLevelItem();
										} else {
											hideMenu();
											item.focus();
										}
									} else {
										selectPreviousTopLevelItem();
									}
								break;
								case keyMap.up:
									if (itemMenu) {
										if (isMenuOpen()) {
											selectPreviousMenuItem(e, 'up');
										}
									}
								break;
								case keyMap.right:
									if (!isMenuOpen() || document.activeElement === item) {
										selectNextTopLevelItem('right');
									}
								break;
								case keyMap.down:
									if (itemMenu) {
										if (isMenuOpen()) {
											selectNextMenuItem('down');
										} else {
											showMenu();
										}
									}
								break;
								case keyMap.tab:
									const isShiftActive = e.getModifierState('Shift');
									if (itemMenu) {
										if (isShiftActive) {
											selectPreviousTopLevelItem();
										} else {
											if (isMenuOpen()) {
												selectNextMenuItem('tab');
											} else {
												showMenu();
											}
										}
									} else {
										if (isShiftActive) {
											selectPreviousTopLevelItem();
										} else {
											selectNextTopLevelItem('tab');
										}
									}
								break;
								case keyMap.esc:
									if (itemMenu && isMenuOpen()) {
										hideMenu();
									}
								break;
								default:
									
								break;
							}
						}
					});
				};
			}

			function is_touch_enabled() {
				return ( 'ontouchstart' in window ) ||
							( navigator.maxTouchPoints > 0 ) ||
							( navigator.msMaxTouchPoints > 0 );
			}

			function setupDocsNav() {
				var docsContainer = document.querySelector('.documentation--current');
				var docsHandle = document.querySelector('.toc-handle');

				function toggleDocsWidth() {
					let isExpanded = docsContainer.classList.toggle('expanded');
					
					if (isExpanded) {
						docsHandle.textContent = ">";
					} else {
						docsHandle.textContent = "<";
					}
				}

				docsHandle.addEventListener('click', toggleDocsWidth);
			}

			
			(function() {
				var navToggle = document.getElementById('top-nav-toggle');
				var nav = document.getElementById('top-nav-container');
				var docsNav = document.querySelector('.docs-nav');

				navToggle.addEventListener('click', () => {
					navToggle.classList.toggle('active');
				});

				
				if (is_touch_enabled()) {
					nav.querySelectorAll('a').forEach(anchor => anchor.removeAttribute('tabindex'));
					window.addEventListener('touchmove', mobilePageScroll, false);
				} else {
					setUpA11yMenus(nav);
					window.addEventListener('scroll', pageScroll, false);
				}

				
				document.addEventListener("DOMContentLoaded", function() {
					
					pageScroll();

					if (docsNav) {
						setupDocsNav();
					}

					
					
					
				});
			}());






</script>
</body>
</html>